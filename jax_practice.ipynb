{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Practice Worksheet\n",
    "\n",
    "A simple study guide covering JAX fundamentals.\n",
    "\n",
    "### What is JAX?\n",
    "\n",
    "JAX is Google's library for high-performance numerical computing and machine learning research. Think of it as **NumPy on steroids** — it gives you a familiar NumPy-like API but adds three superpowers:\n",
    "\n",
    "1. **Automatic differentiation** (`grad`) — compute gradients of any function automatically, which is the backbone of training neural networks and optimization in general.\n",
    "2. **Just-in-time compilation** (`jit`) — compile your Python functions down to optimized machine code using XLA (Accelerated Linear Algebra), the same compiler backend that powers TensorFlow.\n",
    "3. **Auto-vectorization** (`vmap`) — write a function that works on a single example, then instantly vectorize it to work on entire batches with no manual loop writing.\n",
    "\n",
    "### Why JAX instead of NumPy or PyTorch?\n",
    "\n",
    "- **vs NumPy**: JAX can run on GPU/TPU and supports autodiff. NumPy is CPU-only and has no built-in gradients.\n",
    "- **vs PyTorch**: JAX takes a more *functional* approach — no classes, no `nn.Module`, just pure functions. This makes code easier to reason about and compose. PyTorch is more object-oriented and imperative.\n",
    "- **Composability**: JAX transformations (`grad`, `jit`, `vmap`) can be freely composed. You can `jit(vmap(grad(f)))` and it just works.\n",
    "\n",
    "### Key mental model\n",
    "\n",
    "JAX functions should be **pure functions** — they take inputs and return outputs with no side effects. This is what enables all the powerful transformations to work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "We import four core pieces of JAX:\n",
    "\n",
    "- **`jax`** — the top-level module; gives us `jax.devices()` to check what hardware we're running on.\n",
    "- **`jax.numpy` (as `jnp`)** — a drop-in replacement for NumPy. Almost every `np.something()` has a `jnp.something()` equivalent. The key difference is that `jnp` arrays live on accelerators (GPU/TPU) and are immutable.\n",
    "- **`grad`, `jit`, `vmap`** — the three core transformations. These are *higher-order functions*: they take a function as input and return a new, transformed function.\n",
    "- **`jax.random`** — JAX's random number system. Unlike NumPy's `np.random`, JAX doesn't use global random state. Every random call requires an explicit key (more on this in Section 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.9.0\n",
      "Devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"Devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JAX Arrays vs NumPy\n",
    "\n",
    "`jax.numpy` mirrors the NumPy API almost exactly, so if you know NumPy, you already know most of `jnp`. The critical differences:\n",
    "\n",
    "### Immutability\n",
    "\n",
    "JAX arrays are **immutable** — once created, you cannot modify them in-place. This means:\n",
    "\n",
    "```python\n",
    "# NumPy (works fine):\n",
    "x[0] = 5\n",
    "\n",
    "# JAX (raises an error!):\n",
    "x[0] = 5  # TypeError: JAX arrays are immutable\n",
    "```\n",
    "\n",
    "Instead, JAX provides the `.at[].set()` syntax which returns a **new** array with the change applied, leaving the original untouched. This functional style is essential because JAX's transformations (grad, jit, vmap) rely on functions being pure — no side effects, no mutation.\n",
    "\n",
    "### Other `.at[]` operations\n",
    "\n",
    "Beyond `.set()`, you can also use:\n",
    "- `x.at[i].add(v)` — add `v` to position `i`\n",
    "- `x.at[i].multiply(v)` — multiply position `i` by `v`\n",
    "- `x.at[i].min(v)` / `x.at[i].max(v)` — element-wise min/max\n",
    "\n",
    "### Device placement\n",
    "\n",
    "JAX arrays are automatically placed on the best available device (GPU > TPU > CPU). You can check with `x.devices()`. Data transfers between CPU and GPU happen automatically but can be a performance bottleneck if you're not careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1. 2. 3.]\n",
      "b:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "c: [ 50.  55.  60.  65.  70.  75.  80.  85.  90.  95. 100.]\n"
     ]
    }
   ],
   "source": [
    "# Creating arrays (just like numpy)\n",
    "a = jnp.array([1.0, 2.0, 3.0])\n",
    "b = jnp.zeros((3, 3))\n",
    "c = jnp.linspace(50, 100, 11)\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\\n\", b)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [0. 0. 0. 0. 0.]\n",
      "updated:  [ 0.  0. 99.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Immutable updates — use .at[].set()\n",
    "x = jnp.zeros(5)\n",
    "x_updated = x.at[2].set(99.0)\n",
    "\n",
    "print(\"original:\", x)\n",
    "print(\"updated: \", x_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2a\n",
    "Create a 4x4 identity matrix using `jnp.eye()`, then replace the top-left element with `7.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[7. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.eye(4)\n",
    "x_updated = x.at[0,0].set(7.0)\n",
    "print(x)\n",
    "print(x_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Numbers (PRNGKey)\n",
    "\n",
    "This is one of the **biggest differences** between JAX and NumPy, and it trips up almost everyone at first.\n",
    "\n",
    "### The problem with NumPy's random\n",
    "\n",
    "In NumPy, random numbers come from a **global** random state:\n",
    "\n",
    "```python\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(size=(3,))  # uses and mutates global state\n",
    "```\n",
    "\n",
    "This is convenient but causes problems:\n",
    "- **Not reproducible under parallelism** — if two threads pull random numbers, the order is unpredictable.\n",
    "- **Not compatible with `jit`** — JAX's JIT compiler needs pure functions with no hidden state.\n",
    "\n",
    "### JAX's solution: explicit PRNG keys\n",
    "\n",
    "In JAX, every random call takes an explicit **key** (a pair of 32-bit integers):\n",
    "\n",
    "```python\n",
    "key = random.PRNGKey(42)          # create a key from a seed\n",
    "x = random.normal(key, shape=(3,)) # use the key\n",
    "```\n",
    "\n",
    "**Critical rule**: never reuse a key for two different random calls! If you do, you'll get the same \"random\" numbers. Instead, **split** the key:\n",
    "\n",
    "```python\n",
    "key, subkey = random.split(key)   # split into 2 new keys\n",
    "x = random.normal(subkey, (3,))   # use the subkey, keep key for later\n",
    "```\n",
    "\n",
    "### The split pattern\n",
    "\n",
    "`random.split(key, n)` takes one key and returns `n` independent new keys. The common pattern is:\n",
    "\n",
    "```python\n",
    "key, *subkeys = random.split(key, 4)  # keep key, get 3 subkeys\n",
    "```\n",
    "\n",
    "This feels verbose at first, but it guarantees **perfect reproducibility** regardless of execution order, parallelism, or hardware — which is essential for scientific computing and ML research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal samples: [ 0.60576403  0.7990441  -0.908927  ]\n",
      "uniform samples: [0.6672406 0.7214867 0.1267947]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Split the key to get independent sub-keys\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "\n",
    "x = random.normal(subkey1, shape=(3,))\n",
    "y = random.uniform(subkey2, shape=(3,))\n",
    "\n",
    "print(\"normal samples:\", x)\n",
    "print(\"uniform samples:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3a\n",
    "Generate a 2x3 matrix of random integers between 0 and 10. (Hint: `random.randint`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 6 6]\n",
      " [8 7 7]]\n",
      "[[4 2 6]\n",
      " [4 1 4]]\n",
      "[[5 0 5]\n",
      " [7 3 1]]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(1)\n",
    "key,subkey = random.split(key,2)\n",
    "\n",
    "x = random.randint(subkey,(2,3),0,10)\n",
    "print(x)\n",
    "\n",
    "key,subkey = random.split(key,2)\n",
    "x = random.randint(subkey,(2,3),0,10)\n",
    "print(x)\n",
    "\n",
    "key,subkey = random.split(key,2)\n",
    "x = random.randint(subkey,(2,3),0,10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `grad` — Automatic Differentiation\n",
    "\n",
    "This is arguably JAX's most important feature for machine learning.\n",
    "\n",
    "### What is automatic differentiation?\n",
    "\n",
    "Differentiation (finding derivatives/gradients) is the core of how neural networks learn. There are three ways to compute derivatives:\n",
    "\n",
    "1. **Symbolic** — like you'd do by hand in calculus class. Exact but gets messy for complex functions.\n",
    "2. **Numerical** — approximate with `(f(x+h) - f(x)) / h`. Simple but slow and imprecise.\n",
    "3. **Automatic** — what JAX does. It traces through your Python code and applies the chain rule automatically. Exact *and* efficient.\n",
    "\n",
    "### How `grad` works\n",
    "\n",
    "`grad(f)` takes a function `f` and returns a **new function** that computes the derivative:\n",
    "\n",
    "```python\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "df = grad(f)     # df is now a function that computes 2x\n",
    "df(3.0)          # returns 6.0\n",
    "```\n",
    "\n",
    "Key details:\n",
    "- **`grad` differentiates w.r.t. the first argument by default.** Use `argnums` to change this: `grad(f, argnums=1)` differentiates w.r.t. the second argument.\n",
    "- **Input must be a float (or array of floats).** `grad` won't work on integers.\n",
    "- **Output must be a scalar.** If your function returns an array, use `jax.jacobian` instead, or sum/mean the output first.\n",
    "- **You can compose `grad`** — `grad(grad(f))` gives you the second derivative, `grad(grad(grad(f)))` the third, and so on.\n",
    "\n",
    "### Why this matters for ML\n",
    "\n",
    "In machine learning, we define a **loss function** that measures how wrong our model is. `grad` lets us compute exactly how to adjust each parameter to reduce that loss — that's the gradient, and it's the signal that drives learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "def f(x): \n",
    "    return x ** 2\n",
    "\n",
    "df = grad(f)\n",
    "print(df(3.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0)   = 7.0\n",
      "f'(2.0)  = 15.0\n",
      "f''(2.0) = 16.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 3 + 2 * x ** 2 - 5 * x + 1\n",
    "\n",
    "df = grad(f)        # first derivative\n",
    "ddf = grad(grad(f))  # second derivative\n",
    "\n",
    "x = 2.0\n",
    "print(f\"f({x})   = {f(x)}\")\n",
    "print(f\"f'({x})  = {df(x)}\")    # 3x^2 + 4x - 5 => 15\n",
    "print(f\"f''({x}) = {ddf(x)}\")   # 6x + 4 => 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4a\n",
    "Define `g(x) = sin(x) * exp(-x)`. Compute its gradient at `x = 1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  0.3095599\n",
      "Derivative:  -0.110793784\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return jnp.sin(x) * jnp.exp(-x)\n",
    "\n",
    "x=1.0\n",
    "print(\"Output: \", g(x))\n",
    "\n",
    "dv = grad(g)\n",
    "print(\"Derivative: \", dv(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `jit` — Just-In-Time Compilation\n",
    "\n",
    "### The problem: Python is slow\n",
    "\n",
    "Python is an interpreted language, which means each operation is executed one at a time with lots of overhead. For numerical code with many operations, this overhead adds up fast.\n",
    "\n",
    "### The solution: XLA compilation\n",
    "\n",
    "When you wrap a function with `jit`, JAX doesn't run it immediately. Instead, it:\n",
    "\n",
    "1. **Traces** the function — runs it once with abstract \"placeholder\" values to figure out what operations it performs.\n",
    "2. **Compiles** the traced operations into a single optimized XLA program — fusing operations, eliminating redundant computation, and targeting your specific hardware (CPU/GPU/TPU).\n",
    "3. **Caches** the compiled version — subsequent calls with the same input shapes skip tracing and run the optimized code directly.\n",
    "\n",
    "### When to use `jit`\n",
    "\n",
    "- Any function you call repeatedly with the same input shapes (e.g., a training step).\n",
    "- Functions with many small operations that can be fused together.\n",
    "- Inner loops of numerical algorithms.\n",
    "\n",
    "### Gotchas to watch out for\n",
    "\n",
    "- **First call is slow** — that's the tracing + compilation step. All subsequent calls are fast.\n",
    "- **No Python side effects inside `jit`** — `print()` only runs during tracing, not on subsequent calls. Same for any Python-level if/else based on array values.\n",
    "- **Input shapes must be static** — if you pass different-shaped inputs, JAX recompiles (slow). Use `jax.ensure_compile_time_eval()` or `static_argnums` for arguments that change but aren't arrays.\n",
    "- **`block_until_ready()`** — JAX uses async dispatch, so timing benchmarks need this call to force the computation to finish before measuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55 ms ± 104 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.34 ms ± 12.2 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def slow_fn(x):\n",
    "    for _ in range(50):\n",
    "        x = x @ x\n",
    "    return x\n",
    "\n",
    "fast_fn = jit(slow_fn)\n",
    "\n",
    "mat = random.normal(random.PRNGKey(0), (100, 100))\n",
    "\n",
    "%timeit slow_fn(mat).block_until_ready()\n",
    "%timeit fast_fn(mat).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `vmap` — Auto-Vectorization\n",
    "\n",
    "### The problem: batching is tedious\n",
    "\n",
    "In ML, you almost always work with **batches** of data. Say you write a function that processes a single image — now you need it to work on 64 images at once. You have two bad options:\n",
    "\n",
    "1. **Python loop** — `for img in batch: process(img)`. Works but extremely slow.\n",
    "2. **Manual batching** — rewrite your function to handle an extra batch dimension everywhere. Works and is fast, but error-prone and clutters the code.\n",
    "\n",
    "### The solution: `vmap`\n",
    "\n",
    "`vmap(f)` takes a function `f` that works on a **single example** and returns a new function that works on a **batch of examples** — automatically. Under the hood, it transforms the function to operate over an extra leading axis, with no Python loops and no manual reshaping.\n",
    "\n",
    "```python\n",
    "# Works on a single vector\n",
    "def normalize(x):\n",
    "    return x / jnp.linalg.norm(x)\n",
    "\n",
    "# Now works on a batch of vectors\n",
    "batch_normalize = vmap(normalize)\n",
    "```\n",
    "\n",
    "### Key parameters\n",
    "\n",
    "- **`in_axes`** — which axis of each input to map over. Default is `0` (first axis). Use `None` for arguments that shouldn't be batched.\n",
    "  ```python\n",
    "  # x is batched (axis 0), weights is shared across the batch\n",
    "  vmap(f, in_axes=(0, None))(batch_x, weights)\n",
    "  ```\n",
    "- **`out_axes`** — which axis of the output the mapped dimension should appear on. Default is `0`.\n",
    "\n",
    "### Why `vmap` matters\n",
    "\n",
    "- **Clean code** — write single-example logic, get batch processing for free.\n",
    "- **Performance** — `vmap` generates the same efficient batched code you'd write by hand.\n",
    "- **Composability** — `vmap(vmap(f))` maps over two axes (e.g., batch of sequences of vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: (5, 3)\n",
      "norms: [0.22252384 1.284708   2.3824213  0.8305838  1.480686  ]\n"
     ]
    }
   ],
   "source": [
    "def l2_norm(x):\n",
    "    return jnp.sqrt(jnp.sum(x ** 2))\n",
    "\n",
    "batch_l2 = vmap(l2_norm)\n",
    "\n",
    "batch = random.normal(random.PRNGKey(1), (5, 3))\n",
    "print(\"batch shape:\", batch.shape)\n",
    "print(\"norms:\", batch_l2(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6a\n",
    "Write a function `dot_product(a, b)` for single vectors, then use `vmap` to compute dot products for a batch of 10 vector pairs (each of length 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "[[5 2 4 2]\n",
      " [5 4 1 8]\n",
      " [6 5 0 6]\n",
      " ...\n",
      " [3 1 6 3]\n",
      " [4 1 5 6]\n",
      " [7 5 4 3]]\n",
      "b:\n",
      "[[7 9 8 3]\n",
      " [8 7 6 9]\n",
      " [5 4 2 5]\n",
      " ...\n",
      " [4 6 5 6]\n",
      " [5 2 2 2]\n",
      " [0 4 9 8]]\n",
      "Example dot product: 8086866\n",
      "For Loop: [ 91 146  80 ...  66  44  80]\n",
      "VMAP dot product: [ 91 146  80 ...  66  44  80]\n",
      "297 μs ± 21.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "1.61 ms ± 76.1 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Split the key to get independent sub-keys\n",
    "key, subkey = random.split(key, 2)\n",
    "a = random.randint(subkey,shape=(100000,4),minval=0,maxval=10)\n",
    "\n",
    "key, subkey = random.split(key, 2)\n",
    "b = random.randint(subkey,shape=(100000,4),minval=0,maxval=10)\n",
    "\n",
    "print(f'a:\\n{a}\\nb:\\n{b}')\n",
    "\n",
    "# x = [1,2,3,4]\n",
    "# y = [1,2,3,4]\n",
    "\n",
    "def dot_product(a,b): \n",
    "    return jnp.sum(a*b)\n",
    "print(f'Example dot product: {dot_product(a,b)}')\n",
    "\n",
    "results = jnp.array([dot_product(a[i], b[i]) for i in range(len(a))])\n",
    "print(f'For Loop: {results}')\n",
    "\n",
    "batch_dot_product = vmap(dot_product)\n",
    "print(f'VMAP dot product: {batch_dot_product(a,b)}')\n",
    "\n",
    "%timeit dot_product(a,b).block_until_ready()\n",
    "%timeit batch_dot_product(a,b).block_until_ready()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting It Together — Simple Gradient Descent\n",
    "\n",
    "Now we combine `grad` with a loop to do **gradient descent** — the fundamental optimization algorithm behind all of deep learning.\n",
    "\n",
    "### How gradient descent works\n",
    "\n",
    "1. Start with some initial guess for your parameter `x`.\n",
    "2. Compute the **gradient** of your loss function at `x` — this tells you the direction of steepest *increase*.\n",
    "3. Take a small step in the **opposite** direction (to decrease the loss): `x = x - lr * gradient`.\n",
    "4. Repeat until the loss is small enough.\n",
    "\n",
    "### The learning rate (`lr`)\n",
    "\n",
    "The learning rate controls how big each step is:\n",
    "- **Too large** — you overshoot the minimum and the loss explodes.\n",
    "- **Too small** — convergence is painfully slow.\n",
    "- **Just right** — smooth convergence to the minimum.\n",
    "\n",
    "A common starting point is `0.01` or `0.1` for simple problems.\n",
    "\n",
    "### What's happening in the code below\n",
    "\n",
    "We minimize `f(x) = (x - 3)^2`, which has its minimum at `x = 3`. The gradient is `f'(x) = 2(x - 3)`. Starting from `x = 0`, each step nudges `x` closer to 3. JAX computes `f'(x)` for us automatically via `grad` — we never write the derivative by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0: x = 0.0600, loss = 8.6436\n",
      "step  5: x = 0.3425, loss = 7.0625\n",
      "step 10: x = 0.5978, loss = 5.7705\n",
      "step 15: x = 0.8286, loss = 4.7149\n",
      "step 20: x = 1.0372, loss = 3.8525\n",
      "step 25: x = 1.2258, loss = 3.1477\n",
      "step 30: x = 1.3963, loss = 2.5719\n",
      "step 35: x = 1.5504, loss = 2.1015\n",
      "step 40: x = 1.6896, loss = 1.7170\n",
      "step 45: x = 1.8155, loss = 1.4029\n",
      "step 50: x = 1.9293, loss = 1.1463\n",
      "step 55: x = 2.0322, loss = 0.9366\n",
      "step 60: x = 2.1252, loss = 0.7653\n",
      "step 65: x = 2.2092, loss = 0.6253\n",
      "step 70: x = 2.2852, loss = 0.5109\n",
      "step 75: x = 2.3539, loss = 0.4175\n",
      "step 80: x = 2.4160, loss = 0.3411\n",
      "step 85: x = 2.4721, loss = 0.2787\n",
      "step 90: x = 2.5228, loss = 0.2277\n",
      "step 95: x = 2.5687, loss = 0.1861\n",
      "step 100: x = 2.6101, loss = 0.1520\n",
      "step 105: x = 2.6476, loss = 0.1242\n",
      "step 110: x = 2.6814, loss = 0.1015\n",
      "step 115: x = 2.7120, loss = 0.0829\n",
      "step 120: x = 2.7397, loss = 0.0678\n",
      "step 125: x = 2.7647, loss = 0.0554\n",
      "step 130: x = 2.7873, loss = 0.0452\n",
      "step 135: x = 2.8077, loss = 0.0370\n",
      "step 140: x = 2.8262, loss = 0.0302\n",
      "step 145: x = 2.8429, loss = 0.0247\n",
      "step 150: x = 2.8580, loss = 0.0202\n",
      "step 155: x = 2.8717, loss = 0.0165\n",
      "step 160: x = 2.8840, loss = 0.0135\n",
      "step 165: x = 2.8951, loss = 0.0110\n",
      "step 170: x = 2.9052, loss = 0.0090\n",
      "step 175: x = 2.9143, loss = 0.0073\n",
      "step 180: x = 2.9225, loss = 0.0060\n",
      "step 185: x = 2.9300, loss = 0.0049\n",
      "step 190: x = 2.9367, loss = 0.0040\n",
      "step 195: x = 2.9428, loss = 0.0033\n",
      "step 200: x = 2.9483, loss = 0.0027\n",
      "step 205: x = 2.9533, loss = 0.0022\n",
      "step 210: x = 2.9578, loss = 0.0018\n",
      "step 215: x = 2.9618, loss = 0.0015\n",
      "step 220: x = 2.9655, loss = 0.0012\n",
      "step 225: x = 2.9688, loss = 0.0010\n",
      "step 230: x = 2.9718, loss = 0.0008\n",
      "step 235: x = 2.9745, loss = 0.0007\n",
      "step 240: x = 2.9770, loss = 0.0005\n",
      "step 245: x = 2.9792, loss = 0.0004\n",
      "step 250: x = 2.9812, loss = 0.0004\n",
      "step 255: x = 2.9830, loss = 0.0003\n",
      "step 260: x = 2.9846, loss = 0.0002\n",
      "step 265: x = 2.9861, loss = 0.0002\n",
      "step 270: x = 2.9874, loss = 0.0002\n",
      "step 275: x = 2.9886, loss = 0.0001\n",
      "step 280: x = 2.9897, loss = 0.0001\n",
      "step 285: x = 2.9907, loss = 0.0001\n",
      "step 290: x = 2.9916, loss = 0.0001\n",
      "step 295: x = 2.9924, loss = 0.0001\n",
      "step 300: x = 2.9931, loss = 0.0000\n",
      "step 305: x = 2.9938, loss = 0.0000\n",
      "step 310: x = 2.9944, loss = 0.0000\n",
      "step 315: x = 2.9949, loss = 0.0000\n",
      "step 320: x = 2.9954, loss = 0.0000\n",
      "step 325: x = 2.9959, loss = 0.0000\n",
      "step 330: x = 2.9963, loss = 0.0000\n",
      "step 335: x = 2.9966, loss = 0.0000\n",
      "step 340: x = 2.9969, loss = 0.0000\n",
      "step 345: x = 2.9972, loss = 0.0000\n",
      "step 350: x = 2.9975, loss = 0.0000\n",
      "step 355: x = 2.9977, loss = 0.0000\n",
      "step 360: x = 2.9980, loss = 0.0000\n",
      "step 365: x = 2.9982, loss = 0.0000\n",
      "step 370: x = 2.9983, loss = 0.0000\n",
      "step 375: x = 2.9985, loss = 0.0000\n",
      "step 380: x = 2.9986, loss = 0.0000\n",
      "step 385: x = 2.9988, loss = 0.0000\n",
      "step 390: x = 2.9989, loss = 0.0000\n",
      "step 395: x = 2.9990, loss = 0.0000\n",
      "step 400: x = 2.9991, loss = 0.0000\n",
      "step 405: x = 2.9992, loss = 0.0000\n",
      "step 410: x = 2.9993, loss = 0.0000\n",
      "step 415: x = 2.9993, loss = 0.0000\n",
      "step 420: x = 2.9994, loss = 0.0000\n",
      "step 425: x = 2.9995, loss = 0.0000\n",
      "step 430: x = 2.9995, loss = 0.0000\n",
      "step 435: x = 2.9996, loss = 0.0000\n",
      "step 440: x = 2.9996, loss = 0.0000\n",
      "step 445: x = 2.9996, loss = 0.0000\n",
      "step 450: x = 2.9997, loss = 0.0000\n",
      "step 455: x = 2.9997, loss = 0.0000\n",
      "step 460: x = 2.9997, loss = 0.0000\n",
      "step 465: x = 2.9998, loss = 0.0000\n",
      "step 470: x = 2.9998, loss = 0.0000\n",
      "step 475: x = 2.9998, loss = 0.0000\n",
      "step 480: x = 2.9998, loss = 0.0000\n",
      "step 485: x = 2.9998, loss = 0.0000\n",
      "step 490: x = 2.9999, loss = 0.0000\n",
      "step 495: x = 2.9999, loss = 0.0000\n",
      "step 500: x = 2.9999, loss = 0.0000\n",
      "step 505: x = 2.9999, loss = 0.0000\n",
      "step 510: x = 2.9999, loss = 0.0000\n",
      "step 515: x = 2.9999, loss = 0.0000\n",
      "step 520: x = 2.9999, loss = 0.0000\n",
      "step 525: x = 2.9999, loss = 0.0000\n",
      "step 530: x = 2.9999, loss = 0.0000\n",
      "step 535: x = 2.9999, loss = 0.0000\n",
      "step 540: x = 2.9999, loss = 0.0000\n",
      "step 545: x = 3.0000, loss = 0.0000\n",
      "step 550: x = 3.0000, loss = 0.0000\n",
      "step 555: x = 3.0000, loss = 0.0000\n",
      "step 560: x = 3.0000, loss = 0.0000\n",
      "step 565: x = 3.0000, loss = 0.0000\n",
      "step 570: x = 3.0000, loss = 0.0000\n",
      "step 575: x = 3.0000, loss = 0.0000\n",
      "step 580: x = 3.0000, loss = 0.0000\n",
      "step 585: x = 3.0000, loss = 0.0000\n",
      "step 590: x = 3.0000, loss = 0.0000\n",
      "step 595: x = 3.0000, loss = 0.0000\n",
      "step 600: x = 3.0000, loss = 0.0000\n",
      "step 605: x = 3.0000, loss = 0.0000\n",
      "step 610: x = 3.0000, loss = 0.0000\n",
      "step 615: x = 3.0000, loss = 0.0000\n",
      "step 620: x = 3.0000, loss = 0.0000\n",
      "step 625: x = 3.0000, loss = 0.0000\n",
      "step 630: x = 3.0000, loss = 0.0000\n",
      "step 635: x = 3.0000, loss = 0.0000\n",
      "step 640: x = 3.0000, loss = 0.0000\n",
      "step 645: x = 3.0000, loss = 0.0000\n",
      "step 650: x = 3.0000, loss = 0.0000\n",
      "step 655: x = 3.0000, loss = 0.0000\n",
      "step 660: x = 3.0000, loss = 0.0000\n",
      "step 665: x = 3.0000, loss = 0.0000\n",
      "step 670: x = 3.0000, loss = 0.0000\n",
      "step 675: x = 3.0000, loss = 0.0000\n",
      "step 680: x = 3.0000, loss = 0.0000\n",
      "step 685: x = 3.0000, loss = 0.0000\n",
      "step 690: x = 3.0000, loss = 0.0000\n",
      "step 695: x = 3.0000, loss = 0.0000\n",
      "step 700: x = 3.0000, loss = 0.0000\n",
      "step 705: x = 3.0000, loss = 0.0000\n",
      "step 710: x = 3.0000, loss = 0.0000\n",
      "step 715: x = 3.0000, loss = 0.0000\n",
      "step 720: x = 3.0000, loss = 0.0000\n",
      "step 725: x = 3.0000, loss = 0.0000\n",
      "step 730: x = 3.0000, loss = 0.0000\n",
      "step 735: x = 3.0000, loss = 0.0000\n",
      "step 740: x = 3.0000, loss = 0.0000\n",
      "step 745: x = 3.0000, loss = 0.0000\n",
      "step 750: x = 3.0000, loss = 0.0000\n",
      "step 755: x = 3.0000, loss = 0.0000\n",
      "step 760: x = 3.0000, loss = 0.0000\n",
      "step 765: x = 3.0000, loss = 0.0000\n",
      "step 770: x = 3.0000, loss = 0.0000\n",
      "step 775: x = 3.0000, loss = 0.0000\n",
      "step 780: x = 3.0000, loss = 0.0000\n",
      "step 785: x = 3.0000, loss = 0.0000\n",
      "step 790: x = 3.0000, loss = 0.0000\n",
      "step 795: x = 3.0000, loss = 0.0000\n",
      "step 800: x = 3.0000, loss = 0.0000\n",
      "step 805: x = 3.0000, loss = 0.0000\n",
      "step 810: x = 3.0000, loss = 0.0000\n",
      "step 815: x = 3.0000, loss = 0.0000\n",
      "step 820: x = 3.0000, loss = 0.0000\n",
      "step 825: x = 3.0000, loss = 0.0000\n",
      "step 830: x = 3.0000, loss = 0.0000\n",
      "step 835: x = 3.0000, loss = 0.0000\n",
      "step 840: x = 3.0000, loss = 0.0000\n",
      "step 845: x = 3.0000, loss = 0.0000\n",
      "step 850: x = 3.0000, loss = 0.0000\n",
      "step 855: x = 3.0000, loss = 0.0000\n",
      "step 860: x = 3.0000, loss = 0.0000\n",
      "step 865: x = 3.0000, loss = 0.0000\n",
      "step 870: x = 3.0000, loss = 0.0000\n",
      "step 875: x = 3.0000, loss = 0.0000\n",
      "step 880: x = 3.0000, loss = 0.0000\n",
      "step 885: x = 3.0000, loss = 0.0000\n",
      "step 890: x = 3.0000, loss = 0.0000\n",
      "step 895: x = 3.0000, loss = 0.0000\n",
      "step 900: x = 3.0000, loss = 0.0000\n",
      "step 905: x = 3.0000, loss = 0.0000\n",
      "step 910: x = 3.0000, loss = 0.0000\n",
      "step 915: x = 3.0000, loss = 0.0000\n",
      "step 920: x = 3.0000, loss = 0.0000\n",
      "step 925: x = 3.0000, loss = 0.0000\n",
      "step 930: x = 3.0000, loss = 0.0000\n",
      "step 935: x = 3.0000, loss = 0.0000\n",
      "step 940: x = 3.0000, loss = 0.0000\n",
      "step 945: x = 3.0000, loss = 0.0000\n",
      "step 950: x = 3.0000, loss = 0.0000\n",
      "step 955: x = 3.0000, loss = 0.0000\n",
      "step 960: x = 3.0000, loss = 0.0000\n",
      "step 965: x = 3.0000, loss = 0.0000\n",
      "step 970: x = 3.0000, loss = 0.0000\n",
      "step 975: x = 3.0000, loss = 0.0000\n",
      "step 980: x = 3.0000, loss = 0.0000\n",
      "step 985: x = 3.0000, loss = 0.0000\n",
      "step 990: x = 3.0000, loss = 0.0000\n",
      "step 995: x = 3.0000, loss = 0.0000\n",
      "step 1000: x = 3.0000, loss = 0.0000\n",
      "step 1005: x = 3.0000, loss = 0.0000\n",
      "step 1010: x = 3.0000, loss = 0.0000\n",
      "step 1015: x = 3.0000, loss = 0.0000\n",
      "step 1020: x = 3.0000, loss = 0.0000\n",
      "step 1025: x = 3.0000, loss = 0.0000\n",
      "step 1030: x = 3.0000, loss = 0.0000\n",
      "step 1035: x = 3.0000, loss = 0.0000\n",
      "step 1040: x = 3.0000, loss = 0.0000\n",
      "step 1045: x = 3.0000, loss = 0.0000\n",
      "step 1050: x = 3.0000, loss = 0.0000\n",
      "step 1055: x = 3.0000, loss = 0.0000\n",
      "step 1060: x = 3.0000, loss = 0.0000\n",
      "step 1065: x = 3.0000, loss = 0.0000\n",
      "step 1070: x = 3.0000, loss = 0.0000\n",
      "step 1075: x = 3.0000, loss = 0.0000\n",
      "step 1080: x = 3.0000, loss = 0.0000\n",
      "step 1085: x = 3.0000, loss = 0.0000\n",
      "step 1090: x = 3.0000, loss = 0.0000\n",
      "step 1095: x = 3.0000, loss = 0.0000\n",
      "step 1100: x = 3.0000, loss = 0.0000\n",
      "step 1105: x = 3.0000, loss = 0.0000\n",
      "step 1110: x = 3.0000, loss = 0.0000\n",
      "step 1115: x = 3.0000, loss = 0.0000\n",
      "step 1120: x = 3.0000, loss = 0.0000\n",
      "step 1125: x = 3.0000, loss = 0.0000\n",
      "step 1130: x = 3.0000, loss = 0.0000\n",
      "step 1135: x = 3.0000, loss = 0.0000\n",
      "step 1140: x = 3.0000, loss = 0.0000\n",
      "step 1145: x = 3.0000, loss = 0.0000\n",
      "step 1150: x = 3.0000, loss = 0.0000\n",
      "step 1155: x = 3.0000, loss = 0.0000\n",
      "step 1160: x = 3.0000, loss = 0.0000\n",
      "step 1165: x = 3.0000, loss = 0.0000\n",
      "step 1170: x = 3.0000, loss = 0.0000\n",
      "step 1175: x = 3.0000, loss = 0.0000\n",
      "step 1180: x = 3.0000, loss = 0.0000\n",
      "step 1185: x = 3.0000, loss = 0.0000\n",
      "step 1190: x = 3.0000, loss = 0.0000\n",
      "step 1195: x = 3.0000, loss = 0.0000\n",
      "step 1200: x = 3.0000, loss = 0.0000\n",
      "step 1205: x = 3.0000, loss = 0.0000\n",
      "step 1210: x = 3.0000, loss = 0.0000\n",
      "step 1215: x = 3.0000, loss = 0.0000\n",
      "step 1220: x = 3.0000, loss = 0.0000\n",
      "step 1225: x = 3.0000, loss = 0.0000\n",
      "step 1230: x = 3.0000, loss = 0.0000\n",
      "step 1235: x = 3.0000, loss = 0.0000\n",
      "step 1240: x = 3.0000, loss = 0.0000\n",
      "step 1245: x = 3.0000, loss = 0.0000\n",
      "step 1250: x = 3.0000, loss = 0.0000\n",
      "step 1255: x = 3.0000, loss = 0.0000\n",
      "step 1260: x = 3.0000, loss = 0.0000\n",
      "step 1265: x = 3.0000, loss = 0.0000\n",
      "step 1270: x = 3.0000, loss = 0.0000\n",
      "step 1275: x = 3.0000, loss = 0.0000\n",
      "step 1280: x = 3.0000, loss = 0.0000\n",
      "step 1285: x = 3.0000, loss = 0.0000\n",
      "step 1290: x = 3.0000, loss = 0.0000\n",
      "step 1295: x = 3.0000, loss = 0.0000\n",
      "step 1300: x = 3.0000, loss = 0.0000\n",
      "step 1305: x = 3.0000, loss = 0.0000\n",
      "step 1310: x = 3.0000, loss = 0.0000\n",
      "step 1315: x = 3.0000, loss = 0.0000\n",
      "step 1320: x = 3.0000, loss = 0.0000\n",
      "step 1325: x = 3.0000, loss = 0.0000\n",
      "step 1330: x = 3.0000, loss = 0.0000\n",
      "step 1335: x = 3.0000, loss = 0.0000\n",
      "step 1340: x = 3.0000, loss = 0.0000\n",
      "step 1345: x = 3.0000, loss = 0.0000\n",
      "step 1350: x = 3.0000, loss = 0.0000\n",
      "step 1355: x = 3.0000, loss = 0.0000\n",
      "step 1360: x = 3.0000, loss = 0.0000\n",
      "step 1365: x = 3.0000, loss = 0.0000\n",
      "step 1370: x = 3.0000, loss = 0.0000\n",
      "step 1375: x = 3.0000, loss = 0.0000\n",
      "step 1380: x = 3.0000, loss = 0.0000\n",
      "step 1385: x = 3.0000, loss = 0.0000\n",
      "step 1390: x = 3.0000, loss = 0.0000\n",
      "step 1395: x = 3.0000, loss = 0.0000\n",
      "step 1400: x = 3.0000, loss = 0.0000\n",
      "step 1405: x = 3.0000, loss = 0.0000\n",
      "step 1410: x = 3.0000, loss = 0.0000\n",
      "step 1415: x = 3.0000, loss = 0.0000\n",
      "step 1420: x = 3.0000, loss = 0.0000\n",
      "step 1425: x = 3.0000, loss = 0.0000\n",
      "step 1430: x = 3.0000, loss = 0.0000\n",
      "step 1435: x = 3.0000, loss = 0.0000\n",
      "step 1440: x = 3.0000, loss = 0.0000\n",
      "step 1445: x = 3.0000, loss = 0.0000\n",
      "step 1450: x = 3.0000, loss = 0.0000\n",
      "step 1455: x = 3.0000, loss = 0.0000\n",
      "step 1460: x = 3.0000, loss = 0.0000\n",
      "step 1465: x = 3.0000, loss = 0.0000\n",
      "step 1470: x = 3.0000, loss = 0.0000\n",
      "step 1475: x = 3.0000, loss = 0.0000\n",
      "step 1480: x = 3.0000, loss = 0.0000\n",
      "step 1485: x = 3.0000, loss = 0.0000\n",
      "step 1490: x = 3.0000, loss = 0.0000\n",
      "step 1495: x = 3.0000, loss = 0.0000\n",
      "step 1500: x = 3.0000, loss = 0.0000\n",
      "step 1505: x = 3.0000, loss = 0.0000\n",
      "step 1510: x = 3.0000, loss = 0.0000\n",
      "step 1515: x = 3.0000, loss = 0.0000\n",
      "step 1520: x = 3.0000, loss = 0.0000\n",
      "step 1525: x = 3.0000, loss = 0.0000\n",
      "step 1530: x = 3.0000, loss = 0.0000\n",
      "step 1535: x = 3.0000, loss = 0.0000\n",
      "step 1540: x = 3.0000, loss = 0.0000\n",
      "step 1545: x = 3.0000, loss = 0.0000\n",
      "step 1550: x = 3.0000, loss = 0.0000\n",
      "step 1555: x = 3.0000, loss = 0.0000\n",
      "step 1560: x = 3.0000, loss = 0.0000\n",
      "step 1565: x = 3.0000, loss = 0.0000\n",
      "step 1570: x = 3.0000, loss = 0.0000\n",
      "step 1575: x = 3.0000, loss = 0.0000\n",
      "step 1580: x = 3.0000, loss = 0.0000\n",
      "step 1585: x = 3.0000, loss = 0.0000\n",
      "step 1590: x = 3.0000, loss = 0.0000\n",
      "step 1595: x = 3.0000, loss = 0.0000\n",
      "step 1600: x = 3.0000, loss = 0.0000\n",
      "step 1605: x = 3.0000, loss = 0.0000\n",
      "step 1610: x = 3.0000, loss = 0.0000\n",
      "step 1615: x = 3.0000, loss = 0.0000\n",
      "step 1620: x = 3.0000, loss = 0.0000\n",
      "step 1625: x = 3.0000, loss = 0.0000\n",
      "step 1630: x = 3.0000, loss = 0.0000\n",
      "step 1635: x = 3.0000, loss = 0.0000\n",
      "step 1640: x = 3.0000, loss = 0.0000\n",
      "step 1645: x = 3.0000, loss = 0.0000\n",
      "step 1650: x = 3.0000, loss = 0.0000\n",
      "step 1655: x = 3.0000, loss = 0.0000\n",
      "step 1660: x = 3.0000, loss = 0.0000\n",
      "step 1665: x = 3.0000, loss = 0.0000\n",
      "step 1670: x = 3.0000, loss = 0.0000\n",
      "step 1675: x = 3.0000, loss = 0.0000\n",
      "step 1680: x = 3.0000, loss = 0.0000\n",
      "step 1685: x = 3.0000, loss = 0.0000\n",
      "step 1690: x = 3.0000, loss = 0.0000\n",
      "step 1695: x = 3.0000, loss = 0.0000\n",
      "step 1700: x = 3.0000, loss = 0.0000\n",
      "step 1705: x = 3.0000, loss = 0.0000\n",
      "step 1710: x = 3.0000, loss = 0.0000\n",
      "step 1715: x = 3.0000, loss = 0.0000\n",
      "step 1720: x = 3.0000, loss = 0.0000\n",
      "step 1725: x = 3.0000, loss = 0.0000\n",
      "step 1730: x = 3.0000, loss = 0.0000\n",
      "step 1735: x = 3.0000, loss = 0.0000\n",
      "step 1740: x = 3.0000, loss = 0.0000\n",
      "step 1745: x = 3.0000, loss = 0.0000\n",
      "step 1750: x = 3.0000, loss = 0.0000\n",
      "step 1755: x = 3.0000, loss = 0.0000\n",
      "step 1760: x = 3.0000, loss = 0.0000\n",
      "step 1765: x = 3.0000, loss = 0.0000\n",
      "step 1770: x = 3.0000, loss = 0.0000\n",
      "step 1775: x = 3.0000, loss = 0.0000\n",
      "step 1780: x = 3.0000, loss = 0.0000\n",
      "step 1785: x = 3.0000, loss = 0.0000\n",
      "step 1790: x = 3.0000, loss = 0.0000\n",
      "step 1795: x = 3.0000, loss = 0.0000\n",
      "step 1800: x = 3.0000, loss = 0.0000\n",
      "step 1805: x = 3.0000, loss = 0.0000\n",
      "step 1810: x = 3.0000, loss = 0.0000\n",
      "step 1815: x = 3.0000, loss = 0.0000\n",
      "step 1820: x = 3.0000, loss = 0.0000\n",
      "step 1825: x = 3.0000, loss = 0.0000\n",
      "step 1830: x = 3.0000, loss = 0.0000\n",
      "step 1835: x = 3.0000, loss = 0.0000\n",
      "step 1840: x = 3.0000, loss = 0.0000\n",
      "step 1845: x = 3.0000, loss = 0.0000\n",
      "step 1850: x = 3.0000, loss = 0.0000\n",
      "step 1855: x = 3.0000, loss = 0.0000\n",
      "step 1860: x = 3.0000, loss = 0.0000\n",
      "step 1865: x = 3.0000, loss = 0.0000\n",
      "step 1870: x = 3.0000, loss = 0.0000\n",
      "step 1875: x = 3.0000, loss = 0.0000\n",
      "step 1880: x = 3.0000, loss = 0.0000\n",
      "step 1885: x = 3.0000, loss = 0.0000\n",
      "step 1890: x = 3.0000, loss = 0.0000\n",
      "step 1895: x = 3.0000, loss = 0.0000\n",
      "step 1900: x = 3.0000, loss = 0.0000\n",
      "step 1905: x = 3.0000, loss = 0.0000\n",
      "step 1910: x = 3.0000, loss = 0.0000\n",
      "step 1915: x = 3.0000, loss = 0.0000\n",
      "step 1920: x = 3.0000, loss = 0.0000\n",
      "step 1925: x = 3.0000, loss = 0.0000\n",
      "step 1930: x = 3.0000, loss = 0.0000\n",
      "step 1935: x = 3.0000, loss = 0.0000\n",
      "step 1940: x = 3.0000, loss = 0.0000\n",
      "step 1945: x = 3.0000, loss = 0.0000\n",
      "step 1950: x = 3.0000, loss = 0.0000\n",
      "step 1955: x = 3.0000, loss = 0.0000\n",
      "step 1960: x = 3.0000, loss = 0.0000\n",
      "step 1965: x = 3.0000, loss = 0.0000\n",
      "step 1970: x = 3.0000, loss = 0.0000\n",
      "step 1975: x = 3.0000, loss = 0.0000\n",
      "step 1980: x = 3.0000, loss = 0.0000\n",
      "step 1985: x = 3.0000, loss = 0.0000\n",
      "step 1990: x = 3.0000, loss = 0.0000\n",
      "step 1995: x = 3.0000, loss = 0.0000\n",
      "\n",
      "Final x: 3.0000 (should be close to 3.0)\n"
     ]
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return (x - 3.0) ** 2\n",
    "\n",
    "grad_loss = grad(loss)\n",
    "\n",
    "x = 0.0\n",
    "lr = 0.01\n",
    "\n",
    "for i in range(2000):\n",
    "    x = x - lr * grad_loss(x)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"step {i:2d}: x = {x:.4f}, loss = {loss(x):.4f}\")\n",
    "\n",
    "print(f\"\\nFinal x: {x:.4f} (should be close to 3.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7a — Linear Regression with Gradient Descent\n",
    "\n",
    "This is the \"real\" version of what you just saw. Instead of optimizing a single number, you're optimizing a **vector of weights** `w` to fit a linear model `y = Xw`.\n",
    "\n",
    "The loss function is the **mean squared error**: `f(w) = ||Xw - y||^2`\n",
    "\n",
    "Steps:\n",
    "- Generate random `X` (20x3) and `w_true` (3,), compute `y = X @ w_true`\n",
    "- Start from random `w`, run gradient descent to recover `w_true`\n",
    "- `grad` handles the vector calculus for you — it returns a gradient with the same shape as `w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[ 0.60576403  0.7990441  -0.908927  ]\n",
      " [-0.63525754 -1.2226585  -0.83226097]\n",
      " [-0.47417238 -1.2504351  -0.17678244]\n",
      " [-0.04917514 -0.41177532 -0.39363015]\n",
      " [ 1.3116323   0.21555556  0.41164538]\n",
      " [-0.28955024 -0.96516913  0.4492738 ]\n",
      " [-0.4404279  -0.9279748  -0.27167085]\n",
      " [ 0.6813305  -0.48514158  0.5080369 ]\n",
      " [-0.14290465  0.08314767  0.7378238 ]\n",
      " [ 1.2528664   0.8052585  -1.1102904 ]\n",
      " [-0.68016213 -1.5656278  -0.30395982]\n",
      " [ 0.3939658  -0.87947404 -2.4562614 ]\n",
      " [ 0.93186224 -0.94133425  0.4705041 ]\n",
      " [-0.5682559  -0.03568491  0.6551425 ]\n",
      " [-0.6771544  -0.22431315 -0.98438984]\n",
      " [-2.0855458   1.8307798   0.37856096]\n",
      " [ 0.8829633  -0.2824106  -0.27737394]\n",
      " [ 1.1645578   0.21666679 -0.38737997]\n",
      " [-0.45600188  1.1401342  -0.56761235]\n",
      " [ 0.7668255  -0.96149266  0.02561626]]\n",
      "w_true:\n",
      "[-0.21089035 -1.3627948  -0.04500385]\n",
      "y:\n",
      "[-1.1757777   1.8376572   1.8120407   0.5892507  -0.5888942   1.3561717\n",
      "  1.3697474   0.49459875 -0.11638091 -1.3116522   2.2907484   1.2260002\n",
      "  1.06515     0.13898697  0.49279946 -2.0721922   0.21114212 -0.5234328\n",
      " -1.4320577   1.1474482 ]\n",
      "step  0: w = [-0.00088966 -0.02219324 -0.00279509], loss = 1.4782171249389648\n",
      "step  5: w = [-0.00571141 -0.12774374 -0.01566323], loss = 1.2533844709396362\n",
      "step 10: w = [-0.01106317 -0.22481343 -0.02683333], loss = 1.0633506774902344\n",
      "step 15: w = [-0.01683181 -0.31409633 -0.03649034], loss = 0.902643620967865\n",
      "step 20: w = [-0.02291877 -0.39622876 -0.04480066], loss = 0.7666628360748291\n",
      "step 25: w = [-0.02923849 -0.47179416 -0.05191391], loss = 0.6515409350395203\n",
      "step 30: w = [-0.03571691 -0.5413276  -0.05796458], loss = 0.5540241599082947\n",
      "step 35: w = [-0.04229026 -0.6053198  -0.06307346], loss = 0.47137394547462463\n",
      "step 40: w = [-0.04890383 -0.6642209  -0.06734898], loss = 0.4012843072414398\n",
      "step 45: w = [-0.05551092 -0.71844393 -0.07088839], loss = 0.3418126106262207\n",
      "step 50: w = [-0.06207199 -0.76836765 -0.07377888], loss = 0.29132184386253357\n",
      "step 55: w = [-0.06855375 -0.81433976 -0.07609855], loss = 0.24843107163906097\n",
      "step 60: w = [-0.07492846 -0.85667926 -0.0779173 ], loss = 0.21197549998760223\n",
      "step 65: w = [-0.0811732  -0.89567894 -0.07929762], loss = 0.1809719204902649\n",
      "step 70: w = [-0.08726934 -0.9316077  -0.08029536], loss = 0.15458963811397552\n",
      "step 75: w = [-0.09320199 -0.9647123  -0.08096036], loss = 0.13212697207927704\n",
      "step 80: w = [-0.09895948 -0.9952192  -0.08133703], loss = 0.11299077421426773\n",
      "step 85: w = [-0.10453304 -1.0233364  -0.08146498], loss = 0.09667900949716568\n",
      "step 90: w = [-0.1099163  -1.0492553  -0.08137938], loss = 0.0827668234705925\n",
      "step 95: w = [-0.11510506 -1.073151   -0.0811115 ], loss = 0.07089464366436005\n",
      "step 100: w = [-0.12009692 -1.095185   -0.08068907], loss = 0.06075745448470116\n",
      "step 105: w = [-0.12489107 -1.1155055  -0.08013667], loss = 0.05209700018167496\n",
      "step 110: w = [-0.129488   -1.1342483  -0.07947601], loss = 0.04469400271773338\n",
      "step 115: w = [-0.1338894  -1.1515386  -0.07872622], loss = 0.03836241737008095\n",
      "step 120: w = [-0.13809785 -1.1674914  -0.07790422], loss = 0.032944247126579285\n",
      "step 125: w = [-0.14211673 -1.1822124  -0.07702482], loss = 0.02830524556338787\n",
      "step 130: w = [-0.14595006 -1.1957989  -0.07610103], loss = 0.02433115802705288\n",
      "step 135: w = [-0.14960243 -1.20834    -0.07514418], loss = 0.020924946293234825\n",
      "step 140: w = [-0.1530788  -1.2199183  -0.07416419], loss = 0.01800393871963024\n",
      "step 145: w = [-0.15638447 -1.2306088  -0.07316965], loss = 0.015497765503823757\n",
      "step 150: w = [-0.159525   -1.2404815  -0.07216798], loss = 0.013346395455300808\n",
      "step 155: w = [-0.16250612 -1.2496003  -0.07116558], loss = 0.01149867195636034\n",
      "step 160: w = [-0.16533363 -1.258024   -0.07016794], loss = 0.009910988621413708\n",
      "step 165: w = [-0.16801345 -1.2658066  -0.0691797 ], loss = 0.008546113967895508\n",
      "step 170: w = [-0.1705515  -1.272998   -0.06820481], loss = 0.007372215390205383\n",
      "step 175: w = [-0.1729536  -1.2796444  -0.06724657], loss = 0.006362080108374357\n",
      "step 180: w = [-0.1752256  -1.2857877  -0.06630771], loss = 0.005492504220455885\n",
      "step 185: w = [-0.17737326 -1.2914672  -0.06539045], loss = 0.004743561614304781\n",
      "step 190: w = [-0.17940219 -1.2967184  -0.0644966 ], loss = 0.004098270554095507\n",
      "step 195: w = [-0.1813179  -1.3015742  -0.06362756], loss = 0.003542054910212755\n",
      "step 200: w = [-0.18312576 -1.3060653  -0.06278441], loss = 0.0030624084174633026\n",
      "step 205: w = [-0.18483101 -1.3102195  -0.06196793], loss = 0.002648627618327737\n",
      "step 210: w = [-0.18643868 -1.3140627  -0.06117864], loss = 0.002291532000526786\n",
      "step 215: w = [-0.18795365 -1.317619   -0.06041684], loss = 0.0019832116086035967\n",
      "step 220: w = [-0.18938068 -1.32091    -0.05968264], loss = 0.0017169226193800569\n",
      "step 225: w = [-0.19072427 -1.3239561  -0.058976  ], loss = 0.001486839959397912\n",
      "step 230: w = [-0.19198881 -1.3267758  -0.05829672], loss = 0.0012879838468506932\n",
      "step 235: w = [-0.19317849 -1.3293865  -0.05764448], loss = 0.0011160433059558272\n",
      "step 240: w = [-0.1942973  -1.3318037  -0.05701887], loss = 0.0009673374588601291\n",
      "step 245: w = [-0.1953491  -1.3340423  -0.05641939], loss = 0.0008386755362153053\n",
      "step 250: w = [-0.19633758 -1.3361157  -0.05584549], loss = 0.0007273252704180777\n",
      "step 255: w = [-0.19726622 -1.3380364  -0.05529653], loss = 0.0006309249438345432\n",
      "step 260: w = [-0.19813839 -1.3398159  -0.05477187], loss = 0.0005474438657984138\n",
      "step 265: w = [-0.19895725 -1.3414648  -0.05427079], loss = 0.000475125212687999\n",
      "step 270: w = [-0.19972584 -1.3429927  -0.05379257], loss = 0.00041246606269851327\n",
      "step 275: w = [-0.20044704 -1.3444091  -0.05333648], loss = 0.000358146004145965\n",
      "step 280: w = [-0.20112358 -1.345722   -0.05290175], loss = 0.00031105769448913634\n",
      "step 285: w = [-0.20175806 -1.3469392  -0.05248763], loss = 0.00027021855930797756\n",
      "step 290: w = [-0.20235293 -1.3480679  -0.05209337], loss = 0.0002347925037611276\n",
      "step 295: w = [-0.20291051 -1.3491144  -0.05171819], loss = 0.0002040572726400569\n",
      "step 300: w = [-0.20343304 -1.3500851  -0.05136136], loss = 0.00017738206952344626\n",
      "step 305: w = [-0.20392258 -1.3509855  -0.05102212], loss = 0.00015422631986439228\n",
      "step 310: w = [-0.20438112 -1.3518208  -0.05069977], loss = 0.0001341203460469842\n",
      "step 315: w = [-0.20481053 -1.352596   -0.05039357], loss = 0.0001166555448435247\n",
      "step 320: w = [-0.20521255 -1.3533154  -0.05010284], loss = 0.00010148554429179057\n",
      "step 325: w = [-0.20558886 -1.353983   -0.04982689], loss = 8.830326260067523e-05\n",
      "step 330: w = [-0.20594104 -1.3546027  -0.04956506], loss = 7.684822048759088e-05\n",
      "step 335: w = [-0.20627056 -1.355178   -0.04931672], loss = 6.689022848149762e-05\n",
      "step 340: w = [-0.20657884 -1.3557122  -0.04908124], loss = 5.823226092616096e-05\n",
      "step 345: w = [-0.20686716 -1.3562081  -0.04885802], loss = 5.070469342172146e-05\n",
      "step 350: w = [-0.20713678 -1.3566686  -0.0486465 ], loss = 4.415765215526335e-05\n",
      "step 355: w = [-0.20738886 -1.3570963  -0.0484461 ], loss = 3.846233812510036e-05\n",
      "step 360: w = [-0.20762451 -1.3574935  -0.04825629], loss = 3.350788028910756e-05\n",
      "step 365: w = [-0.20784473 -1.3578626  -0.04807657], loss = 2.91956530418247e-05\n",
      "step 370: w = [-0.20805053 -1.3582056  -0.04790642], loss = 2.5442399419262074e-05\n",
      "step 375: w = [-0.2082428  -1.3585241  -0.04774538], loss = 2.2175812773639336e-05\n",
      "step 380: w = [-0.20842241 -1.3588201  -0.047593  ], loss = 1.9331651856191456e-05\n",
      "step 385: w = [-0.20859015 -1.3590952  -0.04744883], loss = 1.6854804925969802e-05\n",
      "step 390: w = [-0.2087468  -1.359351   -0.04731245], loss = 1.4697196093038656e-05\n",
      "step 395: w = [-0.20889308 -1.3595889  -0.04718347], loss = 1.2817569768230896e-05\n",
      "step 400: w = [-0.20902961 -1.35981    -0.04706152], loss = 1.1180098226759583e-05\n",
      "step 405: w = [-0.20915706 -1.3600156  -0.04694622], loss = 9.752910955285188e-06\n",
      "step 410: w = [-0.209276   -1.3602068  -0.04683723], loss = 8.509292456437834e-06\n",
      "step 415: w = [-0.20938699 -1.3603847  -0.04673423], loss = 7.42532211006619e-06\n",
      "step 420: w = [-0.20949054 -1.36055    -0.04663688], loss = 6.480412594100926e-06\n",
      "step 425: w = [-0.20958713 -1.360704   -0.04654491], loss = 5.656574103340972e-06\n",
      "step 430: w = [-0.20967722 -1.3608471  -0.04645802], loss = 4.938020083500305e-06\n",
      "step 435: w = [-0.20976125 -1.3609803  -0.04637593], loss = 4.311373231757898e-06\n",
      "step 440: w = [-0.20983958 -1.3611042  -0.04629841], loss = 3.764741222767043e-06\n",
      "step 445: w = [-0.20991263 -1.3612195  -0.04622519], loss = 3.2880100206966745e-06\n",
      "step 450: w = [-0.20998073 -1.3613268  -0.04615606], loss = 2.8720760383293964e-06\n",
      "step 455: w = [-0.2100442  -1.3614267  -0.04609078], loss = 2.5089846076298272e-06\n",
      "step 460: w = [-0.21010336 -1.3615198  -0.04602915], loss = 2.191902240156196e-06\n",
      "step 465: w = [-0.21015848 -1.3616062  -0.04597097], loss = 1.915655957418494e-06\n",
      "step 470: w = [-0.21020983 -1.3616867  -0.04591605], loss = 1.6743696278354037e-06\n",
      "step 475: w = [-0.21025766 -1.3617618  -0.04586421], loss = 1.4634350691267173e-06\n",
      "step 480: w = [-0.2103022  -1.3618318  -0.04581529], loss = 1.2792394272764795e-06\n",
      "step 485: w = [-0.2103437  -1.3618969  -0.04576911], loss = 1.118470549954509e-06\n",
      "step 490: w = [-0.21038234 -1.3619576  -0.04572554], loss = 9.779550964594819e-07\n",
      "step 495: w = [-0.21041833 -1.3620139  -0.04568443], loss = 8.55334860716539e-07\n",
      "step 500: w = [-0.21045181 -1.3620666  -0.04564565], loss = 7.480426802430884e-07\n",
      "step 505: w = [-0.21048295 -1.3621156  -0.04560905], loss = 6.544298685184913e-07\n",
      "step 510: w = [-0.21051197 -1.3621614  -0.04557452], loss = 5.724683660446317e-07\n",
      "step 515: w = [-0.21053897 -1.3622037  -0.04554195], loss = 5.011162897972099e-07\n",
      "step 520: w = [-0.21056409 -1.3622434  -0.04551123], loss = 4.385454417388246e-07\n",
      "step 525: w = [-0.21058747 -1.3622804  -0.04548226], loss = 3.838391933186358e-07\n",
      "step 530: w = [-0.21060921 -1.3623148  -0.04545492], loss = 3.360236462413013e-07\n",
      "step 535: w = [-0.21062942 -1.3623469  -0.04542914], loss = 2.942037724551483e-07\n",
      "step 540: w = [-0.21064822 -1.3623769  -0.04540483], loss = 2.5755718979780795e-07\n",
      "step 545: w = [-0.21066572 -1.3624048  -0.0453819 ], loss = 2.2557348700047442e-07\n",
      "step 550: w = [-0.21068197 -1.3624308  -0.04536027], loss = 1.9758222435939388e-07\n",
      "step 555: w = [-0.2106971  -1.3624551  -0.04533987], loss = 1.7305235644471395e-07\n",
      "step 560: w = [-0.21071114 -1.3624777  -0.04532065], loss = 1.5165473143952113e-07\n",
      "step 565: w = [-0.21072419 -1.3624986  -0.04530252], loss = 1.3293514200540812e-07\n",
      "step 570: w = [-0.21073632 -1.3625183  -0.04528542], loss = 1.165161265248571e-07\n",
      "step 575: w = [-0.21074758 -1.3625367  -0.0452693 ], loss = 1.0212152545818753e-07\n",
      "step 580: w = [-0.21075806 -1.3625538  -0.0452541 ], loss = 8.95181315740956e-08\n",
      "step 585: w = [-0.21076779 -1.3625698  -0.04523977], loss = 7.84756011285026e-08\n",
      "step 590: w = [-0.21077682 -1.3625847  -0.04522626], loss = 6.88240078261515e-08\n",
      "step 595: w = [-0.21078521 -1.3625987  -0.04521352], loss = 6.032636434838423e-08\n",
      "step 600: w = [-0.21079299 -1.3626115  -0.04520151], loss = 5.2953470941474734e-08\n",
      "step 605: w = [-0.21080022 -1.3626237  -0.04519018], loss = 4.6436223755108585e-08\n",
      "step 610: w = [-0.21080692 -1.3626349  -0.0451795 ], loss = 4.076228421467931e-08\n",
      "step 615: w = [-0.21081315 -1.3626454  -0.04516944], loss = 3.5781155816039245e-08\n",
      "step 620: w = [-0.21081893 -1.3626552  -0.04515996], loss = 3.142310589510089e-08\n",
      "step 625: w = [-0.2108243  -1.3626643  -0.04515101], loss = 2.758154415971603e-08\n",
      "step 630: w = [-0.21082929 -1.3626729  -0.04514258], loss = 2.4212027938119718e-08\n",
      "step 635: w = [-0.2108339  -1.3626809  -0.04513463], loss = 2.1262229310536895e-08\n",
      "step 640: w = [-0.21083817 -1.3626884  -0.04512713], loss = 1.8658511180547066e-08\n",
      "step 645: w = [-0.21084213 -1.3626955  -0.04512007], loss = 1.6385655499107088e-08\n",
      "step 650: w = [-0.21084581 -1.362702   -0.04511341], loss = 1.4386327684690059e-08\n",
      "step 655: w = [-0.21084921 -1.362708   -0.04510713], loss = 1.2654640890730207e-08\n",
      "step 660: w = [-0.21085237 -1.3627137  -0.04510122], loss = 1.11148645842718e-08\n",
      "step 665: w = [-0.21085529 -1.362719   -0.04509564], loss = 9.762485930764342e-09\n",
      "step 670: w = [-0.21085799 -1.362724   -0.04509038], loss = 8.587958788552896e-09\n",
      "step 675: w = [-0.2108605  -1.3627287  -0.04508542], loss = 7.539595614503014e-09\n",
      "step 680: w = [-0.21086283 -1.3627329  -0.04508075], loss = 6.638980476481038e-09\n",
      "step 685: w = [-0.21086498 -1.362737   -0.04507634], loss = 5.83146109178756e-09\n",
      "step 690: w = [-0.21086697 -1.3627406  -0.04507219], loss = 5.136168379493711e-09\n",
      "step 695: w = [-0.2108688  -1.3627442  -0.04506828], loss = 4.517523688463143e-09\n",
      "step 700: w = [-0.2108705  -1.3627474  -0.04506459], loss = 3.9782648286745825e-09\n",
      "step 705: w = [-0.21087207 -1.3627504  -0.04506111], loss = 3.5078664417653727e-09\n",
      "step 710: w = [-0.21087353 -1.3627534  -0.04505783], loss = 3.080769861441013e-09\n",
      "step 715: w = [-0.21087489 -1.362756   -0.04505474], loss = 2.715595748270516e-09\n",
      "step 720: w = [-0.21087612 -1.3627584  -0.04505183], loss = 2.396349119138108e-09\n",
      "step 725: w = [-0.21087727 -1.3627608  -0.04504909], loss = 2.1100448055477727e-09\n",
      "step 730: w = [-0.21087833 -1.3627632  -0.0450465 ], loss = 1.8499527465465349e-09\n",
      "step 735: w = [-0.21087931 -1.3627651  -0.04504406], loss = 1.6331143104508783e-09\n",
      "step 740: w = [-0.21088022 -1.3627669  -0.04504176], loss = 1.4455355801246128e-09\n",
      "step 745: w = [-0.21088105 -1.3627687  -0.04503959], loss = 1.2728593734578908e-09\n",
      "step 750: w = [-0.21088183 -1.3627704  -0.04503755], loss = 1.118332426663926e-09\n",
      "step 755: w = [-0.21088254 -1.3627722  -0.04503563], loss = 9.786326193861328e-10\n",
      "step 760: w = [-0.21088322 -1.3627734  -0.04503381], loss = 8.696714459688337e-10\n",
      "step 765: w = [-0.21088381 -1.3627746  -0.0450321 ], loss = 7.71816777156431e-10\n",
      "step 770: w = [-0.21088436 -1.3627758  -0.04503049], loss = 6.84391043836996e-10\n",
      "step 775: w = [-0.21088488 -1.362777   -0.04502897], loss = 6.038139432895662e-10\n",
      "step 780: w = [-0.21088535 -1.3627782  -0.04502754], loss = 5.309295225686128e-10\n",
      "step 785: w = [-0.2108858  -1.3627794  -0.04502618], loss = 4.655782148699217e-10\n",
      "step 790: w = [-0.21088618 -1.3627806  -0.04502491], loss = 4.074275639087688e-10\n",
      "step 795: w = [-0.21088655 -1.3627815  -0.04502371], loss = 3.5810457377216665e-10\n",
      "step 800: w = [-0.2108869  -1.3627821  -0.04502257], loss = 3.207661081194857e-10\n",
      "step 805: w = [-0.2108872 -1.3627827 -0.0450215], loss = 2.8615065872372725e-10\n",
      "step 810: w = [-0.21088749 -1.3627833  -0.0450205 ], loss = 2.555620992605867e-10\n",
      "step 815: w = [-0.21088775 -1.3627839  -0.04501956], loss = 2.2824404866117476e-10\n",
      "step 820: w = [-0.21088797 -1.3627845  -0.04501867], loss = 2.021319778222619e-10\n",
      "step 825: w = [-0.21088819 -1.3627851  -0.04501782], loss = 1.7984225220146755e-10\n",
      "step 830: w = [-0.21088842 -1.3627857  -0.04501704], loss = 1.588409681563263e-10\n",
      "step 835: w = [-0.2108886  -1.3627863  -0.04501629], loss = 1.4032240647221528e-10\n",
      "step 840: w = [-0.21088874 -1.3627869  -0.04501558], loss = 1.2380452218963e-10\n",
      "step 845: w = [-0.21088889 -1.3627875  -0.04501492], loss = 1.0838168557070205e-10\n",
      "step 850: w = [-0.21088904 -1.3627881  -0.04501429], loss = 9.430489028572353e-11\n",
      "step 855: w = [-0.21088919 -1.3627887  -0.04501369], loss = 8.214612323698134e-11\n",
      "step 860: w = [-0.21088934 -1.3627893  -0.04501313], loss = 7.069455437713756e-11\n",
      "step 865: w = [-0.21088946 -1.3627899  -0.0450126 ], loss = 6.094761501040935e-11\n",
      "step 870: w = [-0.21088953 -1.3627901  -0.0450121 ], loss = 5.4538373817081265e-11\n",
      "step 875: w = [-0.21088961 -1.3627902  -0.04501162], loss = 4.937165259955023e-11\n",
      "step 880: w = [-0.21088968 -1.3627902  -0.04501119], loss = 4.486772064993261e-11\n",
      "step 885: w = [-0.21088976 -1.3627903  -0.04501077], loss = 4.101307488069139e-11\n",
      "step 890: w = [-0.21088983 -1.3627903  -0.04501038], loss = 3.774569545811346e-11\n",
      "step 895: w = [-0.2108899  -1.3627903  -0.04501001], loss = 3.5108419643714583e-11\n",
      "step 900: w = [-0.21088998 -1.3627905  -0.04500967], loss = 3.214582419697187e-11\n",
      "step 905: w = [-0.21089005 -1.3627905  -0.04500935], loss = 2.988847017104668e-11\n",
      "step 910: w = [-0.2108901  -1.3627905  -0.04500905], loss = 2.7935945223767078e-11\n",
      "step 915: w = [-0.2108901  -1.3627906  -0.04500876], loss = 2.562542365802667e-11\n",
      "step 920: w = [-0.2108901  -1.3627906  -0.04500849], loss = 2.445288763142095e-11\n",
      "step 925: w = [-0.2108901  -1.3627906  -0.04500824], loss = 2.3024129477966504e-11\n",
      "step 930: w = [-0.2108901  -1.3627906  -0.04500801], loss = 2.195326385956431e-11\n",
      "step 935: w = [-0.2108901  -1.3627907  -0.04500778], loss = 2.034777901727125e-11\n",
      "step 940: w = [-0.2108901  -1.3627907  -0.04500758], loss = 1.936180209216598e-11\n",
      "step 945: w = [-0.2108901  -1.3627907  -0.04500739], loss = 1.8589832798121542e-11\n",
      "step 950: w = [-0.2108901 -1.3627907 -0.0450072], loss = 1.793150176954139e-11\n",
      "step 955: w = [-0.2108901  -1.3627907  -0.04500703], loss = 1.744032696038289e-11\n",
      "step 960: w = [-0.2108901  -1.3627908  -0.04500687], loss = 1.6150114232060808e-11\n",
      "step 965: w = [-0.2108901  -1.3627908  -0.04500672], loss = 1.5638670913808994e-11\n",
      "step 970: w = [-0.2108901  -1.3627908  -0.04500658], loss = 1.534138614756042e-11\n",
      "step 975: w = [-0.2108901  -1.3627908  -0.04500645], loss = 1.4905533404774296e-11\n",
      "step 980: w = [-0.2108901  -1.3627908  -0.04500632], loss = 1.452193140044633e-11\n",
      "step 985: w = [-0.2108901 -1.362791  -0.0450062], loss = 1.3677315356674935e-11\n",
      "step 990: w = [-0.2108901  -1.362791   -0.04500609], loss = 1.3262427614457728e-11\n",
      "step 995: w = [-0.2108901  -1.362791   -0.04500598], loss = 1.3191406300627762e-11\n",
      "step 1000: w = [-0.2108901  -1.362791   -0.04500589], loss = 1.2943116196872939e-11\n",
      "step 1005: w = [-0.2108901  -1.362791   -0.04500579], loss = 1.2801437861142961e-11\n",
      "step 1010: w = [-0.2108901 -1.362791  -0.0450057], loss = 1.263618376601272e-11\n",
      "step 1015: w = [-0.2108901  -1.362791   -0.04500563], loss = 1.245041830050253e-11\n",
      "step 1020: w = [-0.2108901  -1.362791   -0.04500555], loss = 1.2284550980623532e-11\n",
      "step 1025: w = [-0.2108901  -1.362791   -0.04500548], loss = 1.2363446204310957e-11\n",
      "step 1030: w = [-0.2108901  -1.362791   -0.04500541], loss = 1.2194733937931357e-11\n",
      "step 1035: w = [-0.2108901  -1.362791   -0.04500535], loss = 1.2141876913618344e-11\n",
      "step 1040: w = [-0.2108901 -1.362791  -0.0450053], loss = 1.2162240832502835e-11\n",
      "step 1045: w = [-0.2108901  -1.362791   -0.04500524], loss = 1.196619799775922e-11\n",
      "step 1050: w = [-0.2108901  -1.362791   -0.04500518], loss = 1.1992462578547247e-11\n",
      "step 1055: w = [-0.2108901  -1.3627911  -0.04500513], loss = 1.116903531467317e-11\n",
      "step 1060: w = [-0.2108901  -1.3627911  -0.04500508], loss = 1.1066314531404942e-11\n",
      "step 1065: w = [-0.2108901  -1.3627911  -0.04500504], loss = 1.1000237179481509e-11\n",
      "step 1070: w = [-0.2108901 -1.3627911 -0.045005 ], loss = 1.0940296411854344e-11\n",
      "step 1075: w = [-0.2108901  -1.3627911  -0.04500497], loss = 1.0880721670880611e-11\n",
      "step 1080: w = [-0.2108901  -1.3627911  -0.04500493], loss = 1.0983158826860517e-11\n",
      "step 1085: w = [-0.2108901  -1.3627911  -0.04500489], loss = 1.0963785435080808e-11\n",
      "step 1090: w = [-0.2108901  -1.3627911  -0.04500486], loss = 1.0868384317519464e-11\n",
      "step 1095: w = [-0.2108901  -1.3627911  -0.04500482], loss = 1.070844801720483e-11\n",
      "step 1100: w = [-0.2108901  -1.3627911  -0.04500479], loss = 1.0769530232879188e-11\n",
      "step 1105: w = [-0.2108901  -1.3627911  -0.04500477], loss = 1.0761713568896436e-11\n",
      "step 1110: w = [-0.2108901  -1.3627911  -0.04500475], loss = 1.0759804505711124e-11\n",
      "step 1115: w = [-0.2108901  -1.3627911  -0.04500473], loss = 1.0769852024083981e-11\n",
      "step 1120: w = [-0.2108901  -1.3627911  -0.04500471], loss = 1.0859468706214681e-11\n",
      "step 1125: w = [-0.2108901 -1.3627911 -0.0450047], loss = 1.0954648646532839e-11\n",
      "step 1130: w = [-0.2108901  -1.3627911  -0.04500468], loss = 1.0946246513376945e-11\n",
      "step 1135: w = [-0.2108901  -1.3627911  -0.04500466], loss = 1.09385304633558e-11\n",
      "step 1140: w = [-0.2108901  -1.3627911  -0.04500464], loss = 1.1005910592609691e-11\n",
      "step 1145: w = [-0.2108901  -1.3627911  -0.04500462], loss = 1.0951886966759083e-11\n",
      "step 1150: w = [-0.2108901 -1.3627911 -0.0450046], loss = 1.0916947901229435e-11\n",
      "step 1155: w = [-0.2108901  -1.3627911  -0.04500458], loss = 1.0777193373834315e-11\n",
      "step 1160: w = [-0.2108901  -1.3627911  -0.04500457], loss = 1.078157615269637e-11\n",
      "step 1165: w = [-0.2108901  -1.3627911  -0.04500455], loss = 1.0797363003689497e-11\n",
      "step 1170: w = [-0.2108901  -1.3627911  -0.04500453], loss = 1.0791124070708147e-11\n",
      "step 1175: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0831702722258196e-11\n",
      "step 1180: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1185: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1190: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1195: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1200: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1205: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1210: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1215: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1220: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1225: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1230: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1235: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1240: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1245: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1250: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1255: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1260: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1265: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1270: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1275: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1280: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1285: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1290: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1295: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1300: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1305: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1310: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1315: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1320: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1325: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1330: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1335: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1340: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1345: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1350: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1355: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1360: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1365: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1370: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1375: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1380: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1385: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1390: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1395: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1400: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1405: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1410: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1415: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1420: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1425: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1430: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1435: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1440: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1445: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1450: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1455: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1460: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1465: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1470: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1475: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1480: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1485: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1490: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1495: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1500: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1505: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1510: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1515: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1520: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1525: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1530: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1535: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1540: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1545: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1550: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1555: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1560: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1565: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1570: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1575: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1580: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1585: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1590: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1595: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1600: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1605: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1610: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1615: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1620: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1625: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1630: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1635: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1640: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1645: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1650: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1655: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1660: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1665: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1670: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1675: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1680: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1685: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1690: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1695: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1700: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1705: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1710: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1715: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1720: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1725: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1730: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1735: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1740: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1745: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1750: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1755: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1760: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1765: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1770: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1775: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1780: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1785: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1790: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1795: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1800: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1805: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1810: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1815: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1820: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1825: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1830: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1835: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1840: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1845: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1850: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1855: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1860: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1865: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1870: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1875: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1880: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1885: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1890: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1895: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1900: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1905: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1910: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1915: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1920: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1925: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1930: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1935: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1940: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1945: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1950: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1955: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1960: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1965: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1970: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1975: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1980: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1985: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1990: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1995: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step  0: w = [-0.00088966 -0.02219324 -0.00279509], loss = 1.4782171249389648\n",
      "step  5: w = [-0.00571141 -0.12774374 -0.01566323], loss = 1.2533844709396362\n",
      "step 10: w = [-0.01106317 -0.22481343 -0.02683333], loss = 1.0633506774902344\n",
      "step 15: w = [-0.01683181 -0.31409633 -0.03649034], loss = 0.902643620967865\n",
      "step 20: w = [-0.02291877 -0.39622876 -0.04480066], loss = 0.7666628360748291\n",
      "step 25: w = [-0.02923849 -0.47179416 -0.05191391], loss = 0.6515409350395203\n",
      "step 30: w = [-0.03571691 -0.5413276  -0.05796458], loss = 0.5540241599082947\n",
      "step 35: w = [-0.04229026 -0.6053198  -0.06307346], loss = 0.47137394547462463\n",
      "step 40: w = [-0.04890383 -0.6642209  -0.06734898], loss = 0.4012843072414398\n",
      "step 45: w = [-0.05551092 -0.71844393 -0.07088839], loss = 0.3418126106262207\n",
      "step 50: w = [-0.06207199 -0.76836765 -0.07377888], loss = 0.29132184386253357\n",
      "step 55: w = [-0.06855375 -0.81433976 -0.07609855], loss = 0.24843107163906097\n",
      "step 60: w = [-0.07492846 -0.85667926 -0.0779173 ], loss = 0.21197549998760223\n",
      "step 65: w = [-0.0811732  -0.89567894 -0.07929762], loss = 0.1809719204902649\n",
      "step 70: w = [-0.08726934 -0.9316077  -0.08029536], loss = 0.15458963811397552\n",
      "step 75: w = [-0.09320199 -0.9647123  -0.08096036], loss = 0.13212697207927704\n",
      "step 80: w = [-0.09895948 -0.9952192  -0.08133703], loss = 0.11299077421426773\n",
      "step 85: w = [-0.10453304 -1.0233364  -0.08146498], loss = 0.09667900949716568\n",
      "step 90: w = [-0.1099163  -1.0492553  -0.08137938], loss = 0.0827668234705925\n",
      "step 95: w = [-0.11510506 -1.073151   -0.0811115 ], loss = 0.07089464366436005\n",
      "step 100: w = [-0.12009692 -1.095185   -0.08068907], loss = 0.06075745448470116\n",
      "step 105: w = [-0.12489107 -1.1155055  -0.08013667], loss = 0.05209700018167496\n",
      "step 110: w = [-0.129488   -1.1342483  -0.07947601], loss = 0.04469400271773338\n",
      "step 115: w = [-0.1338894  -1.1515386  -0.07872622], loss = 0.03836241737008095\n",
      "step 120: w = [-0.13809785 -1.1674914  -0.07790422], loss = 0.032944247126579285\n",
      "step 125: w = [-0.14211673 -1.1822124  -0.07702482], loss = 0.02830524556338787\n",
      "step 130: w = [-0.14595006 -1.1957989  -0.07610103], loss = 0.02433115802705288\n",
      "step 135: w = [-0.14960243 -1.20834    -0.07514418], loss = 0.020924946293234825\n",
      "step 140: w = [-0.1530788  -1.2199183  -0.07416419], loss = 0.01800393871963024\n",
      "step 145: w = [-0.15638447 -1.2306088  -0.07316965], loss = 0.015497765503823757\n",
      "step 150: w = [-0.159525   -1.2404815  -0.07216798], loss = 0.013346395455300808\n",
      "step 155: w = [-0.16250612 -1.2496003  -0.07116558], loss = 0.01149867195636034\n",
      "step 160: w = [-0.16533363 -1.258024   -0.07016794], loss = 0.009910988621413708\n",
      "step 165: w = [-0.16801345 -1.2658066  -0.0691797 ], loss = 0.008546113967895508\n",
      "step 170: w = [-0.1705515  -1.272998   -0.06820481], loss = 0.007372215390205383\n",
      "step 175: w = [-0.1729536  -1.2796444  -0.06724657], loss = 0.006362080108374357\n",
      "step 180: w = [-0.1752256  -1.2857877  -0.06630771], loss = 0.005492504220455885\n",
      "step 185: w = [-0.17737326 -1.2914672  -0.06539045], loss = 0.004743561614304781\n",
      "step 190: w = [-0.17940219 -1.2967184  -0.0644966 ], loss = 0.004098270554095507\n",
      "step 195: w = [-0.1813179  -1.3015742  -0.06362756], loss = 0.003542054910212755\n",
      "step 200: w = [-0.18312576 -1.3060653  -0.06278441], loss = 0.0030624084174633026\n",
      "step 205: w = [-0.18483101 -1.3102195  -0.06196793], loss = 0.002648627618327737\n",
      "step 210: w = [-0.18643868 -1.3140627  -0.06117864], loss = 0.002291532000526786\n",
      "step 215: w = [-0.18795365 -1.317619   -0.06041684], loss = 0.0019832116086035967\n",
      "step 220: w = [-0.18938068 -1.32091    -0.05968264], loss = 0.0017169226193800569\n",
      "step 225: w = [-0.19072427 -1.3239561  -0.058976  ], loss = 0.001486839959397912\n",
      "step 230: w = [-0.19198881 -1.3267758  -0.05829672], loss = 0.0012879838468506932\n",
      "step 235: w = [-0.19317849 -1.3293865  -0.05764448], loss = 0.0011160433059558272\n",
      "step 240: w = [-0.1942973  -1.3318037  -0.05701887], loss = 0.0009673374588601291\n",
      "step 245: w = [-0.1953491  -1.3340423  -0.05641939], loss = 0.0008386755362153053\n",
      "step 250: w = [-0.19633758 -1.3361157  -0.05584549], loss = 0.0007273252704180777\n",
      "step 255: w = [-0.19726622 -1.3380364  -0.05529653], loss = 0.0006309249438345432\n",
      "step 260: w = [-0.19813839 -1.3398159  -0.05477187], loss = 0.0005474438657984138\n",
      "step 265: w = [-0.19895725 -1.3414648  -0.05427079], loss = 0.000475125212687999\n",
      "step 270: w = [-0.19972584 -1.3429927  -0.05379257], loss = 0.00041246606269851327\n",
      "step 275: w = [-0.20044704 -1.3444091  -0.05333648], loss = 0.000358146004145965\n",
      "step 280: w = [-0.20112358 -1.345722   -0.05290175], loss = 0.00031105769448913634\n",
      "step 285: w = [-0.20175806 -1.3469392  -0.05248763], loss = 0.00027021855930797756\n",
      "step 290: w = [-0.20235293 -1.3480679  -0.05209337], loss = 0.0002347925037611276\n",
      "step 295: w = [-0.20291051 -1.3491144  -0.05171819], loss = 0.0002040572726400569\n",
      "step 300: w = [-0.20343304 -1.3500851  -0.05136136], loss = 0.00017738206952344626\n",
      "step 305: w = [-0.20392258 -1.3509855  -0.05102212], loss = 0.00015422631986439228\n",
      "step 310: w = [-0.20438112 -1.3518208  -0.05069977], loss = 0.0001341203460469842\n",
      "step 315: w = [-0.20481053 -1.352596   -0.05039357], loss = 0.0001166555448435247\n",
      "step 320: w = [-0.20521255 -1.3533154  -0.05010284], loss = 0.00010148554429179057\n",
      "step 325: w = [-0.20558886 -1.353983   -0.04982689], loss = 8.830326260067523e-05\n",
      "step 330: w = [-0.20594104 -1.3546027  -0.04956506], loss = 7.684822048759088e-05\n",
      "step 335: w = [-0.20627056 -1.355178   -0.04931672], loss = 6.689022848149762e-05\n",
      "step 340: w = [-0.20657884 -1.3557122  -0.04908124], loss = 5.823226092616096e-05\n",
      "step 345: w = [-0.20686716 -1.3562081  -0.04885802], loss = 5.070469342172146e-05\n",
      "step 350: w = [-0.20713678 -1.3566686  -0.0486465 ], loss = 4.415765215526335e-05\n",
      "step 355: w = [-0.20738886 -1.3570963  -0.0484461 ], loss = 3.846233812510036e-05\n",
      "step 360: w = [-0.20762451 -1.3574935  -0.04825629], loss = 3.350788028910756e-05\n",
      "step 365: w = [-0.20784473 -1.3578626  -0.04807657], loss = 2.91956530418247e-05\n",
      "step 370: w = [-0.20805053 -1.3582056  -0.04790642], loss = 2.5442399419262074e-05\n",
      "step 375: w = [-0.2082428  -1.3585241  -0.04774538], loss = 2.2175812773639336e-05\n",
      "step 380: w = [-0.20842241 -1.3588201  -0.047593  ], loss = 1.9331651856191456e-05\n",
      "step 385: w = [-0.20859015 -1.3590952  -0.04744883], loss = 1.6854804925969802e-05\n",
      "step 390: w = [-0.2087468  -1.359351   -0.04731245], loss = 1.4697196093038656e-05\n",
      "step 395: w = [-0.20889308 -1.3595889  -0.04718347], loss = 1.2817569768230896e-05\n",
      "step 400: w = [-0.20902961 -1.35981    -0.04706152], loss = 1.1180098226759583e-05\n",
      "step 405: w = [-0.20915706 -1.3600156  -0.04694622], loss = 9.752910955285188e-06\n",
      "step 410: w = [-0.209276   -1.3602068  -0.04683723], loss = 8.509292456437834e-06\n",
      "step 415: w = [-0.20938699 -1.3603847  -0.04673423], loss = 7.42532211006619e-06\n",
      "step 420: w = [-0.20949054 -1.36055    -0.04663688], loss = 6.480412594100926e-06\n",
      "step 425: w = [-0.20958713 -1.360704   -0.04654491], loss = 5.656574103340972e-06\n",
      "step 430: w = [-0.20967722 -1.3608471  -0.04645802], loss = 4.938020083500305e-06\n",
      "step 435: w = [-0.20976125 -1.3609803  -0.04637593], loss = 4.311373231757898e-06\n",
      "step 440: w = [-0.20983958 -1.3611042  -0.04629841], loss = 3.764741222767043e-06\n",
      "step 445: w = [-0.20991263 -1.3612195  -0.04622519], loss = 3.2880100206966745e-06\n",
      "step 450: w = [-0.20998073 -1.3613268  -0.04615606], loss = 2.8720760383293964e-06\n",
      "step 455: w = [-0.2100442  -1.3614267  -0.04609078], loss = 2.5089846076298272e-06\n",
      "step 460: w = [-0.21010336 -1.3615198  -0.04602915], loss = 2.191902240156196e-06\n",
      "step 465: w = [-0.21015848 -1.3616062  -0.04597097], loss = 1.915655957418494e-06\n",
      "step 470: w = [-0.21020983 -1.3616867  -0.04591605], loss = 1.6743696278354037e-06\n",
      "step 475: w = [-0.21025766 -1.3617618  -0.04586421], loss = 1.4634350691267173e-06\n",
      "step 480: w = [-0.2103022  -1.3618318  -0.04581529], loss = 1.2792394272764795e-06\n",
      "step 485: w = [-0.2103437  -1.3618969  -0.04576911], loss = 1.118470549954509e-06\n",
      "step 490: w = [-0.21038234 -1.3619576  -0.04572554], loss = 9.779550964594819e-07\n",
      "step 495: w = [-0.21041833 -1.3620139  -0.04568443], loss = 8.55334860716539e-07\n",
      "step 500: w = [-0.21045181 -1.3620666  -0.04564565], loss = 7.480426802430884e-07\n",
      "step 505: w = [-0.21048295 -1.3621156  -0.04560905], loss = 6.544298685184913e-07\n",
      "step 510: w = [-0.21051197 -1.3621614  -0.04557452], loss = 5.724683660446317e-07\n",
      "step 515: w = [-0.21053897 -1.3622037  -0.04554195], loss = 5.011162897972099e-07\n",
      "step 520: w = [-0.21056409 -1.3622434  -0.04551123], loss = 4.385454417388246e-07\n",
      "step 525: w = [-0.21058747 -1.3622804  -0.04548226], loss = 3.838391933186358e-07\n",
      "step 530: w = [-0.21060921 -1.3623148  -0.04545492], loss = 3.360236462413013e-07\n",
      "step 535: w = [-0.21062942 -1.3623469  -0.04542914], loss = 2.942037724551483e-07\n",
      "step 540: w = [-0.21064822 -1.3623769  -0.04540483], loss = 2.5755718979780795e-07\n",
      "step 545: w = [-0.21066572 -1.3624048  -0.0453819 ], loss = 2.2557348700047442e-07\n",
      "step 550: w = [-0.21068197 -1.3624308  -0.04536027], loss = 1.9758222435939388e-07\n",
      "step 555: w = [-0.2106971  -1.3624551  -0.04533987], loss = 1.7305235644471395e-07\n",
      "step 560: w = [-0.21071114 -1.3624777  -0.04532065], loss = 1.5165473143952113e-07\n",
      "step 565: w = [-0.21072419 -1.3624986  -0.04530252], loss = 1.3293514200540812e-07\n",
      "step 570: w = [-0.21073632 -1.3625183  -0.04528542], loss = 1.165161265248571e-07\n",
      "step 575: w = [-0.21074758 -1.3625367  -0.0452693 ], loss = 1.0212152545818753e-07\n",
      "step 580: w = [-0.21075806 -1.3625538  -0.0452541 ], loss = 8.95181315740956e-08\n",
      "step 585: w = [-0.21076779 -1.3625698  -0.04523977], loss = 7.84756011285026e-08\n",
      "step 590: w = [-0.21077682 -1.3625847  -0.04522626], loss = 6.88240078261515e-08\n",
      "step 595: w = [-0.21078521 -1.3625987  -0.04521352], loss = 6.032636434838423e-08\n",
      "step 600: w = [-0.21079299 -1.3626115  -0.04520151], loss = 5.2953470941474734e-08\n",
      "step 605: w = [-0.21080022 -1.3626237  -0.04519018], loss = 4.6436223755108585e-08\n",
      "step 610: w = [-0.21080692 -1.3626349  -0.0451795 ], loss = 4.076228421467931e-08\n",
      "step 615: w = [-0.21081315 -1.3626454  -0.04516944], loss = 3.5781155816039245e-08\n",
      "step 620: w = [-0.21081893 -1.3626552  -0.04515996], loss = 3.142310589510089e-08\n",
      "step 625: w = [-0.2108243  -1.3626643  -0.04515101], loss = 2.758154415971603e-08\n",
      "step 630: w = [-0.21082929 -1.3626729  -0.04514258], loss = 2.4212027938119718e-08\n",
      "step 635: w = [-0.2108339  -1.3626809  -0.04513463], loss = 2.1262229310536895e-08\n",
      "step 640: w = [-0.21083817 -1.3626884  -0.04512713], loss = 1.8658511180547066e-08\n",
      "step 645: w = [-0.21084213 -1.3626955  -0.04512007], loss = 1.6385655499107088e-08\n",
      "step 650: w = [-0.21084581 -1.362702   -0.04511341], loss = 1.4386327684690059e-08\n",
      "step 655: w = [-0.21084921 -1.362708   -0.04510713], loss = 1.2654640890730207e-08\n",
      "step 660: w = [-0.21085237 -1.3627137  -0.04510122], loss = 1.11148645842718e-08\n",
      "step 665: w = [-0.21085529 -1.362719   -0.04509564], loss = 9.762485930764342e-09\n",
      "step 670: w = [-0.21085799 -1.362724   -0.04509038], loss = 8.587958788552896e-09\n",
      "step 675: w = [-0.2108605  -1.3627287  -0.04508542], loss = 7.539595614503014e-09\n",
      "step 680: w = [-0.21086283 -1.3627329  -0.04508075], loss = 6.638980476481038e-09\n",
      "step 685: w = [-0.21086498 -1.362737   -0.04507634], loss = 5.83146109178756e-09\n",
      "step 690: w = [-0.21086697 -1.3627406  -0.04507219], loss = 5.136168379493711e-09\n",
      "step 695: w = [-0.2108688  -1.3627442  -0.04506828], loss = 4.517523688463143e-09\n",
      "step 700: w = [-0.2108705  -1.3627474  -0.04506459], loss = 3.9782648286745825e-09\n",
      "step 705: w = [-0.21087207 -1.3627504  -0.04506111], loss = 3.5078664417653727e-09\n",
      "step 710: w = [-0.21087353 -1.3627534  -0.04505783], loss = 3.080769861441013e-09\n",
      "step 715: w = [-0.21087489 -1.362756   -0.04505474], loss = 2.715595748270516e-09\n",
      "step 720: w = [-0.21087612 -1.3627584  -0.04505183], loss = 2.396349119138108e-09\n",
      "step 725: w = [-0.21087727 -1.3627608  -0.04504909], loss = 2.1100448055477727e-09\n",
      "step 730: w = [-0.21087833 -1.3627632  -0.0450465 ], loss = 1.8499527465465349e-09\n",
      "step 735: w = [-0.21087931 -1.3627651  -0.04504406], loss = 1.6331143104508783e-09\n",
      "step 740: w = [-0.21088022 -1.3627669  -0.04504176], loss = 1.4455355801246128e-09\n",
      "step 745: w = [-0.21088105 -1.3627687  -0.04503959], loss = 1.2728593734578908e-09\n",
      "step 750: w = [-0.21088183 -1.3627704  -0.04503755], loss = 1.118332426663926e-09\n",
      "step 755: w = [-0.21088254 -1.3627722  -0.04503563], loss = 9.786326193861328e-10\n",
      "step 760: w = [-0.21088322 -1.3627734  -0.04503381], loss = 8.696714459688337e-10\n",
      "step 765: w = [-0.21088381 -1.3627746  -0.0450321 ], loss = 7.71816777156431e-10\n",
      "step 770: w = [-0.21088436 -1.3627758  -0.04503049], loss = 6.84391043836996e-10\n",
      "step 775: w = [-0.21088488 -1.362777   -0.04502897], loss = 6.038139432895662e-10\n",
      "step 780: w = [-0.21088535 -1.3627782  -0.04502754], loss = 5.309295225686128e-10\n",
      "step 785: w = [-0.2108858  -1.3627794  -0.04502618], loss = 4.655782148699217e-10\n",
      "step 790: w = [-0.21088618 -1.3627806  -0.04502491], loss = 4.074275639087688e-10\n",
      "step 795: w = [-0.21088655 -1.3627815  -0.04502371], loss = 3.5810457377216665e-10\n",
      "step 800: w = [-0.2108869  -1.3627821  -0.04502257], loss = 3.207661081194857e-10\n",
      "step 805: w = [-0.2108872 -1.3627827 -0.0450215], loss = 2.8615065872372725e-10\n",
      "step 810: w = [-0.21088749 -1.3627833  -0.0450205 ], loss = 2.555620992605867e-10\n",
      "step 815: w = [-0.21088775 -1.3627839  -0.04501956], loss = 2.2824404866117476e-10\n",
      "step 820: w = [-0.21088797 -1.3627845  -0.04501867], loss = 2.021319778222619e-10\n",
      "step 825: w = [-0.21088819 -1.3627851  -0.04501782], loss = 1.7984225220146755e-10\n",
      "step 830: w = [-0.21088842 -1.3627857  -0.04501704], loss = 1.588409681563263e-10\n",
      "step 835: w = [-0.2108886  -1.3627863  -0.04501629], loss = 1.4032240647221528e-10\n",
      "step 840: w = [-0.21088874 -1.3627869  -0.04501558], loss = 1.2380452218963e-10\n",
      "step 845: w = [-0.21088889 -1.3627875  -0.04501492], loss = 1.0838168557070205e-10\n",
      "step 850: w = [-0.21088904 -1.3627881  -0.04501429], loss = 9.430489028572353e-11\n",
      "step 855: w = [-0.21088919 -1.3627887  -0.04501369], loss = 8.214612323698134e-11\n",
      "step 860: w = [-0.21088934 -1.3627893  -0.04501313], loss = 7.069455437713756e-11\n",
      "step 865: w = [-0.21088946 -1.3627899  -0.0450126 ], loss = 6.094761501040935e-11\n",
      "step 870: w = [-0.21088953 -1.3627901  -0.0450121 ], loss = 5.4538373817081265e-11\n",
      "step 875: w = [-0.21088961 -1.3627902  -0.04501162], loss = 4.937165259955023e-11\n",
      "step 880: w = [-0.21088968 -1.3627902  -0.04501119], loss = 4.486772064993261e-11\n",
      "step 885: w = [-0.21088976 -1.3627903  -0.04501077], loss = 4.101307488069139e-11\n",
      "step 890: w = [-0.21088983 -1.3627903  -0.04501038], loss = 3.774569545811346e-11\n",
      "step 895: w = [-0.2108899  -1.3627903  -0.04501001], loss = 3.5108419643714583e-11\n",
      "step 900: w = [-0.21088998 -1.3627905  -0.04500967], loss = 3.214582419697187e-11\n",
      "step 905: w = [-0.21089005 -1.3627905  -0.04500935], loss = 2.988847017104668e-11\n",
      "step 910: w = [-0.2108901  -1.3627905  -0.04500905], loss = 2.7935945223767078e-11\n",
      "step 915: w = [-0.2108901  -1.3627906  -0.04500876], loss = 2.562542365802667e-11\n",
      "step 920: w = [-0.2108901  -1.3627906  -0.04500849], loss = 2.445288763142095e-11\n",
      "step 925: w = [-0.2108901  -1.3627906  -0.04500824], loss = 2.3024129477966504e-11\n",
      "step 930: w = [-0.2108901  -1.3627906  -0.04500801], loss = 2.195326385956431e-11\n",
      "step 935: w = [-0.2108901  -1.3627907  -0.04500778], loss = 2.034777901727125e-11\n",
      "step 940: w = [-0.2108901  -1.3627907  -0.04500758], loss = 1.936180209216598e-11\n",
      "step 945: w = [-0.2108901  -1.3627907  -0.04500739], loss = 1.8589832798121542e-11\n",
      "step 950: w = [-0.2108901 -1.3627907 -0.0450072], loss = 1.793150176954139e-11\n",
      "step 955: w = [-0.2108901  -1.3627907  -0.04500703], loss = 1.744032696038289e-11\n",
      "step 960: w = [-0.2108901  -1.3627908  -0.04500687], loss = 1.6150114232060808e-11\n",
      "step 965: w = [-0.2108901  -1.3627908  -0.04500672], loss = 1.5638670913808994e-11\n",
      "step 970: w = [-0.2108901  -1.3627908  -0.04500658], loss = 1.534138614756042e-11\n",
      "step 975: w = [-0.2108901  -1.3627908  -0.04500645], loss = 1.4905533404774296e-11\n",
      "step 980: w = [-0.2108901  -1.3627908  -0.04500632], loss = 1.452193140044633e-11\n",
      "step 985: w = [-0.2108901 -1.362791  -0.0450062], loss = 1.3677315356674935e-11\n",
      "step 990: w = [-0.2108901  -1.362791   -0.04500609], loss = 1.3262427614457728e-11\n",
      "step 995: w = [-0.2108901  -1.362791   -0.04500598], loss = 1.3191406300627762e-11\n",
      "step 1000: w = [-0.2108901  -1.362791   -0.04500589], loss = 1.2943116196872939e-11\n",
      "step 1005: w = [-0.2108901  -1.362791   -0.04500579], loss = 1.2801437861142961e-11\n",
      "step 1010: w = [-0.2108901 -1.362791  -0.0450057], loss = 1.263618376601272e-11\n",
      "step 1015: w = [-0.2108901  -1.362791   -0.04500563], loss = 1.245041830050253e-11\n",
      "step 1020: w = [-0.2108901  -1.362791   -0.04500555], loss = 1.2284550980623532e-11\n",
      "step 1025: w = [-0.2108901  -1.362791   -0.04500548], loss = 1.2363446204310957e-11\n",
      "step 1030: w = [-0.2108901  -1.362791   -0.04500541], loss = 1.2194733937931357e-11\n",
      "step 1035: w = [-0.2108901  -1.362791   -0.04500535], loss = 1.2141876913618344e-11\n",
      "step 1040: w = [-0.2108901 -1.362791  -0.0450053], loss = 1.2162240832502835e-11\n",
      "step 1045: w = [-0.2108901  -1.362791   -0.04500524], loss = 1.196619799775922e-11\n",
      "step 1050: w = [-0.2108901  -1.362791   -0.04500518], loss = 1.1992462578547247e-11\n",
      "step 1055: w = [-0.2108901  -1.3627911  -0.04500513], loss = 1.116903531467317e-11\n",
      "step 1060: w = [-0.2108901  -1.3627911  -0.04500508], loss = 1.1066314531404942e-11\n",
      "step 1065: w = [-0.2108901  -1.3627911  -0.04500504], loss = 1.1000237179481509e-11\n",
      "step 1070: w = [-0.2108901 -1.3627911 -0.045005 ], loss = 1.0940296411854344e-11\n",
      "step 1075: w = [-0.2108901  -1.3627911  -0.04500497], loss = 1.0880721670880611e-11\n",
      "step 1080: w = [-0.2108901  -1.3627911  -0.04500493], loss = 1.0983158826860517e-11\n",
      "step 1085: w = [-0.2108901  -1.3627911  -0.04500489], loss = 1.0963785435080808e-11\n",
      "step 1090: w = [-0.2108901  -1.3627911  -0.04500486], loss = 1.0868384317519464e-11\n",
      "step 1095: w = [-0.2108901  -1.3627911  -0.04500482], loss = 1.070844801720483e-11\n",
      "step 1100: w = [-0.2108901  -1.3627911  -0.04500479], loss = 1.0769530232879188e-11\n",
      "step 1105: w = [-0.2108901  -1.3627911  -0.04500477], loss = 1.0761713568896436e-11\n",
      "step 1110: w = [-0.2108901  -1.3627911  -0.04500475], loss = 1.0759804505711124e-11\n",
      "step 1115: w = [-0.2108901  -1.3627911  -0.04500473], loss = 1.0769852024083981e-11\n",
      "step 1120: w = [-0.2108901  -1.3627911  -0.04500471], loss = 1.0859468706214681e-11\n",
      "step 1125: w = [-0.2108901 -1.3627911 -0.0450047], loss = 1.0954648646532839e-11\n",
      "step 1130: w = [-0.2108901  -1.3627911  -0.04500468], loss = 1.0946246513376945e-11\n",
      "step 1135: w = [-0.2108901  -1.3627911  -0.04500466], loss = 1.09385304633558e-11\n",
      "step 1140: w = [-0.2108901  -1.3627911  -0.04500464], loss = 1.1005910592609691e-11\n",
      "step 1145: w = [-0.2108901  -1.3627911  -0.04500462], loss = 1.0951886966759083e-11\n",
      "step 1150: w = [-0.2108901 -1.3627911 -0.0450046], loss = 1.0916947901229435e-11\n",
      "step 1155: w = [-0.2108901  -1.3627911  -0.04500458], loss = 1.0777193373834315e-11\n",
      "step 1160: w = [-0.2108901  -1.3627911  -0.04500457], loss = 1.078157615269637e-11\n",
      "step 1165: w = [-0.2108901  -1.3627911  -0.04500455], loss = 1.0797363003689497e-11\n",
      "step 1170: w = [-0.2108901  -1.3627911  -0.04500453], loss = 1.0791124070708147e-11\n",
      "step 1175: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0831702722258196e-11\n",
      "step 1180: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1185: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1190: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1195: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1200: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1205: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1210: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1215: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1220: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1225: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1230: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1235: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1240: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1245: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1250: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1255: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1260: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1265: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1270: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1275: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1280: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1285: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1290: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1295: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1300: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1305: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1310: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1315: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1320: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1325: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1330: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1335: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1340: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1345: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1350: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1355: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1360: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1365: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1370: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1375: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1380: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1385: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1390: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1395: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1400: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1405: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1410: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1415: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1420: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1425: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1430: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1435: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1440: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1445: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1450: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1455: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1460: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1465: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1470: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1475: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1480: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1485: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1490: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1495: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1500: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1505: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1510: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1515: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1520: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1525: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1530: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1535: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1540: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1545: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1550: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1555: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1560: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1565: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1570: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1575: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1580: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1585: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1590: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1595: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1600: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1605: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1610: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1615: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1620: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1625: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1630: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1635: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1640: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1645: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1650: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1655: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1660: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1665: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1670: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1675: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1680: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1685: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1690: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1695: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1700: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1705: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1710: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1715: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1720: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1725: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1730: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1735: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1740: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1745: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1750: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1755: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1760: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1765: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1770: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1775: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1780: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1785: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1790: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1795: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1800: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1805: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1810: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1815: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1820: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1825: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1830: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1835: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1840: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1845: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1850: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1855: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1860: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1865: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1870: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1875: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1880: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1885: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1890: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1895: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1900: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1905: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1910: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1915: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1920: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1925: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1930: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1935: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1940: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1945: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1950: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1955: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1960: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1965: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1970: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1975: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1980: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1985: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1990: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1995: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step  0: w = [-0.00088966 -0.02219324 -0.00279509], loss = 1.4782171249389648\n",
      "step  5: w = [-0.00571141 -0.12774374 -0.01566323], loss = 1.2533844709396362\n",
      "step 10: w = [-0.01106317 -0.22481343 -0.02683333], loss = 1.0633506774902344\n",
      "step 15: w = [-0.01683181 -0.31409633 -0.03649034], loss = 0.902643620967865\n",
      "step 20: w = [-0.02291877 -0.39622876 -0.04480066], loss = 0.7666628360748291\n",
      "step 25: w = [-0.02923849 -0.47179416 -0.05191391], loss = 0.6515409350395203\n",
      "step 30: w = [-0.03571691 -0.5413276  -0.05796458], loss = 0.5540241599082947\n",
      "step 35: w = [-0.04229026 -0.6053198  -0.06307346], loss = 0.47137394547462463\n",
      "step 40: w = [-0.04890383 -0.6642209  -0.06734898], loss = 0.4012843072414398\n",
      "step 45: w = [-0.05551092 -0.71844393 -0.07088839], loss = 0.3418126106262207\n",
      "step 50: w = [-0.06207199 -0.76836765 -0.07377888], loss = 0.29132184386253357\n",
      "step 55: w = [-0.06855375 -0.81433976 -0.07609855], loss = 0.24843107163906097\n",
      "step 60: w = [-0.07492846 -0.85667926 -0.0779173 ], loss = 0.21197549998760223\n",
      "step 65: w = [-0.0811732  -0.89567894 -0.07929762], loss = 0.1809719204902649\n",
      "step 70: w = [-0.08726934 -0.9316077  -0.08029536], loss = 0.15458963811397552\n",
      "step 75: w = [-0.09320199 -0.9647123  -0.08096036], loss = 0.13212697207927704\n",
      "step 80: w = [-0.09895948 -0.9952192  -0.08133703], loss = 0.11299077421426773\n",
      "step 85: w = [-0.10453304 -1.0233364  -0.08146498], loss = 0.09667900949716568\n",
      "step 90: w = [-0.1099163  -1.0492553  -0.08137938], loss = 0.0827668234705925\n",
      "step 95: w = [-0.11510506 -1.073151   -0.0811115 ], loss = 0.07089464366436005\n",
      "step 100: w = [-0.12009692 -1.095185   -0.08068907], loss = 0.06075745448470116\n",
      "step 105: w = [-0.12489107 -1.1155055  -0.08013667], loss = 0.05209700018167496\n",
      "step 110: w = [-0.129488   -1.1342483  -0.07947601], loss = 0.04469400271773338\n",
      "step 115: w = [-0.1338894  -1.1515386  -0.07872622], loss = 0.03836241737008095\n",
      "step 120: w = [-0.13809785 -1.1674914  -0.07790422], loss = 0.032944247126579285\n",
      "step 125: w = [-0.14211673 -1.1822124  -0.07702482], loss = 0.02830524556338787\n",
      "step 130: w = [-0.14595006 -1.1957989  -0.07610103], loss = 0.02433115802705288\n",
      "step 135: w = [-0.14960243 -1.20834    -0.07514418], loss = 0.020924946293234825\n",
      "step 140: w = [-0.1530788  -1.2199183  -0.07416419], loss = 0.01800393871963024\n",
      "step 145: w = [-0.15638447 -1.2306088  -0.07316965], loss = 0.015497765503823757\n",
      "step 150: w = [-0.159525   -1.2404815  -0.07216798], loss = 0.013346395455300808\n",
      "step 155: w = [-0.16250612 -1.2496003  -0.07116558], loss = 0.01149867195636034\n",
      "step 160: w = [-0.16533363 -1.258024   -0.07016794], loss = 0.009910988621413708\n",
      "step 165: w = [-0.16801345 -1.2658066  -0.0691797 ], loss = 0.008546113967895508\n",
      "step 170: w = [-0.1705515  -1.272998   -0.06820481], loss = 0.007372215390205383\n",
      "step 175: w = [-0.1729536  -1.2796444  -0.06724657], loss = 0.006362080108374357\n",
      "step 180: w = [-0.1752256  -1.2857877  -0.06630771], loss = 0.005492504220455885\n",
      "step 185: w = [-0.17737326 -1.2914672  -0.06539045], loss = 0.004743561614304781\n",
      "step 190: w = [-0.17940219 -1.2967184  -0.0644966 ], loss = 0.004098270554095507\n",
      "step 195: w = [-0.1813179  -1.3015742  -0.06362756], loss = 0.003542054910212755\n",
      "step 200: w = [-0.18312576 -1.3060653  -0.06278441], loss = 0.0030624084174633026\n",
      "step 205: w = [-0.18483101 -1.3102195  -0.06196793], loss = 0.002648627618327737\n",
      "step 210: w = [-0.18643868 -1.3140627  -0.06117864], loss = 0.002291532000526786\n",
      "step 215: w = [-0.18795365 -1.317619   -0.06041684], loss = 0.0019832116086035967\n",
      "step 220: w = [-0.18938068 -1.32091    -0.05968264], loss = 0.0017169226193800569\n",
      "step 225: w = [-0.19072427 -1.3239561  -0.058976  ], loss = 0.001486839959397912\n",
      "step 230: w = [-0.19198881 -1.3267758  -0.05829672], loss = 0.0012879838468506932\n",
      "step 235: w = [-0.19317849 -1.3293865  -0.05764448], loss = 0.0011160433059558272\n",
      "step 240: w = [-0.1942973  -1.3318037  -0.05701887], loss = 0.0009673374588601291\n",
      "step 245: w = [-0.1953491  -1.3340423  -0.05641939], loss = 0.0008386755362153053\n",
      "step 250: w = [-0.19633758 -1.3361157  -0.05584549], loss = 0.0007273252704180777\n",
      "step 255: w = [-0.19726622 -1.3380364  -0.05529653], loss = 0.0006309249438345432\n",
      "step 260: w = [-0.19813839 -1.3398159  -0.05477187], loss = 0.0005474438657984138\n",
      "step 265: w = [-0.19895725 -1.3414648  -0.05427079], loss = 0.000475125212687999\n",
      "step 270: w = [-0.19972584 -1.3429927  -0.05379257], loss = 0.00041246606269851327\n",
      "step 275: w = [-0.20044704 -1.3444091  -0.05333648], loss = 0.000358146004145965\n",
      "step 280: w = [-0.20112358 -1.345722   -0.05290175], loss = 0.00031105769448913634\n",
      "step 285: w = [-0.20175806 -1.3469392  -0.05248763], loss = 0.00027021855930797756\n",
      "step 290: w = [-0.20235293 -1.3480679  -0.05209337], loss = 0.0002347925037611276\n",
      "step 295: w = [-0.20291051 -1.3491144  -0.05171819], loss = 0.0002040572726400569\n",
      "step 300: w = [-0.20343304 -1.3500851  -0.05136136], loss = 0.00017738206952344626\n",
      "step 305: w = [-0.20392258 -1.3509855  -0.05102212], loss = 0.00015422631986439228\n",
      "step 310: w = [-0.20438112 -1.3518208  -0.05069977], loss = 0.0001341203460469842\n",
      "step 315: w = [-0.20481053 -1.352596   -0.05039357], loss = 0.0001166555448435247\n",
      "step 320: w = [-0.20521255 -1.3533154  -0.05010284], loss = 0.00010148554429179057\n",
      "step 325: w = [-0.20558886 -1.353983   -0.04982689], loss = 8.830326260067523e-05\n",
      "step 330: w = [-0.20594104 -1.3546027  -0.04956506], loss = 7.684822048759088e-05\n",
      "step 335: w = [-0.20627056 -1.355178   -0.04931672], loss = 6.689022848149762e-05\n",
      "step 340: w = [-0.20657884 -1.3557122  -0.04908124], loss = 5.823226092616096e-05\n",
      "step 345: w = [-0.20686716 -1.3562081  -0.04885802], loss = 5.070469342172146e-05\n",
      "step 350: w = [-0.20713678 -1.3566686  -0.0486465 ], loss = 4.415765215526335e-05\n",
      "step 355: w = [-0.20738886 -1.3570963  -0.0484461 ], loss = 3.846233812510036e-05\n",
      "step 360: w = [-0.20762451 -1.3574935  -0.04825629], loss = 3.350788028910756e-05\n",
      "step 365: w = [-0.20784473 -1.3578626  -0.04807657], loss = 2.91956530418247e-05\n",
      "step 370: w = [-0.20805053 -1.3582056  -0.04790642], loss = 2.5442399419262074e-05\n",
      "step 375: w = [-0.2082428  -1.3585241  -0.04774538], loss = 2.2175812773639336e-05\n",
      "step 380: w = [-0.20842241 -1.3588201  -0.047593  ], loss = 1.9331651856191456e-05\n",
      "step 385: w = [-0.20859015 -1.3590952  -0.04744883], loss = 1.6854804925969802e-05\n",
      "step 390: w = [-0.2087468  -1.359351   -0.04731245], loss = 1.4697196093038656e-05\n",
      "step 395: w = [-0.20889308 -1.3595889  -0.04718347], loss = 1.2817569768230896e-05\n",
      "step 400: w = [-0.20902961 -1.35981    -0.04706152], loss = 1.1180098226759583e-05\n",
      "step 405: w = [-0.20915706 -1.3600156  -0.04694622], loss = 9.752910955285188e-06\n",
      "step 410: w = [-0.209276   -1.3602068  -0.04683723], loss = 8.509292456437834e-06\n",
      "step 415: w = [-0.20938699 -1.3603847  -0.04673423], loss = 7.42532211006619e-06\n",
      "step 420: w = [-0.20949054 -1.36055    -0.04663688], loss = 6.480412594100926e-06\n",
      "step 425: w = [-0.20958713 -1.360704   -0.04654491], loss = 5.656574103340972e-06\n",
      "step 430: w = [-0.20967722 -1.3608471  -0.04645802], loss = 4.938020083500305e-06\n",
      "step 435: w = [-0.20976125 -1.3609803  -0.04637593], loss = 4.311373231757898e-06\n",
      "step 440: w = [-0.20983958 -1.3611042  -0.04629841], loss = 3.764741222767043e-06\n",
      "step 445: w = [-0.20991263 -1.3612195  -0.04622519], loss = 3.2880100206966745e-06\n",
      "step 450: w = [-0.20998073 -1.3613268  -0.04615606], loss = 2.8720760383293964e-06\n",
      "step 455: w = [-0.2100442  -1.3614267  -0.04609078], loss = 2.5089846076298272e-06\n",
      "step 460: w = [-0.21010336 -1.3615198  -0.04602915], loss = 2.191902240156196e-06\n",
      "step 465: w = [-0.21015848 -1.3616062  -0.04597097], loss = 1.915655957418494e-06\n",
      "step 470: w = [-0.21020983 -1.3616867  -0.04591605], loss = 1.6743696278354037e-06\n",
      "step 475: w = [-0.21025766 -1.3617618  -0.04586421], loss = 1.4634350691267173e-06\n",
      "step 480: w = [-0.2103022  -1.3618318  -0.04581529], loss = 1.2792394272764795e-06\n",
      "step 485: w = [-0.2103437  -1.3618969  -0.04576911], loss = 1.118470549954509e-06\n",
      "step 490: w = [-0.21038234 -1.3619576  -0.04572554], loss = 9.779550964594819e-07\n",
      "step 495: w = [-0.21041833 -1.3620139  -0.04568443], loss = 8.55334860716539e-07\n",
      "step 500: w = [-0.21045181 -1.3620666  -0.04564565], loss = 7.480426802430884e-07\n",
      "step 505: w = [-0.21048295 -1.3621156  -0.04560905], loss = 6.544298685184913e-07\n",
      "step 510: w = [-0.21051197 -1.3621614  -0.04557452], loss = 5.724683660446317e-07\n",
      "step 515: w = [-0.21053897 -1.3622037  -0.04554195], loss = 5.011162897972099e-07\n",
      "step 520: w = [-0.21056409 -1.3622434  -0.04551123], loss = 4.385454417388246e-07\n",
      "step 525: w = [-0.21058747 -1.3622804  -0.04548226], loss = 3.838391933186358e-07\n",
      "step 530: w = [-0.21060921 -1.3623148  -0.04545492], loss = 3.360236462413013e-07\n",
      "step 535: w = [-0.21062942 -1.3623469  -0.04542914], loss = 2.942037724551483e-07\n",
      "step 540: w = [-0.21064822 -1.3623769  -0.04540483], loss = 2.5755718979780795e-07\n",
      "step 545: w = [-0.21066572 -1.3624048  -0.0453819 ], loss = 2.2557348700047442e-07\n",
      "step 550: w = [-0.21068197 -1.3624308  -0.04536027], loss = 1.9758222435939388e-07\n",
      "step 555: w = [-0.2106971  -1.3624551  -0.04533987], loss = 1.7305235644471395e-07\n",
      "step 560: w = [-0.21071114 -1.3624777  -0.04532065], loss = 1.5165473143952113e-07\n",
      "step 565: w = [-0.21072419 -1.3624986  -0.04530252], loss = 1.3293514200540812e-07\n",
      "step 570: w = [-0.21073632 -1.3625183  -0.04528542], loss = 1.165161265248571e-07\n",
      "step 575: w = [-0.21074758 -1.3625367  -0.0452693 ], loss = 1.0212152545818753e-07\n",
      "step 580: w = [-0.21075806 -1.3625538  -0.0452541 ], loss = 8.95181315740956e-08\n",
      "step 585: w = [-0.21076779 -1.3625698  -0.04523977], loss = 7.84756011285026e-08\n",
      "step 590: w = [-0.21077682 -1.3625847  -0.04522626], loss = 6.88240078261515e-08\n",
      "step 595: w = [-0.21078521 -1.3625987  -0.04521352], loss = 6.032636434838423e-08\n",
      "step 600: w = [-0.21079299 -1.3626115  -0.04520151], loss = 5.2953470941474734e-08\n",
      "step 605: w = [-0.21080022 -1.3626237  -0.04519018], loss = 4.6436223755108585e-08\n",
      "step 610: w = [-0.21080692 -1.3626349  -0.0451795 ], loss = 4.076228421467931e-08\n",
      "step 615: w = [-0.21081315 -1.3626454  -0.04516944], loss = 3.5781155816039245e-08\n",
      "step 620: w = [-0.21081893 -1.3626552  -0.04515996], loss = 3.142310589510089e-08\n",
      "step 625: w = [-0.2108243  -1.3626643  -0.04515101], loss = 2.758154415971603e-08\n",
      "step 630: w = [-0.21082929 -1.3626729  -0.04514258], loss = 2.4212027938119718e-08\n",
      "step 635: w = [-0.2108339  -1.3626809  -0.04513463], loss = 2.1262229310536895e-08\n",
      "step 640: w = [-0.21083817 -1.3626884  -0.04512713], loss = 1.8658511180547066e-08\n",
      "step 645: w = [-0.21084213 -1.3626955  -0.04512007], loss = 1.6385655499107088e-08\n",
      "step 650: w = [-0.21084581 -1.362702   -0.04511341], loss = 1.4386327684690059e-08\n",
      "step 655: w = [-0.21084921 -1.362708   -0.04510713], loss = 1.2654640890730207e-08\n",
      "step 660: w = [-0.21085237 -1.3627137  -0.04510122], loss = 1.11148645842718e-08\n",
      "step 665: w = [-0.21085529 -1.362719   -0.04509564], loss = 9.762485930764342e-09\n",
      "step 670: w = [-0.21085799 -1.362724   -0.04509038], loss = 8.587958788552896e-09\n",
      "step 675: w = [-0.2108605  -1.3627287  -0.04508542], loss = 7.539595614503014e-09\n",
      "step 680: w = [-0.21086283 -1.3627329  -0.04508075], loss = 6.638980476481038e-09\n",
      "step 685: w = [-0.21086498 -1.362737   -0.04507634], loss = 5.83146109178756e-09\n",
      "step 690: w = [-0.21086697 -1.3627406  -0.04507219], loss = 5.136168379493711e-09\n",
      "step 695: w = [-0.2108688  -1.3627442  -0.04506828], loss = 4.517523688463143e-09\n",
      "step 700: w = [-0.2108705  -1.3627474  -0.04506459], loss = 3.9782648286745825e-09\n",
      "step 705: w = [-0.21087207 -1.3627504  -0.04506111], loss = 3.5078664417653727e-09\n",
      "step 710: w = [-0.21087353 -1.3627534  -0.04505783], loss = 3.080769861441013e-09\n",
      "step 715: w = [-0.21087489 -1.362756   -0.04505474], loss = 2.715595748270516e-09\n",
      "step 720: w = [-0.21087612 -1.3627584  -0.04505183], loss = 2.396349119138108e-09\n",
      "step 725: w = [-0.21087727 -1.3627608  -0.04504909], loss = 2.1100448055477727e-09\n",
      "step 730: w = [-0.21087833 -1.3627632  -0.0450465 ], loss = 1.8499527465465349e-09\n",
      "step 735: w = [-0.21087931 -1.3627651  -0.04504406], loss = 1.6331143104508783e-09\n",
      "step 740: w = [-0.21088022 -1.3627669  -0.04504176], loss = 1.4455355801246128e-09\n",
      "step 745: w = [-0.21088105 -1.3627687  -0.04503959], loss = 1.2728593734578908e-09\n",
      "step 750: w = [-0.21088183 -1.3627704  -0.04503755], loss = 1.118332426663926e-09\n",
      "step 755: w = [-0.21088254 -1.3627722  -0.04503563], loss = 9.786326193861328e-10\n",
      "step 760: w = [-0.21088322 -1.3627734  -0.04503381], loss = 8.696714459688337e-10\n",
      "step 765: w = [-0.21088381 -1.3627746  -0.0450321 ], loss = 7.71816777156431e-10\n",
      "step 770: w = [-0.21088436 -1.3627758  -0.04503049], loss = 6.84391043836996e-10\n",
      "step 775: w = [-0.21088488 -1.362777   -0.04502897], loss = 6.038139432895662e-10\n",
      "step 780: w = [-0.21088535 -1.3627782  -0.04502754], loss = 5.309295225686128e-10\n",
      "step 785: w = [-0.2108858  -1.3627794  -0.04502618], loss = 4.655782148699217e-10\n",
      "step 790: w = [-0.21088618 -1.3627806  -0.04502491], loss = 4.074275639087688e-10\n",
      "step 795: w = [-0.21088655 -1.3627815  -0.04502371], loss = 3.5810457377216665e-10\n",
      "step 800: w = [-0.2108869  -1.3627821  -0.04502257], loss = 3.207661081194857e-10\n",
      "step 805: w = [-0.2108872 -1.3627827 -0.0450215], loss = 2.8615065872372725e-10\n",
      "step 810: w = [-0.21088749 -1.3627833  -0.0450205 ], loss = 2.555620992605867e-10\n",
      "step 815: w = [-0.21088775 -1.3627839  -0.04501956], loss = 2.2824404866117476e-10\n",
      "step 820: w = [-0.21088797 -1.3627845  -0.04501867], loss = 2.021319778222619e-10\n",
      "step 825: w = [-0.21088819 -1.3627851  -0.04501782], loss = 1.7984225220146755e-10\n",
      "step 830: w = [-0.21088842 -1.3627857  -0.04501704], loss = 1.588409681563263e-10\n",
      "step 835: w = [-0.2108886  -1.3627863  -0.04501629], loss = 1.4032240647221528e-10\n",
      "step 840: w = [-0.21088874 -1.3627869  -0.04501558], loss = 1.2380452218963e-10\n",
      "step 845: w = [-0.21088889 -1.3627875  -0.04501492], loss = 1.0838168557070205e-10\n",
      "step 850: w = [-0.21088904 -1.3627881  -0.04501429], loss = 9.430489028572353e-11\n",
      "step 855: w = [-0.21088919 -1.3627887  -0.04501369], loss = 8.214612323698134e-11\n",
      "step 860: w = [-0.21088934 -1.3627893  -0.04501313], loss = 7.069455437713756e-11\n",
      "step 865: w = [-0.21088946 -1.3627899  -0.0450126 ], loss = 6.094761501040935e-11\n",
      "step 870: w = [-0.21088953 -1.3627901  -0.0450121 ], loss = 5.4538373817081265e-11\n",
      "step 875: w = [-0.21088961 -1.3627902  -0.04501162], loss = 4.937165259955023e-11\n",
      "step 880: w = [-0.21088968 -1.3627902  -0.04501119], loss = 4.486772064993261e-11\n",
      "step 885: w = [-0.21088976 -1.3627903  -0.04501077], loss = 4.101307488069139e-11\n",
      "step 890: w = [-0.21088983 -1.3627903  -0.04501038], loss = 3.774569545811346e-11\n",
      "step 895: w = [-0.2108899  -1.3627903  -0.04501001], loss = 3.5108419643714583e-11\n",
      "step 900: w = [-0.21088998 -1.3627905  -0.04500967], loss = 3.214582419697187e-11\n",
      "step 905: w = [-0.21089005 -1.3627905  -0.04500935], loss = 2.988847017104668e-11\n",
      "step 910: w = [-0.2108901  -1.3627905  -0.04500905], loss = 2.7935945223767078e-11\n",
      "step 915: w = [-0.2108901  -1.3627906  -0.04500876], loss = 2.562542365802667e-11\n",
      "step 920: w = [-0.2108901  -1.3627906  -0.04500849], loss = 2.445288763142095e-11\n",
      "step 925: w = [-0.2108901  -1.3627906  -0.04500824], loss = 2.3024129477966504e-11\n",
      "step 930: w = [-0.2108901  -1.3627906  -0.04500801], loss = 2.195326385956431e-11\n",
      "step 935: w = [-0.2108901  -1.3627907  -0.04500778], loss = 2.034777901727125e-11\n",
      "step 940: w = [-0.2108901  -1.3627907  -0.04500758], loss = 1.936180209216598e-11\n",
      "step 945: w = [-0.2108901  -1.3627907  -0.04500739], loss = 1.8589832798121542e-11\n",
      "step 950: w = [-0.2108901 -1.3627907 -0.0450072], loss = 1.793150176954139e-11\n",
      "step 955: w = [-0.2108901  -1.3627907  -0.04500703], loss = 1.744032696038289e-11\n",
      "step 960: w = [-0.2108901  -1.3627908  -0.04500687], loss = 1.6150114232060808e-11\n",
      "step 965: w = [-0.2108901  -1.3627908  -0.04500672], loss = 1.5638670913808994e-11\n",
      "step 970: w = [-0.2108901  -1.3627908  -0.04500658], loss = 1.534138614756042e-11\n",
      "step 975: w = [-0.2108901  -1.3627908  -0.04500645], loss = 1.4905533404774296e-11\n",
      "step 980: w = [-0.2108901  -1.3627908  -0.04500632], loss = 1.452193140044633e-11\n",
      "step 985: w = [-0.2108901 -1.362791  -0.0450062], loss = 1.3677315356674935e-11\n",
      "step 990: w = [-0.2108901  -1.362791   -0.04500609], loss = 1.3262427614457728e-11\n",
      "step 995: w = [-0.2108901  -1.362791   -0.04500598], loss = 1.3191406300627762e-11\n",
      "step 1000: w = [-0.2108901  -1.362791   -0.04500589], loss = 1.2943116196872939e-11\n",
      "step 1005: w = [-0.2108901  -1.362791   -0.04500579], loss = 1.2801437861142961e-11\n",
      "step 1010: w = [-0.2108901 -1.362791  -0.0450057], loss = 1.263618376601272e-11\n",
      "step 1015: w = [-0.2108901  -1.362791   -0.04500563], loss = 1.245041830050253e-11\n",
      "step 1020: w = [-0.2108901  -1.362791   -0.04500555], loss = 1.2284550980623532e-11\n",
      "step 1025: w = [-0.2108901  -1.362791   -0.04500548], loss = 1.2363446204310957e-11\n",
      "step 1030: w = [-0.2108901  -1.362791   -0.04500541], loss = 1.2194733937931357e-11\n",
      "step 1035: w = [-0.2108901  -1.362791   -0.04500535], loss = 1.2141876913618344e-11\n",
      "step 1040: w = [-0.2108901 -1.362791  -0.0450053], loss = 1.2162240832502835e-11\n",
      "step 1045: w = [-0.2108901  -1.362791   -0.04500524], loss = 1.196619799775922e-11\n",
      "step 1050: w = [-0.2108901  -1.362791   -0.04500518], loss = 1.1992462578547247e-11\n",
      "step 1055: w = [-0.2108901  -1.3627911  -0.04500513], loss = 1.116903531467317e-11\n",
      "step 1060: w = [-0.2108901  -1.3627911  -0.04500508], loss = 1.1066314531404942e-11\n",
      "step 1065: w = [-0.2108901  -1.3627911  -0.04500504], loss = 1.1000237179481509e-11\n",
      "step 1070: w = [-0.2108901 -1.3627911 -0.045005 ], loss = 1.0940296411854344e-11\n",
      "step 1075: w = [-0.2108901  -1.3627911  -0.04500497], loss = 1.0880721670880611e-11\n",
      "step 1080: w = [-0.2108901  -1.3627911  -0.04500493], loss = 1.0983158826860517e-11\n",
      "step 1085: w = [-0.2108901  -1.3627911  -0.04500489], loss = 1.0963785435080808e-11\n",
      "step 1090: w = [-0.2108901  -1.3627911  -0.04500486], loss = 1.0868384317519464e-11\n",
      "step 1095: w = [-0.2108901  -1.3627911  -0.04500482], loss = 1.070844801720483e-11\n",
      "step 1100: w = [-0.2108901  -1.3627911  -0.04500479], loss = 1.0769530232879188e-11\n",
      "step 1105: w = [-0.2108901  -1.3627911  -0.04500477], loss = 1.0761713568896436e-11\n",
      "step 1110: w = [-0.2108901  -1.3627911  -0.04500475], loss = 1.0759804505711124e-11\n",
      "step 1115: w = [-0.2108901  -1.3627911  -0.04500473], loss = 1.0769852024083981e-11\n",
      "step 1120: w = [-0.2108901  -1.3627911  -0.04500471], loss = 1.0859468706214681e-11\n",
      "step 1125: w = [-0.2108901 -1.3627911 -0.0450047], loss = 1.0954648646532839e-11\n",
      "step 1130: w = [-0.2108901  -1.3627911  -0.04500468], loss = 1.0946246513376945e-11\n",
      "step 1135: w = [-0.2108901  -1.3627911  -0.04500466], loss = 1.09385304633558e-11\n",
      "step 1140: w = [-0.2108901  -1.3627911  -0.04500464], loss = 1.1005910592609691e-11\n",
      "step 1145: w = [-0.2108901  -1.3627911  -0.04500462], loss = 1.0951886966759083e-11\n",
      "step 1150: w = [-0.2108901 -1.3627911 -0.0450046], loss = 1.0916947901229435e-11\n",
      "step 1155: w = [-0.2108901  -1.3627911  -0.04500458], loss = 1.0777193373834315e-11\n",
      "step 1160: w = [-0.2108901  -1.3627911  -0.04500457], loss = 1.078157615269637e-11\n",
      "step 1165: w = [-0.2108901  -1.3627911  -0.04500455], loss = 1.0797363003689497e-11\n",
      "step 1170: w = [-0.2108901  -1.3627911  -0.04500453], loss = 1.0791124070708147e-11\n",
      "step 1175: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0831702722258196e-11\n",
      "step 1180: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1185: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1190: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1195: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1200: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1205: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1210: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1215: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1220: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1225: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1230: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1235: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1240: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1245: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1250: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1255: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1260: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1265: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1270: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1275: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1280: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1285: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1290: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1295: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1300: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1305: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1310: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1315: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1320: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1325: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1330: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1335: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1340: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1345: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1350: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1355: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1360: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1365: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1370: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1375: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1380: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1385: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1390: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1395: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1400: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1405: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1410: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1415: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1420: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1425: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1430: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1435: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1440: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1445: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1450: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1455: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1460: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1465: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1470: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1475: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1480: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1485: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1490: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1495: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1500: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1505: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1510: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1515: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1520: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1525: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1530: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1535: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1540: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1545: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1550: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1555: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1560: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1565: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1570: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1575: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1580: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1585: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1590: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1595: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1600: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1605: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1610: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1615: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1620: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1625: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1630: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1635: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1640: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1645: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1650: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1655: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1660: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1665: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1670: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1675: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1680: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1685: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1690: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1695: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1700: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1705: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1710: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1715: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1720: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1725: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1730: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1735: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1740: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1745: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1750: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1755: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1760: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1765: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1770: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1775: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1780: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1785: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1790: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1795: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1800: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1805: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1810: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1815: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1820: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1825: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1830: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1835: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1840: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1845: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1850: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1855: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1860: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1865: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1870: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1875: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1880: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1885: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1890: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1895: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1900: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1905: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1910: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1915: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1920: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1925: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1930: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1935: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1940: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1945: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1950: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1955: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1960: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1965: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1970: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1975: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1980: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1985: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1990: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1995: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step  0: w = [-0.00088966 -0.02219324 -0.00279509], loss = 1.4782171249389648\n",
      "step  5: w = [-0.00571141 -0.12774374 -0.01566323], loss = 1.2533844709396362\n",
      "step 10: w = [-0.01106317 -0.22481343 -0.02683333], loss = 1.0633506774902344\n",
      "step 15: w = [-0.01683181 -0.31409633 -0.03649034], loss = 0.902643620967865\n",
      "step 20: w = [-0.02291877 -0.39622876 -0.04480066], loss = 0.7666628360748291\n",
      "step 25: w = [-0.02923849 -0.47179416 -0.05191391], loss = 0.6515409350395203\n",
      "step 30: w = [-0.03571691 -0.5413276  -0.05796458], loss = 0.5540241599082947\n",
      "step 35: w = [-0.04229026 -0.6053198  -0.06307346], loss = 0.47137394547462463\n",
      "step 40: w = [-0.04890383 -0.6642209  -0.06734898], loss = 0.4012843072414398\n",
      "step 45: w = [-0.05551092 -0.71844393 -0.07088839], loss = 0.3418126106262207\n",
      "step 50: w = [-0.06207199 -0.76836765 -0.07377888], loss = 0.29132184386253357\n",
      "step 55: w = [-0.06855375 -0.81433976 -0.07609855], loss = 0.24843107163906097\n",
      "step 60: w = [-0.07492846 -0.85667926 -0.0779173 ], loss = 0.21197549998760223\n",
      "step 65: w = [-0.0811732  -0.89567894 -0.07929762], loss = 0.1809719204902649\n",
      "step 70: w = [-0.08726934 -0.9316077  -0.08029536], loss = 0.15458963811397552\n",
      "step 75: w = [-0.09320199 -0.9647123  -0.08096036], loss = 0.13212697207927704\n",
      "step 80: w = [-0.09895948 -0.9952192  -0.08133703], loss = 0.11299077421426773\n",
      "step 85: w = [-0.10453304 -1.0233364  -0.08146498], loss = 0.09667900949716568\n",
      "step 90: w = [-0.1099163  -1.0492553  -0.08137938], loss = 0.0827668234705925\n",
      "step 95: w = [-0.11510506 -1.073151   -0.0811115 ], loss = 0.07089464366436005\n",
      "step 100: w = [-0.12009692 -1.095185   -0.08068907], loss = 0.06075745448470116\n",
      "step 105: w = [-0.12489107 -1.1155055  -0.08013667], loss = 0.05209700018167496\n",
      "step 110: w = [-0.129488   -1.1342483  -0.07947601], loss = 0.04469400271773338\n",
      "step 115: w = [-0.1338894  -1.1515386  -0.07872622], loss = 0.03836241737008095\n",
      "step 120: w = [-0.13809785 -1.1674914  -0.07790422], loss = 0.032944247126579285\n",
      "step 125: w = [-0.14211673 -1.1822124  -0.07702482], loss = 0.02830524556338787\n",
      "step 130: w = [-0.14595006 -1.1957989  -0.07610103], loss = 0.02433115802705288\n",
      "step 135: w = [-0.14960243 -1.20834    -0.07514418], loss = 0.020924946293234825\n",
      "step 140: w = [-0.1530788  -1.2199183  -0.07416419], loss = 0.01800393871963024\n",
      "step 145: w = [-0.15638447 -1.2306088  -0.07316965], loss = 0.015497765503823757\n",
      "step 150: w = [-0.159525   -1.2404815  -0.07216798], loss = 0.013346395455300808\n",
      "step 155: w = [-0.16250612 -1.2496003  -0.07116558], loss = 0.01149867195636034\n",
      "step 160: w = [-0.16533363 -1.258024   -0.07016794], loss = 0.009910988621413708\n",
      "step 165: w = [-0.16801345 -1.2658066  -0.0691797 ], loss = 0.008546113967895508\n",
      "step 170: w = [-0.1705515  -1.272998   -0.06820481], loss = 0.007372215390205383\n",
      "step 175: w = [-0.1729536  -1.2796444  -0.06724657], loss = 0.006362080108374357\n",
      "step 180: w = [-0.1752256  -1.2857877  -0.06630771], loss = 0.005492504220455885\n",
      "step 185: w = [-0.17737326 -1.2914672  -0.06539045], loss = 0.004743561614304781\n",
      "step 190: w = [-0.17940219 -1.2967184  -0.0644966 ], loss = 0.004098270554095507\n",
      "step 195: w = [-0.1813179  -1.3015742  -0.06362756], loss = 0.003542054910212755\n",
      "step 200: w = [-0.18312576 -1.3060653  -0.06278441], loss = 0.0030624084174633026\n",
      "step 205: w = [-0.18483101 -1.3102195  -0.06196793], loss = 0.002648627618327737\n",
      "step 210: w = [-0.18643868 -1.3140627  -0.06117864], loss = 0.002291532000526786\n",
      "step 215: w = [-0.18795365 -1.317619   -0.06041684], loss = 0.0019832116086035967\n",
      "step 220: w = [-0.18938068 -1.32091    -0.05968264], loss = 0.0017169226193800569\n",
      "step 225: w = [-0.19072427 -1.3239561  -0.058976  ], loss = 0.001486839959397912\n",
      "step 230: w = [-0.19198881 -1.3267758  -0.05829672], loss = 0.0012879838468506932\n",
      "step 235: w = [-0.19317849 -1.3293865  -0.05764448], loss = 0.0011160433059558272\n",
      "step 240: w = [-0.1942973  -1.3318037  -0.05701887], loss = 0.0009673374588601291\n",
      "step 245: w = [-0.1953491  -1.3340423  -0.05641939], loss = 0.0008386755362153053\n",
      "step 250: w = [-0.19633758 -1.3361157  -0.05584549], loss = 0.0007273252704180777\n",
      "step 255: w = [-0.19726622 -1.3380364  -0.05529653], loss = 0.0006309249438345432\n",
      "step 260: w = [-0.19813839 -1.3398159  -0.05477187], loss = 0.0005474438657984138\n",
      "step 265: w = [-0.19895725 -1.3414648  -0.05427079], loss = 0.000475125212687999\n",
      "step 270: w = [-0.19972584 -1.3429927  -0.05379257], loss = 0.00041246606269851327\n",
      "step 275: w = [-0.20044704 -1.3444091  -0.05333648], loss = 0.000358146004145965\n",
      "step 280: w = [-0.20112358 -1.345722   -0.05290175], loss = 0.00031105769448913634\n",
      "step 285: w = [-0.20175806 -1.3469392  -0.05248763], loss = 0.00027021855930797756\n",
      "step 290: w = [-0.20235293 -1.3480679  -0.05209337], loss = 0.0002347925037611276\n",
      "step 295: w = [-0.20291051 -1.3491144  -0.05171819], loss = 0.0002040572726400569\n",
      "step 300: w = [-0.20343304 -1.3500851  -0.05136136], loss = 0.00017738206952344626\n",
      "step 305: w = [-0.20392258 -1.3509855  -0.05102212], loss = 0.00015422631986439228\n",
      "step 310: w = [-0.20438112 -1.3518208  -0.05069977], loss = 0.0001341203460469842\n",
      "step 315: w = [-0.20481053 -1.352596   -0.05039357], loss = 0.0001166555448435247\n",
      "step 320: w = [-0.20521255 -1.3533154  -0.05010284], loss = 0.00010148554429179057\n",
      "step 325: w = [-0.20558886 -1.353983   -0.04982689], loss = 8.830326260067523e-05\n",
      "step 330: w = [-0.20594104 -1.3546027  -0.04956506], loss = 7.684822048759088e-05\n",
      "step 335: w = [-0.20627056 -1.355178   -0.04931672], loss = 6.689022848149762e-05\n",
      "step 340: w = [-0.20657884 -1.3557122  -0.04908124], loss = 5.823226092616096e-05\n",
      "step 345: w = [-0.20686716 -1.3562081  -0.04885802], loss = 5.070469342172146e-05\n",
      "step 350: w = [-0.20713678 -1.3566686  -0.0486465 ], loss = 4.415765215526335e-05\n",
      "step 355: w = [-0.20738886 -1.3570963  -0.0484461 ], loss = 3.846233812510036e-05\n",
      "step 360: w = [-0.20762451 -1.3574935  -0.04825629], loss = 3.350788028910756e-05\n",
      "step 365: w = [-0.20784473 -1.3578626  -0.04807657], loss = 2.91956530418247e-05\n",
      "step 370: w = [-0.20805053 -1.3582056  -0.04790642], loss = 2.5442399419262074e-05\n",
      "step 375: w = [-0.2082428  -1.3585241  -0.04774538], loss = 2.2175812773639336e-05\n",
      "step 380: w = [-0.20842241 -1.3588201  -0.047593  ], loss = 1.9331651856191456e-05\n",
      "step 385: w = [-0.20859015 -1.3590952  -0.04744883], loss = 1.6854804925969802e-05\n",
      "step 390: w = [-0.2087468  -1.359351   -0.04731245], loss = 1.4697196093038656e-05\n",
      "step 395: w = [-0.20889308 -1.3595889  -0.04718347], loss = 1.2817569768230896e-05\n",
      "step 400: w = [-0.20902961 -1.35981    -0.04706152], loss = 1.1180098226759583e-05\n",
      "step 405: w = [-0.20915706 -1.3600156  -0.04694622], loss = 9.752910955285188e-06\n",
      "step 410: w = [-0.209276   -1.3602068  -0.04683723], loss = 8.509292456437834e-06\n",
      "step 415: w = [-0.20938699 -1.3603847  -0.04673423], loss = 7.42532211006619e-06\n",
      "step 420: w = [-0.20949054 -1.36055    -0.04663688], loss = 6.480412594100926e-06\n",
      "step 425: w = [-0.20958713 -1.360704   -0.04654491], loss = 5.656574103340972e-06\n",
      "step 430: w = [-0.20967722 -1.3608471  -0.04645802], loss = 4.938020083500305e-06\n",
      "step 435: w = [-0.20976125 -1.3609803  -0.04637593], loss = 4.311373231757898e-06\n",
      "step 440: w = [-0.20983958 -1.3611042  -0.04629841], loss = 3.764741222767043e-06\n",
      "step 445: w = [-0.20991263 -1.3612195  -0.04622519], loss = 3.2880100206966745e-06\n",
      "step 450: w = [-0.20998073 -1.3613268  -0.04615606], loss = 2.8720760383293964e-06\n",
      "step 455: w = [-0.2100442  -1.3614267  -0.04609078], loss = 2.5089846076298272e-06\n",
      "step 460: w = [-0.21010336 -1.3615198  -0.04602915], loss = 2.191902240156196e-06\n",
      "step 465: w = [-0.21015848 -1.3616062  -0.04597097], loss = 1.915655957418494e-06\n",
      "step 470: w = [-0.21020983 -1.3616867  -0.04591605], loss = 1.6743696278354037e-06\n",
      "step 475: w = [-0.21025766 -1.3617618  -0.04586421], loss = 1.4634350691267173e-06\n",
      "step 480: w = [-0.2103022  -1.3618318  -0.04581529], loss = 1.2792394272764795e-06\n",
      "step 485: w = [-0.2103437  -1.3618969  -0.04576911], loss = 1.118470549954509e-06\n",
      "step 490: w = [-0.21038234 -1.3619576  -0.04572554], loss = 9.779550964594819e-07\n",
      "step 495: w = [-0.21041833 -1.3620139  -0.04568443], loss = 8.55334860716539e-07\n",
      "step 500: w = [-0.21045181 -1.3620666  -0.04564565], loss = 7.480426802430884e-07\n",
      "step 505: w = [-0.21048295 -1.3621156  -0.04560905], loss = 6.544298685184913e-07\n",
      "step 510: w = [-0.21051197 -1.3621614  -0.04557452], loss = 5.724683660446317e-07\n",
      "step 515: w = [-0.21053897 -1.3622037  -0.04554195], loss = 5.011162897972099e-07\n",
      "step 520: w = [-0.21056409 -1.3622434  -0.04551123], loss = 4.385454417388246e-07\n",
      "step 525: w = [-0.21058747 -1.3622804  -0.04548226], loss = 3.838391933186358e-07\n",
      "step 530: w = [-0.21060921 -1.3623148  -0.04545492], loss = 3.360236462413013e-07\n",
      "step 535: w = [-0.21062942 -1.3623469  -0.04542914], loss = 2.942037724551483e-07\n",
      "step 540: w = [-0.21064822 -1.3623769  -0.04540483], loss = 2.5755718979780795e-07\n",
      "step 545: w = [-0.21066572 -1.3624048  -0.0453819 ], loss = 2.2557348700047442e-07\n",
      "step 550: w = [-0.21068197 -1.3624308  -0.04536027], loss = 1.9758222435939388e-07\n",
      "step 555: w = [-0.2106971  -1.3624551  -0.04533987], loss = 1.7305235644471395e-07\n",
      "step 560: w = [-0.21071114 -1.3624777  -0.04532065], loss = 1.5165473143952113e-07\n",
      "step 565: w = [-0.21072419 -1.3624986  -0.04530252], loss = 1.3293514200540812e-07\n",
      "step 570: w = [-0.21073632 -1.3625183  -0.04528542], loss = 1.165161265248571e-07\n",
      "step 575: w = [-0.21074758 -1.3625367  -0.0452693 ], loss = 1.0212152545818753e-07\n",
      "step 580: w = [-0.21075806 -1.3625538  -0.0452541 ], loss = 8.95181315740956e-08\n",
      "step 585: w = [-0.21076779 -1.3625698  -0.04523977], loss = 7.84756011285026e-08\n",
      "step 590: w = [-0.21077682 -1.3625847  -0.04522626], loss = 6.88240078261515e-08\n",
      "step 595: w = [-0.21078521 -1.3625987  -0.04521352], loss = 6.032636434838423e-08\n",
      "step 600: w = [-0.21079299 -1.3626115  -0.04520151], loss = 5.2953470941474734e-08\n",
      "step 605: w = [-0.21080022 -1.3626237  -0.04519018], loss = 4.6436223755108585e-08\n",
      "step 610: w = [-0.21080692 -1.3626349  -0.0451795 ], loss = 4.076228421467931e-08\n",
      "step 615: w = [-0.21081315 -1.3626454  -0.04516944], loss = 3.5781155816039245e-08\n",
      "step 620: w = [-0.21081893 -1.3626552  -0.04515996], loss = 3.142310589510089e-08\n",
      "step 625: w = [-0.2108243  -1.3626643  -0.04515101], loss = 2.758154415971603e-08\n",
      "step 630: w = [-0.21082929 -1.3626729  -0.04514258], loss = 2.4212027938119718e-08\n",
      "step 635: w = [-0.2108339  -1.3626809  -0.04513463], loss = 2.1262229310536895e-08\n",
      "step 640: w = [-0.21083817 -1.3626884  -0.04512713], loss = 1.8658511180547066e-08\n",
      "step 645: w = [-0.21084213 -1.3626955  -0.04512007], loss = 1.6385655499107088e-08\n",
      "step 650: w = [-0.21084581 -1.362702   -0.04511341], loss = 1.4386327684690059e-08\n",
      "step 655: w = [-0.21084921 -1.362708   -0.04510713], loss = 1.2654640890730207e-08\n",
      "step 660: w = [-0.21085237 -1.3627137  -0.04510122], loss = 1.11148645842718e-08\n",
      "step 665: w = [-0.21085529 -1.362719   -0.04509564], loss = 9.762485930764342e-09\n",
      "step 670: w = [-0.21085799 -1.362724   -0.04509038], loss = 8.587958788552896e-09\n",
      "step 675: w = [-0.2108605  -1.3627287  -0.04508542], loss = 7.539595614503014e-09\n",
      "step 680: w = [-0.21086283 -1.3627329  -0.04508075], loss = 6.638980476481038e-09\n",
      "step 685: w = [-0.21086498 -1.362737   -0.04507634], loss = 5.83146109178756e-09\n",
      "step 690: w = [-0.21086697 -1.3627406  -0.04507219], loss = 5.136168379493711e-09\n",
      "step 695: w = [-0.2108688  -1.3627442  -0.04506828], loss = 4.517523688463143e-09\n",
      "step 700: w = [-0.2108705  -1.3627474  -0.04506459], loss = 3.9782648286745825e-09\n",
      "step 705: w = [-0.21087207 -1.3627504  -0.04506111], loss = 3.5078664417653727e-09\n",
      "step 710: w = [-0.21087353 -1.3627534  -0.04505783], loss = 3.080769861441013e-09\n",
      "step 715: w = [-0.21087489 -1.362756   -0.04505474], loss = 2.715595748270516e-09\n",
      "step 720: w = [-0.21087612 -1.3627584  -0.04505183], loss = 2.396349119138108e-09\n",
      "step 725: w = [-0.21087727 -1.3627608  -0.04504909], loss = 2.1100448055477727e-09\n",
      "step 730: w = [-0.21087833 -1.3627632  -0.0450465 ], loss = 1.8499527465465349e-09\n",
      "step 735: w = [-0.21087931 -1.3627651  -0.04504406], loss = 1.6331143104508783e-09\n",
      "step 740: w = [-0.21088022 -1.3627669  -0.04504176], loss = 1.4455355801246128e-09\n",
      "step 745: w = [-0.21088105 -1.3627687  -0.04503959], loss = 1.2728593734578908e-09\n",
      "step 750: w = [-0.21088183 -1.3627704  -0.04503755], loss = 1.118332426663926e-09\n",
      "step 755: w = [-0.21088254 -1.3627722  -0.04503563], loss = 9.786326193861328e-10\n",
      "step 760: w = [-0.21088322 -1.3627734  -0.04503381], loss = 8.696714459688337e-10\n",
      "step 765: w = [-0.21088381 -1.3627746  -0.0450321 ], loss = 7.71816777156431e-10\n",
      "step 770: w = [-0.21088436 -1.3627758  -0.04503049], loss = 6.84391043836996e-10\n",
      "step 775: w = [-0.21088488 -1.362777   -0.04502897], loss = 6.038139432895662e-10\n",
      "step 780: w = [-0.21088535 -1.3627782  -0.04502754], loss = 5.309295225686128e-10\n",
      "step 785: w = [-0.2108858  -1.3627794  -0.04502618], loss = 4.655782148699217e-10\n",
      "step 790: w = [-0.21088618 -1.3627806  -0.04502491], loss = 4.074275639087688e-10\n",
      "step 795: w = [-0.21088655 -1.3627815  -0.04502371], loss = 3.5810457377216665e-10\n",
      "step 800: w = [-0.2108869  -1.3627821  -0.04502257], loss = 3.207661081194857e-10\n",
      "step 805: w = [-0.2108872 -1.3627827 -0.0450215], loss = 2.8615065872372725e-10\n",
      "step 810: w = [-0.21088749 -1.3627833  -0.0450205 ], loss = 2.555620992605867e-10\n",
      "step 815: w = [-0.21088775 -1.3627839  -0.04501956], loss = 2.2824404866117476e-10\n",
      "step 820: w = [-0.21088797 -1.3627845  -0.04501867], loss = 2.021319778222619e-10\n",
      "step 825: w = [-0.21088819 -1.3627851  -0.04501782], loss = 1.7984225220146755e-10\n",
      "step 830: w = [-0.21088842 -1.3627857  -0.04501704], loss = 1.588409681563263e-10\n",
      "step 835: w = [-0.2108886  -1.3627863  -0.04501629], loss = 1.4032240647221528e-10\n",
      "step 840: w = [-0.21088874 -1.3627869  -0.04501558], loss = 1.2380452218963e-10\n",
      "step 845: w = [-0.21088889 -1.3627875  -0.04501492], loss = 1.0838168557070205e-10\n",
      "step 850: w = [-0.21088904 -1.3627881  -0.04501429], loss = 9.430489028572353e-11\n",
      "step 855: w = [-0.21088919 -1.3627887  -0.04501369], loss = 8.214612323698134e-11\n",
      "step 860: w = [-0.21088934 -1.3627893  -0.04501313], loss = 7.069455437713756e-11\n",
      "step 865: w = [-0.21088946 -1.3627899  -0.0450126 ], loss = 6.094761501040935e-11\n",
      "step 870: w = [-0.21088953 -1.3627901  -0.0450121 ], loss = 5.4538373817081265e-11\n",
      "step 875: w = [-0.21088961 -1.3627902  -0.04501162], loss = 4.937165259955023e-11\n",
      "step 880: w = [-0.21088968 -1.3627902  -0.04501119], loss = 4.486772064993261e-11\n",
      "step 885: w = [-0.21088976 -1.3627903  -0.04501077], loss = 4.101307488069139e-11\n",
      "step 890: w = [-0.21088983 -1.3627903  -0.04501038], loss = 3.774569545811346e-11\n",
      "step 895: w = [-0.2108899  -1.3627903  -0.04501001], loss = 3.5108419643714583e-11\n",
      "step 900: w = [-0.21088998 -1.3627905  -0.04500967], loss = 3.214582419697187e-11\n",
      "step 905: w = [-0.21089005 -1.3627905  -0.04500935], loss = 2.988847017104668e-11\n",
      "step 910: w = [-0.2108901  -1.3627905  -0.04500905], loss = 2.7935945223767078e-11\n",
      "step 915: w = [-0.2108901  -1.3627906  -0.04500876], loss = 2.562542365802667e-11\n",
      "step 920: w = [-0.2108901  -1.3627906  -0.04500849], loss = 2.445288763142095e-11\n",
      "step 925: w = [-0.2108901  -1.3627906  -0.04500824], loss = 2.3024129477966504e-11\n",
      "step 930: w = [-0.2108901  -1.3627906  -0.04500801], loss = 2.195326385956431e-11\n",
      "step 935: w = [-0.2108901  -1.3627907  -0.04500778], loss = 2.034777901727125e-11\n",
      "step 940: w = [-0.2108901  -1.3627907  -0.04500758], loss = 1.936180209216598e-11\n",
      "step 945: w = [-0.2108901  -1.3627907  -0.04500739], loss = 1.8589832798121542e-11\n",
      "step 950: w = [-0.2108901 -1.3627907 -0.0450072], loss = 1.793150176954139e-11\n",
      "step 955: w = [-0.2108901  -1.3627907  -0.04500703], loss = 1.744032696038289e-11\n",
      "step 960: w = [-0.2108901  -1.3627908  -0.04500687], loss = 1.6150114232060808e-11\n",
      "step 965: w = [-0.2108901  -1.3627908  -0.04500672], loss = 1.5638670913808994e-11\n",
      "step 970: w = [-0.2108901  -1.3627908  -0.04500658], loss = 1.534138614756042e-11\n",
      "step 975: w = [-0.2108901  -1.3627908  -0.04500645], loss = 1.4905533404774296e-11\n",
      "step 980: w = [-0.2108901  -1.3627908  -0.04500632], loss = 1.452193140044633e-11\n",
      "step 985: w = [-0.2108901 -1.362791  -0.0450062], loss = 1.3677315356674935e-11\n",
      "step 990: w = [-0.2108901  -1.362791   -0.04500609], loss = 1.3262427614457728e-11\n",
      "step 995: w = [-0.2108901  -1.362791   -0.04500598], loss = 1.3191406300627762e-11\n",
      "step 1000: w = [-0.2108901  -1.362791   -0.04500589], loss = 1.2943116196872939e-11\n",
      "step 1005: w = [-0.2108901  -1.362791   -0.04500579], loss = 1.2801437861142961e-11\n",
      "step 1010: w = [-0.2108901 -1.362791  -0.0450057], loss = 1.263618376601272e-11\n",
      "step 1015: w = [-0.2108901  -1.362791   -0.04500563], loss = 1.245041830050253e-11\n",
      "step 1020: w = [-0.2108901  -1.362791   -0.04500555], loss = 1.2284550980623532e-11\n",
      "step 1025: w = [-0.2108901  -1.362791   -0.04500548], loss = 1.2363446204310957e-11\n",
      "step 1030: w = [-0.2108901  -1.362791   -0.04500541], loss = 1.2194733937931357e-11\n",
      "step 1035: w = [-0.2108901  -1.362791   -0.04500535], loss = 1.2141876913618344e-11\n",
      "step 1040: w = [-0.2108901 -1.362791  -0.0450053], loss = 1.2162240832502835e-11\n",
      "step 1045: w = [-0.2108901  -1.362791   -0.04500524], loss = 1.196619799775922e-11\n",
      "step 1050: w = [-0.2108901  -1.362791   -0.04500518], loss = 1.1992462578547247e-11\n",
      "step 1055: w = [-0.2108901  -1.3627911  -0.04500513], loss = 1.116903531467317e-11\n",
      "step 1060: w = [-0.2108901  -1.3627911  -0.04500508], loss = 1.1066314531404942e-11\n",
      "step 1065: w = [-0.2108901  -1.3627911  -0.04500504], loss = 1.1000237179481509e-11\n",
      "step 1070: w = [-0.2108901 -1.3627911 -0.045005 ], loss = 1.0940296411854344e-11\n",
      "step 1075: w = [-0.2108901  -1.3627911  -0.04500497], loss = 1.0880721670880611e-11\n",
      "step 1080: w = [-0.2108901  -1.3627911  -0.04500493], loss = 1.0983158826860517e-11\n",
      "step 1085: w = [-0.2108901  -1.3627911  -0.04500489], loss = 1.0963785435080808e-11\n",
      "step 1090: w = [-0.2108901  -1.3627911  -0.04500486], loss = 1.0868384317519464e-11\n",
      "step 1095: w = [-0.2108901  -1.3627911  -0.04500482], loss = 1.070844801720483e-11\n",
      "step 1100: w = [-0.2108901  -1.3627911  -0.04500479], loss = 1.0769530232879188e-11\n",
      "step 1105: w = [-0.2108901  -1.3627911  -0.04500477], loss = 1.0761713568896436e-11\n",
      "step 1110: w = [-0.2108901  -1.3627911  -0.04500475], loss = 1.0759804505711124e-11\n",
      "step 1115: w = [-0.2108901  -1.3627911  -0.04500473], loss = 1.0769852024083981e-11\n",
      "step 1120: w = [-0.2108901  -1.3627911  -0.04500471], loss = 1.0859468706214681e-11\n",
      "step 1125: w = [-0.2108901 -1.3627911 -0.0450047], loss = 1.0954648646532839e-11\n",
      "step 1130: w = [-0.2108901  -1.3627911  -0.04500468], loss = 1.0946246513376945e-11\n",
      "step 1135: w = [-0.2108901  -1.3627911  -0.04500466], loss = 1.09385304633558e-11\n",
      "step 1140: w = [-0.2108901  -1.3627911  -0.04500464], loss = 1.1005910592609691e-11\n",
      "step 1145: w = [-0.2108901  -1.3627911  -0.04500462], loss = 1.0951886966759083e-11\n",
      "step 1150: w = [-0.2108901 -1.3627911 -0.0450046], loss = 1.0916947901229435e-11\n",
      "step 1155: w = [-0.2108901  -1.3627911  -0.04500458], loss = 1.0777193373834315e-11\n",
      "step 1160: w = [-0.2108901  -1.3627911  -0.04500457], loss = 1.078157615269637e-11\n",
      "step 1165: w = [-0.2108901  -1.3627911  -0.04500455], loss = 1.0797363003689497e-11\n",
      "step 1170: w = [-0.2108901  -1.3627911  -0.04500453], loss = 1.0791124070708147e-11\n",
      "step 1175: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0831702722258196e-11\n",
      "step 1180: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1185: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1190: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1195: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1200: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1205: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1210: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1215: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1220: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1225: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1230: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1235: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1240: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1245: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1250: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1255: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1260: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1265: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1270: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1275: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1280: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1285: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1290: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1295: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1300: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1305: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1310: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1315: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1320: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1325: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1330: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1335: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1340: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1345: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1350: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1355: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1360: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1365: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1370: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1375: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1380: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1385: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1390: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1395: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1400: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1405: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1410: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1415: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1420: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1425: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1430: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1435: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1440: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1445: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1450: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1455: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1460: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1465: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1470: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1475: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1480: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1485: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1490: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1495: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1500: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1505: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1510: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1515: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1520: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1525: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1530: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1535: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1540: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1545: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1550: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1555: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1560: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1565: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1570: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1575: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1580: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1585: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1590: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1595: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1600: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1605: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1610: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1615: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1620: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1625: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1630: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1635: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1640: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1645: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1650: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1655: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1660: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1665: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1670: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1675: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1680: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1685: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1690: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1695: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1700: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1705: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1710: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1715: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1720: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1725: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1730: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1735: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1740: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1745: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1750: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1755: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1760: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1765: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1770: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1775: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1780: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1785: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1790: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1795: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1800: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1805: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1810: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1815: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1820: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1825: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1830: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1835: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1840: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1845: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1850: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1855: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1860: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1865: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1870: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1875: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1880: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1885: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1890: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1895: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1900: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1905: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1910: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1915: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1920: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1925: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1930: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1935: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1940: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1945: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1950: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1955: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1960: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1965: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1970: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1975: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1980: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1985: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1990: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1995: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step  0: w = [-0.00088966 -0.02219324 -0.00279509], loss = 1.4782171249389648\n",
      "step  5: w = [-0.00571141 -0.12774374 -0.01566323], loss = 1.2533844709396362\n",
      "step 10: w = [-0.01106317 -0.22481343 -0.02683333], loss = 1.0633506774902344\n",
      "step 15: w = [-0.01683181 -0.31409633 -0.03649034], loss = 0.902643620967865\n",
      "step 20: w = [-0.02291877 -0.39622876 -0.04480066], loss = 0.7666628360748291\n",
      "step 25: w = [-0.02923849 -0.47179416 -0.05191391], loss = 0.6515409350395203\n",
      "step 30: w = [-0.03571691 -0.5413276  -0.05796458], loss = 0.5540241599082947\n",
      "step 35: w = [-0.04229026 -0.6053198  -0.06307346], loss = 0.47137394547462463\n",
      "step 40: w = [-0.04890383 -0.6642209  -0.06734898], loss = 0.4012843072414398\n",
      "step 45: w = [-0.05551092 -0.71844393 -0.07088839], loss = 0.3418126106262207\n",
      "step 50: w = [-0.06207199 -0.76836765 -0.07377888], loss = 0.29132184386253357\n",
      "step 55: w = [-0.06855375 -0.81433976 -0.07609855], loss = 0.24843107163906097\n",
      "step 60: w = [-0.07492846 -0.85667926 -0.0779173 ], loss = 0.21197549998760223\n",
      "step 65: w = [-0.0811732  -0.89567894 -0.07929762], loss = 0.1809719204902649\n",
      "step 70: w = [-0.08726934 -0.9316077  -0.08029536], loss = 0.15458963811397552\n",
      "step 75: w = [-0.09320199 -0.9647123  -0.08096036], loss = 0.13212697207927704\n",
      "step 80: w = [-0.09895948 -0.9952192  -0.08133703], loss = 0.11299077421426773\n",
      "step 85: w = [-0.10453304 -1.0233364  -0.08146498], loss = 0.09667900949716568\n",
      "step 90: w = [-0.1099163  -1.0492553  -0.08137938], loss = 0.0827668234705925\n",
      "step 95: w = [-0.11510506 -1.073151   -0.0811115 ], loss = 0.07089464366436005\n",
      "step 100: w = [-0.12009692 -1.095185   -0.08068907], loss = 0.06075745448470116\n",
      "step 105: w = [-0.12489107 -1.1155055  -0.08013667], loss = 0.05209700018167496\n",
      "step 110: w = [-0.129488   -1.1342483  -0.07947601], loss = 0.04469400271773338\n",
      "step 115: w = [-0.1338894  -1.1515386  -0.07872622], loss = 0.03836241737008095\n",
      "step 120: w = [-0.13809785 -1.1674914  -0.07790422], loss = 0.032944247126579285\n",
      "step 125: w = [-0.14211673 -1.1822124  -0.07702482], loss = 0.02830524556338787\n",
      "step 130: w = [-0.14595006 -1.1957989  -0.07610103], loss = 0.02433115802705288\n",
      "step 135: w = [-0.14960243 -1.20834    -0.07514418], loss = 0.020924946293234825\n",
      "step 140: w = [-0.1530788  -1.2199183  -0.07416419], loss = 0.01800393871963024\n",
      "step 145: w = [-0.15638447 -1.2306088  -0.07316965], loss = 0.015497765503823757\n",
      "step 150: w = [-0.159525   -1.2404815  -0.07216798], loss = 0.013346395455300808\n",
      "step 155: w = [-0.16250612 -1.2496003  -0.07116558], loss = 0.01149867195636034\n",
      "step 160: w = [-0.16533363 -1.258024   -0.07016794], loss = 0.009910988621413708\n",
      "step 165: w = [-0.16801345 -1.2658066  -0.0691797 ], loss = 0.008546113967895508\n",
      "step 170: w = [-0.1705515  -1.272998   -0.06820481], loss = 0.007372215390205383\n",
      "step 175: w = [-0.1729536  -1.2796444  -0.06724657], loss = 0.006362080108374357\n",
      "step 180: w = [-0.1752256  -1.2857877  -0.06630771], loss = 0.005492504220455885\n",
      "step 185: w = [-0.17737326 -1.2914672  -0.06539045], loss = 0.004743561614304781\n",
      "step 190: w = [-0.17940219 -1.2967184  -0.0644966 ], loss = 0.004098270554095507\n",
      "step 195: w = [-0.1813179  -1.3015742  -0.06362756], loss = 0.003542054910212755\n",
      "step 200: w = [-0.18312576 -1.3060653  -0.06278441], loss = 0.0030624084174633026\n",
      "step 205: w = [-0.18483101 -1.3102195  -0.06196793], loss = 0.002648627618327737\n",
      "step 210: w = [-0.18643868 -1.3140627  -0.06117864], loss = 0.002291532000526786\n",
      "step 215: w = [-0.18795365 -1.317619   -0.06041684], loss = 0.0019832116086035967\n",
      "step 220: w = [-0.18938068 -1.32091    -0.05968264], loss = 0.0017169226193800569\n",
      "step 225: w = [-0.19072427 -1.3239561  -0.058976  ], loss = 0.001486839959397912\n",
      "step 230: w = [-0.19198881 -1.3267758  -0.05829672], loss = 0.0012879838468506932\n",
      "step 235: w = [-0.19317849 -1.3293865  -0.05764448], loss = 0.0011160433059558272\n",
      "step 240: w = [-0.1942973  -1.3318037  -0.05701887], loss = 0.0009673374588601291\n",
      "step 245: w = [-0.1953491  -1.3340423  -0.05641939], loss = 0.0008386755362153053\n",
      "step 250: w = [-0.19633758 -1.3361157  -0.05584549], loss = 0.0007273252704180777\n",
      "step 255: w = [-0.19726622 -1.3380364  -0.05529653], loss = 0.0006309249438345432\n",
      "step 260: w = [-0.19813839 -1.3398159  -0.05477187], loss = 0.0005474438657984138\n",
      "step 265: w = [-0.19895725 -1.3414648  -0.05427079], loss = 0.000475125212687999\n",
      "step 270: w = [-0.19972584 -1.3429927  -0.05379257], loss = 0.00041246606269851327\n",
      "step 275: w = [-0.20044704 -1.3444091  -0.05333648], loss = 0.000358146004145965\n",
      "step 280: w = [-0.20112358 -1.345722   -0.05290175], loss = 0.00031105769448913634\n",
      "step 285: w = [-0.20175806 -1.3469392  -0.05248763], loss = 0.00027021855930797756\n",
      "step 290: w = [-0.20235293 -1.3480679  -0.05209337], loss = 0.0002347925037611276\n",
      "step 295: w = [-0.20291051 -1.3491144  -0.05171819], loss = 0.0002040572726400569\n",
      "step 300: w = [-0.20343304 -1.3500851  -0.05136136], loss = 0.00017738206952344626\n",
      "step 305: w = [-0.20392258 -1.3509855  -0.05102212], loss = 0.00015422631986439228\n",
      "step 310: w = [-0.20438112 -1.3518208  -0.05069977], loss = 0.0001341203460469842\n",
      "step 315: w = [-0.20481053 -1.352596   -0.05039357], loss = 0.0001166555448435247\n",
      "step 320: w = [-0.20521255 -1.3533154  -0.05010284], loss = 0.00010148554429179057\n",
      "step 325: w = [-0.20558886 -1.353983   -0.04982689], loss = 8.830326260067523e-05\n",
      "step 330: w = [-0.20594104 -1.3546027  -0.04956506], loss = 7.684822048759088e-05\n",
      "step 335: w = [-0.20627056 -1.355178   -0.04931672], loss = 6.689022848149762e-05\n",
      "step 340: w = [-0.20657884 -1.3557122  -0.04908124], loss = 5.823226092616096e-05\n",
      "step 345: w = [-0.20686716 -1.3562081  -0.04885802], loss = 5.070469342172146e-05\n",
      "step 350: w = [-0.20713678 -1.3566686  -0.0486465 ], loss = 4.415765215526335e-05\n",
      "step 355: w = [-0.20738886 -1.3570963  -0.0484461 ], loss = 3.846233812510036e-05\n",
      "step 360: w = [-0.20762451 -1.3574935  -0.04825629], loss = 3.350788028910756e-05\n",
      "step 365: w = [-0.20784473 -1.3578626  -0.04807657], loss = 2.91956530418247e-05\n",
      "step 370: w = [-0.20805053 -1.3582056  -0.04790642], loss = 2.5442399419262074e-05\n",
      "step 375: w = [-0.2082428  -1.3585241  -0.04774538], loss = 2.2175812773639336e-05\n",
      "step 380: w = [-0.20842241 -1.3588201  -0.047593  ], loss = 1.9331651856191456e-05\n",
      "step 385: w = [-0.20859015 -1.3590952  -0.04744883], loss = 1.6854804925969802e-05\n",
      "step 390: w = [-0.2087468  -1.359351   -0.04731245], loss = 1.4697196093038656e-05\n",
      "step 395: w = [-0.20889308 -1.3595889  -0.04718347], loss = 1.2817569768230896e-05\n",
      "step 400: w = [-0.20902961 -1.35981    -0.04706152], loss = 1.1180098226759583e-05\n",
      "step 405: w = [-0.20915706 -1.3600156  -0.04694622], loss = 9.752910955285188e-06\n",
      "step 410: w = [-0.209276   -1.3602068  -0.04683723], loss = 8.509292456437834e-06\n",
      "step 415: w = [-0.20938699 -1.3603847  -0.04673423], loss = 7.42532211006619e-06\n",
      "step 420: w = [-0.20949054 -1.36055    -0.04663688], loss = 6.480412594100926e-06\n",
      "step 425: w = [-0.20958713 -1.360704   -0.04654491], loss = 5.656574103340972e-06\n",
      "step 430: w = [-0.20967722 -1.3608471  -0.04645802], loss = 4.938020083500305e-06\n",
      "step 435: w = [-0.20976125 -1.3609803  -0.04637593], loss = 4.311373231757898e-06\n",
      "step 440: w = [-0.20983958 -1.3611042  -0.04629841], loss = 3.764741222767043e-06\n",
      "step 445: w = [-0.20991263 -1.3612195  -0.04622519], loss = 3.2880100206966745e-06\n",
      "step 450: w = [-0.20998073 -1.3613268  -0.04615606], loss = 2.8720760383293964e-06\n",
      "step 455: w = [-0.2100442  -1.3614267  -0.04609078], loss = 2.5089846076298272e-06\n",
      "step 460: w = [-0.21010336 -1.3615198  -0.04602915], loss = 2.191902240156196e-06\n",
      "step 465: w = [-0.21015848 -1.3616062  -0.04597097], loss = 1.915655957418494e-06\n",
      "step 470: w = [-0.21020983 -1.3616867  -0.04591605], loss = 1.6743696278354037e-06\n",
      "step 475: w = [-0.21025766 -1.3617618  -0.04586421], loss = 1.4634350691267173e-06\n",
      "step 480: w = [-0.2103022  -1.3618318  -0.04581529], loss = 1.2792394272764795e-06\n",
      "step 485: w = [-0.2103437  -1.3618969  -0.04576911], loss = 1.118470549954509e-06\n",
      "step 490: w = [-0.21038234 -1.3619576  -0.04572554], loss = 9.779550964594819e-07\n",
      "step 495: w = [-0.21041833 -1.3620139  -0.04568443], loss = 8.55334860716539e-07\n",
      "step 500: w = [-0.21045181 -1.3620666  -0.04564565], loss = 7.480426802430884e-07\n",
      "step 505: w = [-0.21048295 -1.3621156  -0.04560905], loss = 6.544298685184913e-07\n",
      "step 510: w = [-0.21051197 -1.3621614  -0.04557452], loss = 5.724683660446317e-07\n",
      "step 515: w = [-0.21053897 -1.3622037  -0.04554195], loss = 5.011162897972099e-07\n",
      "step 520: w = [-0.21056409 -1.3622434  -0.04551123], loss = 4.385454417388246e-07\n",
      "step 525: w = [-0.21058747 -1.3622804  -0.04548226], loss = 3.838391933186358e-07\n",
      "step 530: w = [-0.21060921 -1.3623148  -0.04545492], loss = 3.360236462413013e-07\n",
      "step 535: w = [-0.21062942 -1.3623469  -0.04542914], loss = 2.942037724551483e-07\n",
      "step 540: w = [-0.21064822 -1.3623769  -0.04540483], loss = 2.5755718979780795e-07\n",
      "step 545: w = [-0.21066572 -1.3624048  -0.0453819 ], loss = 2.2557348700047442e-07\n",
      "step 550: w = [-0.21068197 -1.3624308  -0.04536027], loss = 1.9758222435939388e-07\n",
      "step 555: w = [-0.2106971  -1.3624551  -0.04533987], loss = 1.7305235644471395e-07\n",
      "step 560: w = [-0.21071114 -1.3624777  -0.04532065], loss = 1.5165473143952113e-07\n",
      "step 565: w = [-0.21072419 -1.3624986  -0.04530252], loss = 1.3293514200540812e-07\n",
      "step 570: w = [-0.21073632 -1.3625183  -0.04528542], loss = 1.165161265248571e-07\n",
      "step 575: w = [-0.21074758 -1.3625367  -0.0452693 ], loss = 1.0212152545818753e-07\n",
      "step 580: w = [-0.21075806 -1.3625538  -0.0452541 ], loss = 8.95181315740956e-08\n",
      "step 585: w = [-0.21076779 -1.3625698  -0.04523977], loss = 7.84756011285026e-08\n",
      "step 590: w = [-0.21077682 -1.3625847  -0.04522626], loss = 6.88240078261515e-08\n",
      "step 595: w = [-0.21078521 -1.3625987  -0.04521352], loss = 6.032636434838423e-08\n",
      "step 600: w = [-0.21079299 -1.3626115  -0.04520151], loss = 5.2953470941474734e-08\n",
      "step 605: w = [-0.21080022 -1.3626237  -0.04519018], loss = 4.6436223755108585e-08\n",
      "step 610: w = [-0.21080692 -1.3626349  -0.0451795 ], loss = 4.076228421467931e-08\n",
      "step 615: w = [-0.21081315 -1.3626454  -0.04516944], loss = 3.5781155816039245e-08\n",
      "step 620: w = [-0.21081893 -1.3626552  -0.04515996], loss = 3.142310589510089e-08\n",
      "step 625: w = [-0.2108243  -1.3626643  -0.04515101], loss = 2.758154415971603e-08\n",
      "step 630: w = [-0.21082929 -1.3626729  -0.04514258], loss = 2.4212027938119718e-08\n",
      "step 635: w = [-0.2108339  -1.3626809  -0.04513463], loss = 2.1262229310536895e-08\n",
      "step 640: w = [-0.21083817 -1.3626884  -0.04512713], loss = 1.8658511180547066e-08\n",
      "step 645: w = [-0.21084213 -1.3626955  -0.04512007], loss = 1.6385655499107088e-08\n",
      "step 650: w = [-0.21084581 -1.362702   -0.04511341], loss = 1.4386327684690059e-08\n",
      "step 655: w = [-0.21084921 -1.362708   -0.04510713], loss = 1.2654640890730207e-08\n",
      "step 660: w = [-0.21085237 -1.3627137  -0.04510122], loss = 1.11148645842718e-08\n",
      "step 665: w = [-0.21085529 -1.362719   -0.04509564], loss = 9.762485930764342e-09\n",
      "step 670: w = [-0.21085799 -1.362724   -0.04509038], loss = 8.587958788552896e-09\n",
      "step 675: w = [-0.2108605  -1.3627287  -0.04508542], loss = 7.539595614503014e-09\n",
      "step 680: w = [-0.21086283 -1.3627329  -0.04508075], loss = 6.638980476481038e-09\n",
      "step 685: w = [-0.21086498 -1.362737   -0.04507634], loss = 5.83146109178756e-09\n",
      "step 690: w = [-0.21086697 -1.3627406  -0.04507219], loss = 5.136168379493711e-09\n",
      "step 695: w = [-0.2108688  -1.3627442  -0.04506828], loss = 4.517523688463143e-09\n",
      "step 700: w = [-0.2108705  -1.3627474  -0.04506459], loss = 3.9782648286745825e-09\n",
      "step 705: w = [-0.21087207 -1.3627504  -0.04506111], loss = 3.5078664417653727e-09\n",
      "step 710: w = [-0.21087353 -1.3627534  -0.04505783], loss = 3.080769861441013e-09\n",
      "step 715: w = [-0.21087489 -1.362756   -0.04505474], loss = 2.715595748270516e-09\n",
      "step 720: w = [-0.21087612 -1.3627584  -0.04505183], loss = 2.396349119138108e-09\n",
      "step 725: w = [-0.21087727 -1.3627608  -0.04504909], loss = 2.1100448055477727e-09\n",
      "step 730: w = [-0.21087833 -1.3627632  -0.0450465 ], loss = 1.8499527465465349e-09\n",
      "step 735: w = [-0.21087931 -1.3627651  -0.04504406], loss = 1.6331143104508783e-09\n",
      "step 740: w = [-0.21088022 -1.3627669  -0.04504176], loss = 1.4455355801246128e-09\n",
      "step 745: w = [-0.21088105 -1.3627687  -0.04503959], loss = 1.2728593734578908e-09\n",
      "step 750: w = [-0.21088183 -1.3627704  -0.04503755], loss = 1.118332426663926e-09\n",
      "step 755: w = [-0.21088254 -1.3627722  -0.04503563], loss = 9.786326193861328e-10\n",
      "step 760: w = [-0.21088322 -1.3627734  -0.04503381], loss = 8.696714459688337e-10\n",
      "step 765: w = [-0.21088381 -1.3627746  -0.0450321 ], loss = 7.71816777156431e-10\n",
      "step 770: w = [-0.21088436 -1.3627758  -0.04503049], loss = 6.84391043836996e-10\n",
      "step 775: w = [-0.21088488 -1.362777   -0.04502897], loss = 6.038139432895662e-10\n",
      "step 780: w = [-0.21088535 -1.3627782  -0.04502754], loss = 5.309295225686128e-10\n",
      "step 785: w = [-0.2108858  -1.3627794  -0.04502618], loss = 4.655782148699217e-10\n",
      "step 790: w = [-0.21088618 -1.3627806  -0.04502491], loss = 4.074275639087688e-10\n",
      "step 795: w = [-0.21088655 -1.3627815  -0.04502371], loss = 3.5810457377216665e-10\n",
      "step 800: w = [-0.2108869  -1.3627821  -0.04502257], loss = 3.207661081194857e-10\n",
      "step 805: w = [-0.2108872 -1.3627827 -0.0450215], loss = 2.8615065872372725e-10\n",
      "step 810: w = [-0.21088749 -1.3627833  -0.0450205 ], loss = 2.555620992605867e-10\n",
      "step 815: w = [-0.21088775 -1.3627839  -0.04501956], loss = 2.2824404866117476e-10\n",
      "step 820: w = [-0.21088797 -1.3627845  -0.04501867], loss = 2.021319778222619e-10\n",
      "step 825: w = [-0.21088819 -1.3627851  -0.04501782], loss = 1.7984225220146755e-10\n",
      "step 830: w = [-0.21088842 -1.3627857  -0.04501704], loss = 1.588409681563263e-10\n",
      "step 835: w = [-0.2108886  -1.3627863  -0.04501629], loss = 1.4032240647221528e-10\n",
      "step 840: w = [-0.21088874 -1.3627869  -0.04501558], loss = 1.2380452218963e-10\n",
      "step 845: w = [-0.21088889 -1.3627875  -0.04501492], loss = 1.0838168557070205e-10\n",
      "step 850: w = [-0.21088904 -1.3627881  -0.04501429], loss = 9.430489028572353e-11\n",
      "step 855: w = [-0.21088919 -1.3627887  -0.04501369], loss = 8.214612323698134e-11\n",
      "step 860: w = [-0.21088934 -1.3627893  -0.04501313], loss = 7.069455437713756e-11\n",
      "step 865: w = [-0.21088946 -1.3627899  -0.0450126 ], loss = 6.094761501040935e-11\n",
      "step 870: w = [-0.21088953 -1.3627901  -0.0450121 ], loss = 5.4538373817081265e-11\n",
      "step 875: w = [-0.21088961 -1.3627902  -0.04501162], loss = 4.937165259955023e-11\n",
      "step 880: w = [-0.21088968 -1.3627902  -0.04501119], loss = 4.486772064993261e-11\n",
      "step 885: w = [-0.21088976 -1.3627903  -0.04501077], loss = 4.101307488069139e-11\n",
      "step 890: w = [-0.21088983 -1.3627903  -0.04501038], loss = 3.774569545811346e-11\n",
      "step 895: w = [-0.2108899  -1.3627903  -0.04501001], loss = 3.5108419643714583e-11\n",
      "step 900: w = [-0.21088998 -1.3627905  -0.04500967], loss = 3.214582419697187e-11\n",
      "step 905: w = [-0.21089005 -1.3627905  -0.04500935], loss = 2.988847017104668e-11\n",
      "step 910: w = [-0.2108901  -1.3627905  -0.04500905], loss = 2.7935945223767078e-11\n",
      "step 915: w = [-0.2108901  -1.3627906  -0.04500876], loss = 2.562542365802667e-11\n",
      "step 920: w = [-0.2108901  -1.3627906  -0.04500849], loss = 2.445288763142095e-11\n",
      "step 925: w = [-0.2108901  -1.3627906  -0.04500824], loss = 2.3024129477966504e-11\n",
      "step 930: w = [-0.2108901  -1.3627906  -0.04500801], loss = 2.195326385956431e-11\n",
      "step 935: w = [-0.2108901  -1.3627907  -0.04500778], loss = 2.034777901727125e-11\n",
      "step 940: w = [-0.2108901  -1.3627907  -0.04500758], loss = 1.936180209216598e-11\n",
      "step 945: w = [-0.2108901  -1.3627907  -0.04500739], loss = 1.8589832798121542e-11\n",
      "step 950: w = [-0.2108901 -1.3627907 -0.0450072], loss = 1.793150176954139e-11\n",
      "step 955: w = [-0.2108901  -1.3627907  -0.04500703], loss = 1.744032696038289e-11\n",
      "step 960: w = [-0.2108901  -1.3627908  -0.04500687], loss = 1.6150114232060808e-11\n",
      "step 965: w = [-0.2108901  -1.3627908  -0.04500672], loss = 1.5638670913808994e-11\n",
      "step 970: w = [-0.2108901  -1.3627908  -0.04500658], loss = 1.534138614756042e-11\n",
      "step 975: w = [-0.2108901  -1.3627908  -0.04500645], loss = 1.4905533404774296e-11\n",
      "step 980: w = [-0.2108901  -1.3627908  -0.04500632], loss = 1.452193140044633e-11\n",
      "step 985: w = [-0.2108901 -1.362791  -0.0450062], loss = 1.3677315356674935e-11\n",
      "step 990: w = [-0.2108901  -1.362791   -0.04500609], loss = 1.3262427614457728e-11\n",
      "step 995: w = [-0.2108901  -1.362791   -0.04500598], loss = 1.3191406300627762e-11\n",
      "step 1000: w = [-0.2108901  -1.362791   -0.04500589], loss = 1.2943116196872939e-11\n",
      "step 1005: w = [-0.2108901  -1.362791   -0.04500579], loss = 1.2801437861142961e-11\n",
      "step 1010: w = [-0.2108901 -1.362791  -0.0450057], loss = 1.263618376601272e-11\n",
      "step 1015: w = [-0.2108901  -1.362791   -0.04500563], loss = 1.245041830050253e-11\n",
      "step 1020: w = [-0.2108901  -1.362791   -0.04500555], loss = 1.2284550980623532e-11\n",
      "step 1025: w = [-0.2108901  -1.362791   -0.04500548], loss = 1.2363446204310957e-11\n",
      "step 1030: w = [-0.2108901  -1.362791   -0.04500541], loss = 1.2194733937931357e-11\n",
      "step 1035: w = [-0.2108901  -1.362791   -0.04500535], loss = 1.2141876913618344e-11\n",
      "step 1040: w = [-0.2108901 -1.362791  -0.0450053], loss = 1.2162240832502835e-11\n",
      "step 1045: w = [-0.2108901  -1.362791   -0.04500524], loss = 1.196619799775922e-11\n",
      "step 1050: w = [-0.2108901  -1.362791   -0.04500518], loss = 1.1992462578547247e-11\n",
      "step 1055: w = [-0.2108901  -1.3627911  -0.04500513], loss = 1.116903531467317e-11\n",
      "step 1060: w = [-0.2108901  -1.3627911  -0.04500508], loss = 1.1066314531404942e-11\n",
      "step 1065: w = [-0.2108901  -1.3627911  -0.04500504], loss = 1.1000237179481509e-11\n",
      "step 1070: w = [-0.2108901 -1.3627911 -0.045005 ], loss = 1.0940296411854344e-11\n",
      "step 1075: w = [-0.2108901  -1.3627911  -0.04500497], loss = 1.0880721670880611e-11\n",
      "step 1080: w = [-0.2108901  -1.3627911  -0.04500493], loss = 1.0983158826860517e-11\n",
      "step 1085: w = [-0.2108901  -1.3627911  -0.04500489], loss = 1.0963785435080808e-11\n",
      "step 1090: w = [-0.2108901  -1.3627911  -0.04500486], loss = 1.0868384317519464e-11\n",
      "step 1095: w = [-0.2108901  -1.3627911  -0.04500482], loss = 1.070844801720483e-11\n",
      "step 1100: w = [-0.2108901  -1.3627911  -0.04500479], loss = 1.0769530232879188e-11\n",
      "step 1105: w = [-0.2108901  -1.3627911  -0.04500477], loss = 1.0761713568896436e-11\n",
      "step 1110: w = [-0.2108901  -1.3627911  -0.04500475], loss = 1.0759804505711124e-11\n",
      "step 1115: w = [-0.2108901  -1.3627911  -0.04500473], loss = 1.0769852024083981e-11\n",
      "step 1120: w = [-0.2108901  -1.3627911  -0.04500471], loss = 1.0859468706214681e-11\n",
      "step 1125: w = [-0.2108901 -1.3627911 -0.0450047], loss = 1.0954648646532839e-11\n",
      "step 1130: w = [-0.2108901  -1.3627911  -0.04500468], loss = 1.0946246513376945e-11\n",
      "step 1135: w = [-0.2108901  -1.3627911  -0.04500466], loss = 1.09385304633558e-11\n",
      "step 1140: w = [-0.2108901  -1.3627911  -0.04500464], loss = 1.1005910592609691e-11\n",
      "step 1145: w = [-0.2108901  -1.3627911  -0.04500462], loss = 1.0951886966759083e-11\n",
      "step 1150: w = [-0.2108901 -1.3627911 -0.0450046], loss = 1.0916947901229435e-11\n",
      "step 1155: w = [-0.2108901  -1.3627911  -0.04500458], loss = 1.0777193373834315e-11\n",
      "step 1160: w = [-0.2108901  -1.3627911  -0.04500457], loss = 1.078157615269637e-11\n",
      "step 1165: w = [-0.2108901  -1.3627911  -0.04500455], loss = 1.0797363003689497e-11\n",
      "step 1170: w = [-0.2108901  -1.3627911  -0.04500453], loss = 1.0791124070708147e-11\n",
      "step 1175: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0831702722258196e-11\n",
      "step 1180: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1185: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1190: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1195: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1200: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1205: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1210: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1215: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1220: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1225: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1230: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1235: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1240: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1245: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1250: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1255: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1260: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1265: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1270: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1275: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1280: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1285: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1290: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1295: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1300: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1305: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1310: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1315: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1320: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1325: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1330: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1335: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1340: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1345: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1350: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1355: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1360: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1365: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1370: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1375: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1380: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1385: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1390: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1395: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1400: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1405: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1410: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1415: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1420: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1425: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1430: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1435: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1440: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1445: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1450: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1455: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1460: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1465: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1470: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1475: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1480: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1485: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1490: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1495: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1500: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1505: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1510: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1515: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n",
      "step 1520: w = [-0.2108901  -1.3627911  -0.04500451], loss = 1.0849466290652199e-11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m w\n\u001b[32m     29\u001b[39m fast_backprop = jit(backprop,static_argnums=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackprop(w,lr,epoch).block_until_ready()\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mtimeit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfast_backprop(w,lr,epoch).block_until_ready()\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/IPython/core/interactiveshell.py:2511\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2509\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2511\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/IPython/core/magics/execution.py:1226\u001b[39m, in \u001b[36mExecutionMagics.timeit\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1223\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m time_number >= \u001b[32m0.2\u001b[39m:\n\u001b[32m   1224\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1226\u001b[39m all_runs = \u001b[43mtimer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1227\u001b[39m best = \u001b[38;5;28mmin\u001b[39m(all_runs) / number\n\u001b[32m   1228\u001b[39m worst = \u001b[38;5;28mmax\u001b[39m(all_runs) / number\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.2/Frameworks/Python.framework/Versions/3.14/lib/python3.14/timeit.py:205\u001b[39m, in \u001b[36mTimer.repeat\u001b[39m\u001b[34m(self, repeat, number)\u001b[39m\n\u001b[32m    203\u001b[39m r = []\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     r.append(t)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/IPython/core/magics/execution.py:184\u001b[39m, in \u001b[36mTimer.timeit\u001b[39m\u001b[34m(self, number)\u001b[39m\n\u001b[32m    182\u001b[39m gc.disable()\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     timing = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<magic-timeit>:1\u001b[39m, in \u001b[36minner\u001b[39m\u001b[34m(_it, _timer)\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mbackprop\u001b[39m\u001b[34m(w, lr, epoch)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(w,lr,epoch):\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         w = w - lr * \u001b[43mgrad_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     26\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: w = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss(w)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/api.py:469\u001b[39m, in \u001b[36mgrad.<locals>.grad_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fun, docstr=docstr, argnums=argnums)\n\u001b[32m    467\u001b[39m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrad_f\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m   _, g = \u001b[43mvalue_and_grad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/api.py:543\u001b[39m, in \u001b[36mvalue_and_grad.<locals>.value_and_grad_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    541\u001b[39m   _check_input_dtype_grad(holomorphic, allow_int, leaf)\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m   ans, vjp_py = \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mdyn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    545\u001b[39m   ans, vjp_py, aux = _vjp(f_partial, *dyn_args, has_aux=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/api.py:2230\u001b[39m, in \u001b[36m_vjp\u001b[39m\u001b[34m(fun, has_aux, *primals)\u001b[39m\n\u001b[32m   2228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[32m   2229\u001b[39m   flat_fun, out_tree = flatten_fun_nokwargs(fun, in_tree)\n\u001b[32m-> \u001b[39m\u001b[32m2230\u001b[39m   out_primals_flat, out_pvals, jaxpr, residuals = \u001b[43mad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m      \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2232\u001b[39m   out_tree = out_tree()\n\u001b[32m   2233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/interpreters/ad.py:281\u001b[39m, in \u001b[36mlinearize\u001b[39m\u001b[34m(traceable, *primals, **kwargs)\u001b[39m\n\u001b[32m    279\u001b[39m has_aux = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mhas_aux\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.use_direct_linearize.value:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdirect_linearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[32m    283\u001b[39m   jvpfun = jvp(traceable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/interpreters/ad.py:255\u001b[39m, in \u001b[36mdirect_linearize\u001b[39m\u001b[34m(traceable, primals, kwargs, has_aux, tag)\u001b[39m\n\u001b[32m    250\u001b[39m   aux_primals = [x.primal\n\u001b[32m    251\u001b[39m                  \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, LinearizeTracer)\n\u001b[32m    252\u001b[39m                  \u001b[38;5;129;01mand\u001b[39;00m x._trace.tag \u001b[38;5;129;01mis\u001b[39;00m linearize_trace.tag\n\u001b[32m    253\u001b[39m                  \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m aux]\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m   ans = \u001b[43mtraceable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   aux = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    257\u001b[39m out_primals, out_tangents = unzip2(\u001b[38;5;28mmap\u001b[39m(linearize_trace.to_primal_tangent_pair, ans))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    211\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/api_util.py:91\u001b[39m, in \u001b[36mflatten_fun_nokwargs\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;129m@lu\u001b[39m.transformation_with_aux2\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_fun_nokwargs\u001b[39m(f: Callable, store: lu.Store,\n\u001b[32m     89\u001b[39m                          in_tree: PyTreeDef, *args_flat):\n\u001b[32m     90\u001b[39m   py_args = tree_unflatten(in_tree, args_flat)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m   ans = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m   ans, out_tree = tree_flatten(ans)\n\u001b[32m     93\u001b[39m   store.store(out_tree)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/api_util.py:293\u001b[39m, in \u001b[36m_argnums_partial\u001b[39m\u001b[34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m args = [\u001b[38;5;28mnext\u001b[39m(fixed_args_).val \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m sentinel \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(fixed_args_, sentinel) \u001b[38;5;129;01mis\u001b[39;00m sentinel\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mloss\u001b[39m\u001b[34m(w)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(w):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/numpy/reductions.py:864\u001b[39m, in \u001b[36mmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m    800\u001b[39m \u001b[38;5;129m@export\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(a: ArrayLike, axis: Axis = \u001b[38;5;28;01mNone\u001b[39;00m, dtype: DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    802\u001b[39m          out: \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, keepdims: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, *,\n\u001b[32m    803\u001b[39m          where: ArrayLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Array:\n\u001b[32m    804\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return the mean of array elements along a given axis.\u001b[39;00m\n\u001b[32m    805\u001b[39m \n\u001b[32m    806\u001b[39m \u001b[33;03m  JAX implementation of :func:`numpy.mean`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    862\u001b[39m \u001b[33;03m           [6. ]], dtype=float32)\u001b[39;00m\n\u001b[32m    863\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ensure_optional_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m               \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupcast_f16_for_computation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/pjit.py:257\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    255\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    256\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m p, args_flat = \u001b[43m_trace_for_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m    259\u001b[39m  executable, pgle_profiler, const_args) = _run_python_pjit(\n\u001b[32m    260\u001b[39m      p, args_flat, fun, jit_info, args, kwargs)\n\u001b[32m    262\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    263\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    264\u001b[39m     pgle_profiler, const_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/pjit.py:479\u001b[39m, in \u001b[36m_trace_for_jit\u001b[39m\u001b[34m(fun, ji, args, kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m dbg = debug_info(\n\u001b[32m    472\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mjit\u001b[39m\u001b[33m'\u001b[39m, fun, args, kwargs, static_argnums=ji.static_argnums,\n\u001b[32m    473\u001b[39m     static_argnames=ji.static_argnames, sourceinfo=ji.fun_sourceinfo,\n\u001b[32m    474\u001b[39m     signature=ji.fun_signature)\n\u001b[32m    476\u001b[39m signature, dynargs = jax_jit.parse_arguments(\n\u001b[32m    477\u001b[39m     args, \u001b[38;5;28mtuple\u001b[39m(kwargs.values()), \u001b[38;5;28mtuple\u001b[39m(kwargs.keys()), ji.static_argnums,\n\u001b[32m    478\u001b[39m     ji.static_argnames, tree_util.default_registry)\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m avals_list = \u001b[43m_infer_input_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m args_ft = FlatTree.flatten_static_argnums_argnames(\n\u001b[32m    481\u001b[39m     args, kwargs, ji.static_argnums, ji.static_argnames)\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# TODO(dougalm): args_ft.vals and dynargs should be exactly the same here.\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Why did we need to flatten in C++ and again in Python?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Programming/practice/env/lib/python3.14/site-packages/jax/_src/pjit.py:599\u001b[39m, in \u001b[36m_infer_input_type\u001b[39m\u001b[34m(fun, dbg, explicit_args)\u001b[39m\n\u001b[32m    595\u001b[39m   p = PjitParams(consts, params, avals.vals, avals.tree_without_statics,\n\u001b[32m    596\u001b[39m                  out_avals.tree, dbg.safe_arg_names(\u001b[38;5;28mlen\u001b[39m(avals)))\n\u001b[32m    597\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m p, p.consts + dynargs\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_infer_input_type\u001b[39m(fun: Callable, dbg: core.DebugInfo,\n\u001b[32m    600\u001b[39m                       explicit_args) -> \u001b[38;5;28mtuple\u001b[39m[core.AbstractValue, ...]:\n\u001b[32m    601\u001b[39m   avals = []\n\u001b[32m    602\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Split the key to get independent sub-keys\n",
    "key, subkey = random.split(key, 2)\n",
    "X = random.normal(subkey,shape=(20,3))\n",
    "\n",
    "key, subkey = random.split(key, 2)\n",
    "w_true = random.normal(subkey,shape=(3,))\n",
    "y = X @ w_true \n",
    "\n",
    "print(f'X:\\n{X}\\nw_true:\\n{w_true}\\ny:\\n{y}')\n",
    "\n",
    "def loss(w):\n",
    "    return jnp.mean((X @ w - y) ** 2)\n",
    "\n",
    "grad_loss = grad(loss)\n",
    "\n",
    "w = jnp.zeros(3)\n",
    "lr = 0.01\n",
    "epoch = 2000\n",
    "\n",
    "def backprop(w,lr,epoch):\n",
    "    for i in range(epoch):\n",
    "        w = w - lr * grad_loss(w)\n",
    "        if i % 5 == 0:\n",
    "            print(f\"step {i:2d}: w = {w}, loss = {loss(w)}\")\n",
    "    return w\n",
    "\n",
    "fast_backprop = jit(backprop,static_argnums=2)\n",
    "%timeit backprop(w,lr,epoch).block_until_ready()\n",
    "%timeit fast_backprop(w,lr,epoch).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `jnp.*` | NumPy-like array ops (immutable) |\n",
    "| `grad(f)` | Auto-differentiation |\n",
    "| `jit(f)` | XLA compilation for speed |\n",
    "| `vmap(f)` | Auto-vectorize over batches |\n",
    "| `random.PRNGKey(seed)` | Create explicit RNG key |\n",
    "| `random.split(key, n)` | Split key into n sub-keys |\n",
    "| `x.at[i].set(v)` | Immutable array update |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
