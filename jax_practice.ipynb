{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Practice Worksheet\n",
    "\n",
    "A simple study guide covering JAX fundamentals.\n",
    "\n",
    "### What is JAX?\n",
    "\n",
    "JAX is Google's library for high-performance numerical computing and machine learning research. Think of it as **NumPy on steroids** — it gives you a familiar NumPy-like API but adds three superpowers:\n",
    "\n",
    "1. **Automatic differentiation** (`grad`) — compute gradients of any function automatically, which is the backbone of training neural networks and optimization in general.\n",
    "2. **Just-in-time compilation** (`jit`) — compile your Python functions down to optimized machine code using XLA (Accelerated Linear Algebra), the same compiler backend that powers TensorFlow.\n",
    "3. **Auto-vectorization** (`vmap`) — write a function that works on a single example, then instantly vectorize it to work on entire batches with no manual loop writing.\n",
    "\n",
    "### Why JAX instead of NumPy or PyTorch?\n",
    "\n",
    "- **vs NumPy**: JAX can run on GPU/TPU and supports autodiff. NumPy is CPU-only and has no built-in gradients.\n",
    "- **vs PyTorch**: JAX takes a more *functional* approach — no classes, no `nn.Module`, just pure functions. This makes code easier to reason about and compose. PyTorch is more object-oriented and imperative.\n",
    "- **Composability**: JAX transformations (`grad`, `jit`, `vmap`) can be freely composed. You can `jit(vmap(grad(f)))` and it just works.\n",
    "\n",
    "### Key mental model\n",
    "\n",
    "JAX functions should be **pure functions** — they take inputs and return outputs with no side effects. This is what enables all the powerful transformations to work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "We import four core pieces of JAX:\n",
    "\n",
    "- **`jax`** — the top-level module; gives us `jax.devices()` to check what hardware we're running on.\n",
    "- **`jax.numpy` (as `jnp`)** — a drop-in replacement for NumPy. Almost every `np.something()` has a `jnp.something()` equivalent. The key difference is that `jnp` arrays live on accelerators (GPU/TPU) and are immutable.\n",
    "- **`grad`, `jit`, `vmap`** — the three core transformations. These are *higher-order functions*: they take a function as input and return a new, transformed function.\n",
    "- **`jax.random`** — JAX's random number system. Unlike NumPy's `np.random`, JAX doesn't use global random state. Every random call requires an explicit key (more on this in Section 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.9.0\n",
      "Devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"Devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JAX Arrays vs NumPy\n",
    "\n",
    "`jax.numpy` mirrors the NumPy API almost exactly, so if you know NumPy, you already know most of `jnp`. The critical differences:\n",
    "\n",
    "### Immutability\n",
    "\n",
    "JAX arrays are **immutable** — once created, you cannot modify them in-place. This means:\n",
    "\n",
    "```python\n",
    "# NumPy (works fine):\n",
    "x[0] = 5\n",
    "\n",
    "# JAX (raises an error!):\n",
    "x[0] = 5  # TypeError: JAX arrays are immutable\n",
    "```\n",
    "\n",
    "Instead, JAX provides the `.at[].set()` syntax which returns a **new** array with the change applied, leaving the original untouched. This functional style is essential because JAX's transformations (grad, jit, vmap) rely on functions being pure — no side effects, no mutation.\n",
    "\n",
    "### Other `.at[]` operations\n",
    "\n",
    "Beyond `.set()`, you can also use:\n",
    "- `x.at[i].add(v)` — add `v` to position `i`\n",
    "- `x.at[i].multiply(v)` — multiply position `i` by `v`\n",
    "- `x.at[i].min(v)` / `x.at[i].max(v)` — element-wise min/max\n",
    "\n",
    "### Device placement\n",
    "\n",
    "JAX arrays are automatically placed on the best available device (GPU > TPU > CPU). You can check with `x.devices()`. Data transfers between CPU and GPU happen automatically but can be a performance bottleneck if you're not careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1. 2. 3.]\n",
      "b:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "c: [ 50.  55.  60.  65.  70.  75.  80.  85.  90.  95. 100.]\n"
     ]
    }
   ],
   "source": [
    "# Creating arrays (just like numpy)\n",
    "a = jnp.array([1.0, 2.0, 3.0])\n",
    "b = jnp.zeros((3, 3))\n",
    "c = jnp.linspace(50, 100, 11)\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b:\\n\", b)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [0. 0. 0. 0. 0.]\n",
      "updated:  [ 0.  0. 99.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Immutable updates — use .at[].set()\n",
    "x = jnp.zeros(5)\n",
    "x_updated = x.at[2].set(99.0)\n",
    "\n",
    "print(\"original:\", x)\n",
    "print(\"updated: \", x_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2a\n",
    "Create a 4x4 identity matrix using `jnp.eye()`, then replace the top-left element with `7.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[7. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.eye(4)\n",
    "x_updated = x.at[0,0].set(7.0)\n",
    "print(x)\n",
    "print(x_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Numbers (PRNGKey)\n",
    "\n",
    "This is one of the **biggest differences** between JAX and NumPy, and it trips up almost everyone at first.\n",
    "\n",
    "### The problem with NumPy's random\n",
    "\n",
    "In NumPy, random numbers come from a **global** random state:\n",
    "\n",
    "```python\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(size=(3,))  # uses and mutates global state\n",
    "```\n",
    "\n",
    "This is convenient but causes problems:\n",
    "- **Not reproducible under parallelism** — if two threads pull random numbers, the order is unpredictable.\n",
    "- **Not compatible with `jit`** — JAX's JIT compiler needs pure functions with no hidden state.\n",
    "\n",
    "### JAX's solution: explicit PRNG keys\n",
    "\n",
    "In JAX, every random call takes an explicit **key** (a pair of 32-bit integers):\n",
    "\n",
    "```python\n",
    "key = random.PRNGKey(42)          # create a key from a seed\n",
    "x = random.normal(key, shape=(3,)) # use the key\n",
    "```\n",
    "\n",
    "**Critical rule**: never reuse a key for two different random calls! If you do, you'll get the same \"random\" numbers. Instead, **split** the key:\n",
    "\n",
    "```python\n",
    "key, subkey = random.split(key)   # split into 2 new keys\n",
    "x = random.normal(subkey, (3,))   # use the subkey, keep key for later\n",
    "```\n",
    "\n",
    "### The split pattern\n",
    "\n",
    "`random.split(key, n)` takes one key and returns `n` independent new keys. The common pattern is:\n",
    "\n",
    "```python\n",
    "key, *subkeys = random.split(key, 4)  # keep key, get 3 subkeys\n",
    "```\n",
    "\n",
    "This feels verbose at first, but it guarantees **perfect reproducibility** regardless of execution order, parallelism, or hardware — which is essential for scientific computing and ML research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal samples: [ 0.60576403  0.7990441  -0.908927  ]\n",
      "uniform samples: [0.6672406 0.7214867 0.1267947]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Split the key to get independent sub-keys\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "\n",
    "x = random.normal(subkey1, shape=(3,))\n",
    "y = random.uniform(subkey2, shape=(3,))\n",
    "\n",
    "print(\"normal samples:\", x)\n",
    "print(\"uniform samples:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3a\n",
    "Generate a 2x3 matrix of random integers between 0 and 10. (Hint: `random.randint`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 6 6]\n",
      " [8 7 7]]\n",
      "[[4 2 6]\n",
      " [4 1 4]]\n",
      "[[5 0 5]\n",
      " [7 3 1]]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(1)\n",
    "key,subkey = random.split(key,2)\n",
    "\n",
    "x = random.randint(subkey,(2,3),0,10)\n",
    "print(x)\n",
    "\n",
    "key,subkey = random.split(key,2)\n",
    "x = random.randint(subkey,(2,3),0,10)\n",
    "print(x)\n",
    "\n",
    "key,subkey = random.split(key,2)\n",
    "x = random.randint(subkey,(2,3),0,10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `grad` — Automatic Differentiation\n",
    "\n",
    "This is arguably JAX's most important feature for machine learning.\n",
    "\n",
    "### What is automatic differentiation?\n",
    "\n",
    "Differentiation (finding derivatives/gradients) is the core of how neural networks learn. There are three ways to compute derivatives:\n",
    "\n",
    "1. **Symbolic** — like you'd do by hand in calculus class. Exact but gets messy for complex functions.\n",
    "2. **Numerical** — approximate with `(f(x+h) - f(x)) / h`. Simple but slow and imprecise.\n",
    "3. **Automatic** — what JAX does. It traces through your Python code and applies the chain rule automatically. Exact *and* efficient.\n",
    "\n",
    "### How `grad` works\n",
    "\n",
    "`grad(f)` takes a function `f` and returns a **new function** that computes the derivative:\n",
    "\n",
    "```python\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "df = grad(f)     # df is now a function that computes 2x\n",
    "df(3.0)          # returns 6.0\n",
    "```\n",
    "\n",
    "Key details:\n",
    "- **`grad` differentiates w.r.t. the first argument by default.** Use `argnums` to change this: `grad(f, argnums=1)` differentiates w.r.t. the second argument.\n",
    "- **Input must be a float (or array of floats).** `grad` won't work on integers.\n",
    "- **Output must be a scalar.** If your function returns an array, use `jax.jacobian` instead, or sum/mean the output first.\n",
    "- **You can compose `grad`** — `grad(grad(f))` gives you the second derivative, `grad(grad(grad(f)))` the third, and so on.\n",
    "\n",
    "### Why this matters for ML\n",
    "\n",
    "In machine learning, we define a **loss function** that measures how wrong our model is. `grad` lets us compute exactly how to adjust each parameter to reduce that loss — that's the gradient, and it's the signal that drives learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "def f(x): \n",
    "    return x ** 2\n",
    "\n",
    "df = grad(f)\n",
    "print(df(3.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0)   = 7.0\n",
      "f'(2.0)  = 15.0\n",
      "f''(2.0) = 16.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 3 + 2 * x ** 2 - 5 * x + 1\n",
    "\n",
    "df = grad(f)        # first derivative\n",
    "ddf = grad(grad(f))  # second derivative\n",
    "\n",
    "x = 2.0\n",
    "print(f\"f({x})   = {f(x)}\")\n",
    "print(f\"f'({x})  = {df(x)}\")    # 3x^2 + 4x - 5 => 15\n",
    "print(f\"f''({x}) = {ddf(x)}\")   # 6x + 4 => 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4a\n",
    "Define `g(x) = sin(x) * exp(-x)`. Compute its gradient at `x = 1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  0.3095599\n",
      "Derivative:  -0.110793784\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return jnp.sin(x) * jnp.exp(-x)\n",
    "\n",
    "x=1.0\n",
    "print(\"Output: \", g(x))\n",
    "\n",
    "dv = grad(g)\n",
    "print(\"Derivative: \", dv(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `jit` — Just-In-Time Compilation\n",
    "\n",
    "### The problem: Python is slow\n",
    "\n",
    "Python is an interpreted language, which means each operation is executed one at a time with lots of overhead. For numerical code with many operations, this overhead adds up fast.\n",
    "\n",
    "### The solution: XLA compilation\n",
    "\n",
    "When you wrap a function with `jit`, JAX doesn't run it immediately. Instead, it:\n",
    "\n",
    "1. **Traces** the function — runs it once with abstract \"placeholder\" values to figure out what operations it performs.\n",
    "2. **Compiles** the traced operations into a single optimized XLA program — fusing operations, eliminating redundant computation, and targeting your specific hardware (CPU/GPU/TPU).\n",
    "3. **Caches** the compiled version — subsequent calls with the same input shapes skip tracing and run the optimized code directly.\n",
    "\n",
    "### When to use `jit`\n",
    "\n",
    "- Any function you call repeatedly with the same input shapes (e.g., a training step).\n",
    "- Functions with many small operations that can be fused together.\n",
    "- Inner loops of numerical algorithms.\n",
    "\n",
    "### Gotchas to watch out for\n",
    "\n",
    "- **First call is slow** — that's the tracing + compilation step. All subsequent calls are fast.\n",
    "- **No Python side effects inside `jit`** — `print()` only runs during tracing, not on subsequent calls. Same for any Python-level if/else based on array values.\n",
    "- **Input shapes must be static** — if you pass different-shaped inputs, JAX recompiles (slow). Use `jax.ensure_compile_time_eval()` or `static_argnums` for arguments that change but aren't arrays.\n",
    "- **`block_until_ready()`** — JAX uses async dispatch, so timing benchmarks need this call to force the computation to finish before measuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55 ms ± 104 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.34 ms ± 12.2 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def slow_fn(x):\n",
    "    for _ in range(50):\n",
    "        x = x @ x\n",
    "    return x\n",
    "\n",
    "fast_fn = jit(slow_fn)\n",
    "\n",
    "mat = random.normal(random.PRNGKey(0), (100, 100))\n",
    "\n",
    "%timeit slow_fn(mat).block_until_ready()\n",
    "%timeit fast_fn(mat).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `vmap` — Auto-Vectorization\n",
    "\n",
    "### The problem: batching is tedious\n",
    "\n",
    "In ML, you almost always work with **batches** of data. Say you write a function that processes a single image — now you need it to work on 64 images at once. You have two bad options:\n",
    "\n",
    "1. **Python loop** — `for img in batch: process(img)`. Works but extremely slow.\n",
    "2. **Manual batching** — rewrite your function to handle an extra batch dimension everywhere. Works and is fast, but error-prone and clutters the code.\n",
    "\n",
    "### The solution: `vmap`\n",
    "\n",
    "`vmap(f)` takes a function `f` that works on a **single example** and returns a new function that works on a **batch of examples** — automatically. Under the hood, it transforms the function to operate over an extra leading axis, with no Python loops and no manual reshaping.\n",
    "\n",
    "```python\n",
    "# Works on a single vector\n",
    "def normalize(x):\n",
    "    return x / jnp.linalg.norm(x)\n",
    "\n",
    "# Now works on a batch of vectors\n",
    "batch_normalize = vmap(normalize)\n",
    "```\n",
    "\n",
    "### Key parameters\n",
    "\n",
    "- **`in_axes`** — which axis of each input to map over. Default is `0` (first axis). Use `None` for arguments that shouldn't be batched.\n",
    "  ```python\n",
    "  # x is batched (axis 0), weights is shared across the batch\n",
    "  vmap(f, in_axes=(0, None))(batch_x, weights)\n",
    "  ```\n",
    "- **`out_axes`** — which axis of the output the mapped dimension should appear on. Default is `0`.\n",
    "\n",
    "### Why `vmap` matters\n",
    "\n",
    "- **Clean code** — write single-example logic, get batch processing for free.\n",
    "- **Performance** — `vmap` generates the same efficient batched code you'd write by hand.\n",
    "- **Composability** — `vmap(vmap(f))` maps over two axes (e.g., batch of sequences of vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(x):\n",
    "    return jnp.sqrt(jnp.sum(x ** 2))\n",
    "\n",
    "batch_l2 = vmap(l2_norm)\n",
    "\n",
    "batch = random.normal(random.PRNGKey(1), (5, 3))\n",
    "print(\"batch shape:\", batch.shape)\n",
    "print(\"norms:\", batch_l2(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6a\n",
    "Write a function `dot_product(a, b)` for single vectors, then use `vmap` to compute dot products for a batch of 10 vector pairs (each of length 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting It Together — Simple Gradient Descent\n",
    "\n",
    "Now we combine `grad` with a loop to do **gradient descent** — the fundamental optimization algorithm behind all of deep learning.\n",
    "\n",
    "### How gradient descent works\n",
    "\n",
    "1. Start with some initial guess for your parameter `x`.\n",
    "2. Compute the **gradient** of your loss function at `x` — this tells you the direction of steepest *increase*.\n",
    "3. Take a small step in the **opposite** direction (to decrease the loss): `x = x - lr * gradient`.\n",
    "4. Repeat until the loss is small enough.\n",
    "\n",
    "### The learning rate (`lr`)\n",
    "\n",
    "The learning rate controls how big each step is:\n",
    "- **Too large** — you overshoot the minimum and the loss explodes.\n",
    "- **Too small** — convergence is painfully slow.\n",
    "- **Just right** — smooth convergence to the minimum.\n",
    "\n",
    "A common starting point is `0.01` or `0.1` for simple problems.\n",
    "\n",
    "### What's happening in the code below\n",
    "\n",
    "We minimize `f(x) = (x - 3)^2`, which has its minimum at `x = 3`. The gradient is `f'(x) = 2(x - 3)`. Starting from `x = 0`, each step nudges `x` closer to 3. JAX computes `f'(x)` for us automatically via `grad` — we never write the derivative by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return (x - 3.0) ** 2\n",
    "\n",
    "grad_loss = grad(loss)\n",
    "\n",
    "x = 0.0\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(20):\n",
    "    x = x - lr * grad_loss(x)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"step {i:2d}: x = {x:.4f}, loss = {loss(x):.4f}\")\n",
    "\n",
    "print(f\"\\nFinal x: {x:.4f} (should be close to 3.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7a — Linear Regression with Gradient Descent\n",
    "\n",
    "This is the \"real\" version of what you just saw. Instead of optimizing a single number, you're optimizing a **vector of weights** `w` to fit a linear model `y = Xw`.\n",
    "\n",
    "The loss function is the **mean squared error**: `f(w) = ||Xw - y||^2`\n",
    "\n",
    "Steps:\n",
    "- Generate random `X` (20x3) and `w_true` (3,), compute `y = X @ w_true`\n",
    "- Start from random `w`, run gradient descent to recover `w_true`\n",
    "- `grad` handles the vector calculus for you — it returns a gradient with the same shape as `w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `jnp.*` | NumPy-like array ops (immutable) |\n",
    "| `grad(f)` | Auto-differentiation |\n",
    "| `jit(f)` | XLA compilation for speed |\n",
    "| `vmap(f)` | Auto-vectorize over batches |\n",
    "| `random.PRNGKey(seed)` | Create explicit RNG key |\n",
    "| `random.split(key, n)` | Split key into n sub-keys |\n",
    "| `x.at[i].set(v)` | Immutable array update |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jax_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
